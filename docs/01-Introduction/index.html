<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.fd17476c3">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="THE DATA ENGINEERING COOKBOOK Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="THE DATA ENGINEERING COOKBOOK Blog Atom Feed"><title data-react-helmet="true">01-Introduction | THE DATA ENGINEERING COOKBOOK</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="01-Introduction | THE DATA ENGINEERING COOKBOOK"><meta data-react-helmet="true" name="description" content="Introduction"><meta data-react-helmet="true" property="og:description" content="Introduction"><meta data-react-helmet="true" property="og:url" content="http://cookbook.learndataengineering.com/docs/01-Introduction"><link data-react-helmet="true" rel="shortcut icon" href="/images/CookbookCover.jpg"><link data-react-helmet="true" rel="alternate" href="http://cookbook.learndataengineering.com/docs/01-Introduction" hreflang="x-default"><link data-react-helmet="true" rel="canonical" href="http://cookbook.learndataengineering.com/docs/01-Introduction"><link rel="stylesheet" href="/assets/css/styles.5ba6ab3f.css">
<link rel="preload" href="/assets/js/styles.4aacfb65.js" as="script">
<link rel="preload" href="/assets/js/runtime~main.6087afab.js" as="script">
<link rel="preload" href="/assets/js/main.4c5ae3c4.js" as="script">
<link rel="preload" href="/assets/js/1.4743d043.js" as="script">
<link rel="preload" href="/assets/js/2.d8ee9dd6.js" as="script">
<link rel="preload" href="/assets/js/18.28e25878.js" as="script">
<link rel="preload" href="/assets/js/19.f0bdf1f6.js" as="script">
<link rel="preload" href="/assets/js/935f2afb.cff03dc6.js" as="script">
<link rel="preload" href="/assets/js/17896441.aacbb830.js" as="script">
<link rel="preload" href="/assets/js/f5648033.2c628efe.js" as="script">
</head>
<body>
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_1oUP">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/01-Introduction">Cookbook</a></div><div class="navbar__items navbar__items--right"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Data Engineering Academy</a><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Plumbers Of Data Science</a><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs/01-Introduction">Cookbook</a></li><li class="menu__list-item"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="menu__link">Data Engineering Academy</a></li><li class="menu__list-item"><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="menu__link">Plumbers Of Data Science</a></li><li class="menu__list-item"><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Data Engineering</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/01-Introduction">01-Introduction</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/02-BasicSkills">02-BasicSkills</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/03-AdvancedSkills">03-AdvancedSkills</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/04-HandsOnCourse">04-HandsOnCourse</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/05-CaseStudies">05-CaseStudies</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/06-BestPracticesCloud">06-BestPracticesCloud</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/07-DataSources">07-DataSources</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/08-InterviewQuestions">08-InterviewQuestions</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/09-BooksAndCourses">09-BooksAndCourses</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/10-Updates">10-Updates</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">01-Introduction</h1></header><div class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h1><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="contents"></a>Contents<a class="hash-link" href="#contents" title="Direct link to heading">#</a></h2><ul><li><a href="/docs/01-Introduction#what-is-this-cookbook">What is this Cookbook</a></li><li><a href="/docs/01-Introduction#data-engineers">Data Engineers</a></li><li><a href="/docs/01-Introduction#my-data-science-platform-blueprint">My Data Science Platform Blueprint</a><ul><li><a href="/docs/01-Introduction#connect">Connect</a></li><li><a href="/docs/01-Introduction#buffer">Buffer</a></li><li><a href="/docs/01-Introduction#processing-framework">Processing Framework</a></li><li><a href="/docs/01-Introduction#store">Store</a></li><li><a href="/docs/01-Introduction#visualize">Visualize</a></li></ul></li><li><a href="/docs/01-Introduction#who-companies-need">Who Companies Need</a></li><li><a href="/docs/01-Introduction#how-to-learn-data-engineering">How to Learn Data Engineering</a><ul><li><a href="/docs/01-Introduction#Interview-with-Andreas-on-the-Super-Data-Science-Podcast">Andreas interview on the Super Data Science Podcast</a></li><li><a href="/docs/01-Introduction#building-blocks-to-learn-data-engineering">Building Blocks to Learn Data Engineering</a></li><li><a href="/docs/01-Introduction#roadmap-for-data-analysts">Roadmap for Beginners</a></li><li><a href="/docs/01-Introduction#roadmap-for-data-analysts">Roadmap for Data Analysts</a></li><li><a href="/docs/01-Introduction#roadmap-for-data-scientists">Roadmap for Data Scientists</a></li><li><a href="/docs/01-Introduction#roadmap-for-software-engineers">Roadmap for Software Engineers</a></li></ul></li><li><a href="/docs/01-Introduction#data-engineers-skills-matrix">Data Engineers Skills Matrix</a></li><li><a href="/docs/01-Introduction#how-to-become-a-senior-data-engineer">How to Become a Senior Data Engineer</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-this-cookbook"></a>What is this Cookbook<a class="hash-link" href="#what-is-this-cookbook" title="Direct link to heading">#</a></h2><p>I get asked a lot:
&quot;What do you actually need to learn to become an awesome data engineer?&quot;</p><p>Well, look no further. You&#x27;ll find it here!</p><p>If you are looking for AI algorithms and such data scientist things,
this book is not for you.</p><p><strong>How to use this Cookbook:</strong>
This book is intended to be a starting point for you. It is not a training! I want to help you to identify the topics to look into to become an awesome data engineer in the process.</p><p>It hinges on my Data Science Platform Blueprint. Check it out below. Once you understand it, you can find in the book tools that fit into each key area of a Data Science platform (Connect, Buffer, Processing Framework, Store, Visualize).</p><p>Select a few tools you are interested in, then research and work with them.</p><p>Don&#x27;t learn everything in this book! Focus.</p><p><strong>What types of content are in this book?</strong>
You are going to find five types of content in this book: Articles
I wrote, links to my podcast episodes (video &amp; audio), more than 200
links to helpful websites I like, data engineering interview questions
and case studies.</p><p><strong>This book is a work in progress!</strong>
As you can see, this book is not finished. I&#x27;m constantly adding new
stuff and doing videos for the topics. But, obviously, because I do this
as a hobby, my time is limited. You can help make this book even
better.</p><p><strong>Help make this book awesome!</strong>
If you have some cool links or topics for the cookbook, please become a
contributor on GitHub: <a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer">https://github.com/andkret/Cookbook</a>. Fork the
repo, add them, and create a pull request. Or join the discussion by
opening Issues. Tell me your thoughts, what you value,
what you think should be included, or correct me where I am wrong.
You can also write me an email any time to
plumbersofdatascience\@gmail.com anytime.</p><p><strong>This Cookbook is and will always be free!</strong></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="if-you-like-this-book--need-more-help"></a>If You Like This Book &amp; Need More Help:<a class="hash-link" href="#if-you-like-this-book--need-more-help" title="Direct link to heading">#</a></h2><p>Check out my Data Engineering Academy at LearnDataEngineering.com</p><p><strong>Visit learndataengineering.com:</strong> <a href="https://learndataengineering.com" target="_blank" rel="noopener noreferrer">Click Here</a></p><ul><li>Huge Step by step Data Engineering Academy with over 30 courses</li><li>Unlimited access incl. future courses during subsciption</li><li>Access to all courses and example projects in the Academy</li><li>Associate Data Engineer Certification</li><li>Data Engineering on AWS E-Commerce example project</li><li>Microsoft Azure example project</li><li>Document Streaming example project with Docker, FastAPI, Apache Kafka, Apache Spark,</li><li>MongoDB and Streamlit</li><li>Time Series example project with InfluxDB and Grafana</li><li>Lifetime access to the private Discord Workspace</li><li>Course certificates</li><li>Currently over 54 hours of videos</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="support-this-book-for-free"></a>Support This Book For Free!<a class="hash-link" href="#support-this-book-for-free" title="Direct link to heading">#</a></h2><ul><li><strong>Amazon:</strong> <a href="https://www.amazon.com/shop/plumbersofdatascience" target="_blank" rel="noopener noreferrer">Click Here</a> buy whatever you like from Amazon using this link* (Also check out my complete podcast gear and books)</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-contribute"></a>How To Contribute<a class="hash-link" href="#how-to-contribute" title="Direct link to heading">#</a></h2><p>If you have some cool links or topics for the cookbook, please become a contributor.</p><p>Simply pull the repo, add your ideas and create a pull request.
You can also open an issue and put your thoughts there.</p><p>Please use the &quot;Issues&quot; function for comments.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-engineers"></a>Data Engineers<a class="hash-link" href="#data-engineers" title="Direct link to heading">#</a></h2><p>Data Engineers are the link between the management&#x27;s data strategy
and the data scientists or analysts that need to work with data.</p><p>What they do is build the platforms that enable data scientists to do
their magic.</p><p>These platforms are usually used in five different ways:</p><ul><li><p>Data ingestion and storage of large amounts of data.</p></li><li><p>Algorithm creation by data scientists.</p></li><li><p>Automation of the data scientist&#x27;s machine learning models and
algorithms for production use.</p></li><li><p>Data visualization for employees and customers.</p></li><li><p>Most of the time these guys start as traditional solution architects
for systems that involve SQL databases, web servers, SAP
installations and other &quot;standard&quot; systems.</p></li></ul><p>But, to create big data platforms, the engineer needs to be an expert in
specifying, setting up, and maintaining big data technologies like:
Hadoop, Spark, HBase, Cassandra, MongoDB, Kafka, Redis, and more.</p><p>What they also need is experience on how to deploy systems on cloud
infrastructure like at Amazon or Google, or on-premise hardware.</p><p>| Podcast Episode: #048 From Wannabe Data Scientist To Engineer My Journey
|------------------|
|In this episode Kate Strachnyi interviews me for her humans of data science podcast. We talk about how I found out that I am more into the engineering part of data science.<br>
| <a href="https://youtu.be/pIZkTuN5AMM" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/048-From-Wannabe-Data-Scientist-To-Engineer-My-Journey-e45i2o" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="my-data-science-platform-blueprint"></a>My Data Science Platform Blueprint<a class="hash-link" href="#my-data-science-platform-blueprint" title="Direct link to heading">#</a></h2><p>I have created a simple and modular big data platform
blueprint. It is based on what I have seen in the field and
read in tech blogs all over the internet.</p><p>Why do I believe it will be super useful to you? Because, unlike other blueprints, it is not focused on technology.</p><p>Following my blueprint will allow you to create the big data platform
that fits exactly your needs. Building the perfect platform will allow
data scientists to discover new insights. It will enable you to perfectly handle big data and allow you to make
data-driven decisions.</p><p>The blueprint is focused on the five key areas: Connect, Buffer, Processing Frameworks, Store, and Visualize.</p><p><img alt="Data Science Platform Blueprint" src="/assets/images/Data-Science-Blueprint-New-0d2ae636eca9abb0a36e777467ca0a5e.jpg"></p><p>Having the platform split like this turns it into a modular platform with
loosely coupled interfaces.</p><p>Why is it so important to have a modular platform?</p><p>If you have a platform that is not modular, you end up with something
that is fixed or hard to modify. This means you can not adjust the
platform to changing requirements of the company.</p><p>Because of modularity, it is possible to specifically select tools for your use case. It also allows you to replace every component, if you need it.</p><p>Now, lets talk more about each key area.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="connect"></a>Connect<a class="hash-link" href="#connect" title="Direct link to heading">#</a></h3><p>Ingestion is all about getting the data in from the source and making it
available to later stages. Sources can be everything from tweets to server
logs, to IoT sensor data (e.g. from cars).</p><p>Sources send data to your API Services. The API is going to push the
data into temporary storage.</p><p>The temporary storage allows other stages simple and fast access to
incoming data.</p><p>A great solution is to use messaging queue systems like Apache Kafka,
RabbitMQ or AWS Kinesis. Sometimes people also use caches for
specialised applications like Redis.</p><p>A good practice is that the temporary storage follows the
publish-subscribe pattern. This way APIs can publish messages and
Analytics can quickly consume them.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="buffer"></a>Buffer<a class="hash-link" href="#buffer" title="Direct link to heading">#</a></h3><p>In the buffer phase you have pub/sub systems like Apache Kafka, Redis, or other Cloud tools like Google pub/sub or AWS Kinesis.</p><p>These systems are more or less message Queues.
You put something in on one side and take it out on the other.</p><p>The idea behind buffers is to have an intermediate system for the incoming data.</p><p>How this works is, for instance, you&#x27;re getting data in from from an API.
The API is publishing into the message queue. Data is buffered there until it is picked up by the processing.</p><p>If you don&#x27;t have a buffer, you can run into problems when writing directly into a store or you&#x27;re processing the data directly. You can always have peaks of incoming data that stall the systems.</p><p>Like, it&#x27;s lunch break and people are working with your app way more than usual.
There&#x27;s more data coming in very very fast, faster than the analytics of the storage can handle.</p><p>In this case, you would run into problems, because the whole system would stall. It would therefore take long to process the data, and your customers would be annoyed.</p><p>With a buffer, you buffer the incoming data. Processes for storage and analytics can take out only as much data as they can process. You are no longer in danger of overpowering systems.</p><p>Buffers are also really good for building pipelines.</p><p>You take data out of Kafka, pre-process it, and put it back into Kafka.
Then, with another analytics process, you take the processed data back out and put it into a store.</p><p>Ta-da! A pipeline.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="processing-framework"></a>Processing Framework<a class="hash-link" href="#processing-framework" title="Direct link to heading">#</a></h3><p>The analyse stage is where the actual analytics is done in
the form of stream and batch processing.</p><p>Streaming data is taken from ingest and fed into analytics. Streaming
analyses the &quot;live&quot; data, thus generating fast results.</p><p>As the central and most important stage, analytics also has access to
the big data storage. Because of that connection, analytics can take a
big chunk of data and analyse it.</p><p>This type of analysis is called batch processing. It will deliver you
answers for the big questions.</p><p>For a short video about batch and stream processing and their use cases, click on the link below:</p><p><a href="https://www.youtube.com/watch?v=o-aGi3FmdfU" target="_blank" rel="noopener noreferrer">Adding Batch to a Streaming Pipeline</a></p><p>The analytics process, batch or streaming, is not a one-way process.
Analytics can also write data back to the big data storage.</p><p>Oftentimes, writing data back to the storage makes sense. It allows you
to combine previous analytics outputs with the raw data.</p><p>Analytics give insights when you combine
raw data. This combination will often allow you to create even more
useful insights.</p><p>A wide variety of analytics tools are available. Ranging from MapReduce
or AWS Elastic MapReduce to Apache Spark and AWS lambda.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="store"></a>Store<a class="hash-link" href="#store" title="Direct link to heading">#</a></h3><p>This is the typical big-data storage where you just store everything. It
enables you to analyse the big picture.</p><p>Most of the data might seem useless for now, but it is of utmost
importance to keep it. Throwing data away is a big no-no.</p><p>Why not throw something away when it is useless?</p><p>Although it seems useless for now, data scientists can work with the
data. They might find new ways to analyse the data and generate valuable
insights from it.</p><p>What kind of systems can be used to store big data?</p><p>Systems like Hadoop HDFS, Hbase, Amazon S3 or DynamoDB are a perfect fit
to store big data.</p><p>Check out my podcast how to decide between SQL and NoSQL:
<a href="https://anchor.fm/andreaskayy/embed/episodes/NoSQL-Vs-SQL-How-To-Choose-e12f1o" target="_blank" rel="noopener noreferrer">https://anchor.fm/andreaskayy/embed/episodes/NoSQL-Vs-SQL-How-To-Choose-e12f1o</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="visualize"></a>Visualize<a class="hash-link" href="#visualize" title="Direct link to heading">#</a></h3><p>Displaying data is as important as ingesting, storing, and analysing it.
Visualizations enable business users to make data-driven decisions.</p><p>This is why it is important to have a good visual presentation of the
data. Sometimes you have a lot of different use cases or projects using
the platform.</p><p>It might not be possible to build the perfect UI that fits
everyone&#x27;s needs. What you should do in this case is enable others to build the
perfect UI themselves.</p><p>How to do that? By creating APIs to access the data and making them
available to developers.</p><p>Either way, UI or API, the trick is to give the display stage direct
access to the data in the big-data cluster. This kind of access will
allow the developers to use analytics results as well as raw data to
build the perfect application.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="who-companies-need"></a>Who Companies Need<a class="hash-link" href="#who-companies-need" title="Direct link to heading">#</a></h2><p>For a company, it is important to have well-trained data engineers.</p><p>That&#x27;s why companies are looking for people with experience of tools in every part of the above platform blueprint. One common theme I see is cloud platform experience on AWS, Azure or GCP.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-learn-data-engineering"></a>How to Learn Data Engineering<a class="hash-link" href="#how-to-learn-data-engineering" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="interview-with-andreas-on-the-super-data-science-podcast"></a>Interview with Andreas on the Super Data Science Podcast<a class="hash-link" href="#interview-with-andreas-on-the-super-data-science-podcast" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h4><p>This interview with Andreas  on Jon Krohn&#x27;s Super Data Science podcast delves into the intricacies of data engineering, highlighting its critical role in the broader data science ecosystem. Andreas, calling from Northern Bavaria, Germany, shares his journey from a data analyst to becoming a renowned data engineering educator through his Learn Data Engineering Academy. The conversation touches upon the foundational importance of data engineering in ensuring data quality, scalability, and accessibility for data scientists and analysts.</p><p>Andreas emphasizes that the best data engineers often have a background in the companies domain/niche, which equips them with a deep understanding of the end user&#x27;s needs. The discussion also explores the essential tools and skills required in the field, such as relational databases, APIs, ETL tools, data streaming with Kafka, and the significance of learning platforms like AWS, Azure, and GCP. Andreas highlights the evolving landscape of data engineering, with a nod towards the emergence of roles like analytics engineers and the increasing importance of automation and advanced data processing tools like Snowflake, Databricks, and DBT.</p><p>The interview is not just a technical deep dive but also a personal journey of discovery and passion for data engineering, underscoring the perpetual learning and adaptation required in the fast-evolving field of data science.</p><p>| Watch or listen to this interview -&gt; 657: How to Learn Data Engineering — with Andreas Kretz
|------------------|
| Was super fun talking with Jon about Data Engineering on the podcast. Think this will be very helpful for you :)
| <a href="https://youtu.be/sbDFADS-zo8" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> / <a href="https://www.superdatascience.com/podcast/how-to-learn-data-engineering" target="_blank" rel="noopener noreferrer">Listen to the Podcast</a>|</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="qa-highlights"></a>Q&amp;A Highlights<a class="hash-link" href="#qa-highlights" title="Direct link to heading">#</a></h4><p><strong>Q: What is data engineering, and why is it important?</strong> A: Data engineering is the foundation of the data science process, focusing on collecting, cleaning, and managing data to make it accessible and usable for data scientists and analysts. It&#x27;s crucial for automating data processes, ensuring data quality, and enabling scalable data analysis and machine learning models.</p><p><strong>Q: How does one transition from data analysis to data engineering?</strong>
A: The transition involves gaining a deep understanding of data pipelines, learning to work with various data processing and management tools, and developing skills in programming languages and technologies relevant to data engineering, such as SQL, Python, and cloud platforms like AWS or Azure.</p><p><strong>Q: What are the key skills and tools for a data engineer?</strong>
A: Essential skills include proficiency in SQL, experience with ETL tools, knowledge of programming languages like Python, and familiarity with cloud services and data processing frameworks like Apache Spark. Tools like Kafka for data streaming and platforms like Snowflake and Databricks are also becoming increasingly important.</p><p><strong>Q: Can you elaborate on the emerging role of analytics engineers?</strong>
A: Analytics engineers focus on bridging the gap between raw data management and data analysis, working closely with data warehouses and using tools like dbt to prepare and model data for easy analysis. This role is pivotal in making data more accessible and actionable for decision-making processes.</p><p><strong>Q: What advice would you give to someone aspiring to become a data engineer?</strong>
A: Start by mastering the basics of SQL and Python, then explore and gain experience with various data engineering tools and technologies. It&#x27;s also important to understand the data science lifecycle and how data engineering fits within it. Continuous learning and staying updated with industry trends are key to success in this field.</p><p><strong>Q: How does a data engineer&#x27;s role evolve with experience?</strong>
A: A data engineer&#x27;s journey typically starts with focusing on specific tasks or segments of data pipelines, using a limited set of tools. As they gain experience, they broaden their skill set, manage entire data pipelines, and take on more complex projects. Senior data engineers often lead teams, design data architectures, and collaborate closely with data scientists and business stakeholders to drive data-driven decisions.</p><p><strong>Q: What distinguishes data engineering from machine learning engineering?</strong>
A: While both fields overlap, especially in the use of data, data engineering focuses on the infrastructure and processes for handling data, ensuring its quality and accessibility. Machine learning engineering, on the other hand, centers on deploying and maintaining machine learning models in production environments. A strong data engineering foundation is essential for effective machine learning engineering.</p><p><strong>Q: Why might a data analyst transition to data engineering?</strong>
A: Data analysts may transition to data engineering to work on more technical aspects of data handling, such as building and maintaining data pipelines, automating data processes, and ensuring data scalability. This transition allows them to have a more significant impact on the data lifecycle and contribute to more strategic data initiatives within an organization.</p><p><strong>Q: Can you share a challenging project you worked on as a data engineer?</strong>
A: One challenging project involved creating a scalable data pipeline for real-time processing of machine-generated data. The complexity lay in handling vast volumes of data, ensuring its quality, and integrating various data sources while maintaining high performance. This project highlighted the importance of selecting the right tools and technologies, such as Kafka for data streaming and Apache Spark for data processing, to meet the project&#x27;s demands.</p><p><strong>Q: How does the cloud influence data engineering?</strong>
A: Cloud platforms like AWS, Azure, and GCP have transformed data engineering by providing scalable, flexible, and cost-effective solutions for data storage, processing, and analysis. They offer a wide range of services and tools that data engineers can leverage to build robust data pipelines and infrastructure, facilitating easier access to advanced data processing capabilities and enabling more innovative data solutions.</p><p><strong>Q: What future trends do you see in data engineering?</strong>
A: Future trends in data engineering include the increasing adoption of cloud-native services, the rise of real-time data processing and analytics, greater emphasis on data governance and security, and the continued growth of machine learning and AI-driven data processes. Additionally, tools and platforms that simplify data engineering tasks and enable more accessible data integration and analysis will become more prevalent, democratizing data across organizations.</p><p><strong>Q: How does the background of a data analyst contribute to their success as a data engineer?</strong>
A: Data analysts have a unique advantage when transitioning to data engineering due to their understanding of data&#x27;s end-use. Their experience in analyzing data gives them insights into what makes data valuable and usable, enabling them to design more effective and user-centric data pipelines and storage solutions.</p><p><strong>Q: What role does automation play in data engineering?</strong>
A: Automation is crucial in data engineering for scaling data processes, reducing manual errors, and ensuring consistency in data handling. Automated data pipelines allow for real-time data processing and integration, making data more readily available for analysis and decision-making.</p><p><strong>Q: Can you discuss the significance of cloud platforms in data engineering?</strong>
A: Cloud platforms like AWS, Azure, and GCP offer scalable, flexible, and cost-effective solutions for data storage, processing, and analysis. They provide data engineers with a suite of tools and services to build robust data pipelines, implement machine learning models, and manage large volumes of data efficiently.</p><p><strong>Q: How does data engineering support data science and machine learning projects?</strong>
A: Data engineering lays the groundwork for data science and machine learning by preparing and managing the data infrastructure. It ensures that high-quality, relevant data is available for model training and analysis, thereby enabling more accurate predictions and insights.</p><p><strong>Q: What emerging technologies or trends should data engineers be aware of?</strong>
A: Data engineers should keep an eye on the rise of machine learning operations (MLOps) for integrating machine learning models into production, the growing importance of real-time data processing and analytics, and the adoption of serverless computing for more efficient resource management. Additionally, technologies like containerization (e.g., Docker) and orchestration (e.g., Kubernetes) are becoming critical for deploying and managing scalable data applications.</p><p><strong>Q: What challenges do data engineers face, and how can they be addressed?</strong>
A: Data engineers often grapple with data quality issues, integrating disparate data sources, and scaling data infrastructure to meet growing data volumes. Addressing these challenges requires a solid understanding of data architecture principles, continuous monitoring and testing of data pipelines, and adopting best practices for data governance and management.</p><p><strong>Q: How important is collaboration between data engineers and other data professionals?</strong>
A: Collaboration is key in the data ecosystem. Data engineers need to work closely with data scientists, analysts, and business stakeholders to ensure that data pipelines are aligned with business needs and analytical goals. Effective communication and a shared understanding of data objectives are vital for the success of data-driven projects.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="building-blocks-to-learn-data-engineering"></a>Building Blocks to Learn Data Engineering<a class="hash-link" href="#building-blocks-to-learn-data-engineering" title="Direct link to heading">#</a></h3><p>The following Roadmaps all hinge on the courses in my Data Engineering Academy. They are designed to help students who come from many different professions and enable to build a customized curriculum.</p><p>Here are all the courses currently available February 2024:</p><p><strong>Colors:</strong> Blue (The Basics), Green (Platform &amp; Pipeline Fundamentals), Orange (Fundamental Tools), Red (Example Projects)</p><p><img alt="Building blocks of your curriculum" src="/assets/images/All-Courses-at-Learn-Data-Engineering-6586ddb571ab4307243ceb69cb48f620.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="roadmap-for-beginners"></a>Roadmap for Beginners<a class="hash-link" href="#roadmap-for-beginners" title="Direct link to heading">#</a></h3><p>Start this roadmap at my Academy: <a href="https://learndataengineering.com/p/data-engineering-for-beginners" target="_blank" rel="noopener noreferrer">Start Today</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="11-week-data-engineering-roadmap-for-beginners--graduates"></a>11-Week Data Engineering Roadmap for Beginners &amp; Graduates<a class="hash-link" href="#11-week-data-engineering-roadmap-for-beginners--graduates" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="master-the-fundamentals-and-build-your-first-data-pipelines"></a>Master the Fundamentals and Build Your First Data Pipelines<a class="hash-link" href="#master-the-fundamentals-and-build-your-first-data-pipelines" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="starting-in-data-engineering"></a>Starting in Data Engineering<a class="hash-link" href="#starting-in-data-engineering" title="Direct link to heading">#</a></h4><p>Starting in data engineering can feel overwhelming, especially if you’re coming from a non-technical background or have only limited experience with coding and databases.</p><p>This 11-week roadmap, with a time commitment of 5–10 hours per week, is designed to help you build strong foundations in data engineering, step by step, before moving into cloud platforms and more advanced pipelines. You’ll learn essential concepts, hands-on coding, data modeling, and cloud ETL development—everything you need to kickstart your career as a data engineer.</p><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-this-roadmap-is-for-you"></a>Why This Roadmap is for You<a class="hash-link" href="#why-this-roadmap-is-for-you" title="Direct link to heading">#</a></h4><ul><li>You’re just starting in data engineering and need a clear learning path</li><li>You want to build a strong foundation in data platforms, SQL, and Python</li><li>You need hands-on experience with data modeling, cloud ETL, and automation</li><li>You want to work on real-world projects that prepare you for a data engineering job</li></ul><p>By the end of this roadmap, you’ll have the skills, tools, and project experience to confidently apply for entry-level data engineering roles and start your career in the field.</p><p><img alt="Building blocks of your curriculum" src="/assets/images/Roadmap-For-Beginners-3aae71b024c0b2d83cd2b7ec3d86fb9b.jpg"></p><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-youll-achieve-in-this-roadmap"></a>What You’ll Achieve in This Roadmap<a class="hash-link" href="#what-youll-achieve-in-this-roadmap" title="Direct link to heading">#</a></h4><p>This roadmap is structured to help you understand the full data engineering workflow: from learning the fundamentals of data platforms and modeling to working with Python, SQL, and cloud-based ETL pipelines.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="learning-goals"></a>Learning Goals<a class="hash-link" href="#learning-goals" title="Direct link to heading">#</a></h4><table><thead><tr><th>Goal</th><th>Description</th></tr></thead><tbody><tr><td><strong>Goal #1</strong></td><td>Gain Experience in Data Platforms &amp; Pipeline Design</td></tr><tr><td><strong>Goal #2</strong></td><td>Work with Data Like a Data Engineer Using Python &amp; SQL</td></tr><tr><td><strong>Goal #3</strong></td><td>Learn Dimensional Data Modeling &amp; Data Warehousing with Snowflake</td></tr><tr><td><strong>Goal #4</strong></td><td>Gain Experience with ELT Using dbt &amp; Orchestration with Airflow</td></tr><tr><td><strong>Goal #5</strong></td><td>Build Your First ETL Pipeline on a Cloud Platform</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="11-week-learning-roadmap"></a>11-Week Learning Roadmap<a class="hash-link" href="#11-week-learning-roadmap" title="Direct link to heading">#</a></h4><table><thead><tr><th>Week</th><th>Topic</th><th>Key Learning Outcomes</th></tr></thead><tbody><tr><td><strong>Week 1</strong></td><td>Introduction &amp; Platform &amp; Pipeline Design</td><td>Understand data platforms, data pipelines, and the tools used in data engineering</td></tr><tr><td><strong>Week 2</strong></td><td>Relational Data Modeling</td><td>Develop skills in creating relational data models for structured data</td></tr><tr><td><strong>Week 3 &amp; 4</strong></td><td>Python for Data Engineers</td><td>Learn Python for data processing, data manipulation, and pipeline development</td></tr><tr><td><strong>Week 5</strong></td><td>Advanced SQL</td><td>Gain expertise in querying, storing, and manipulating data in relational databases</td></tr><tr><td><strong>Week 6</strong></td><td>Dimensional Data Modeling</td><td>Master the techniques of dimensional modeling for analytics and reporting</td></tr><tr><td><strong>Week 7</strong></td><td>Snowflake Data Warehousing</td><td>Learn how to use Snowflake as a cloud data warehouse</td></tr><tr><td><strong>Week 8</strong></td><td>Data Transformation with dbt</td><td>Transform and model data efficiently using dbt</td></tr><tr><td><strong>Week 9</strong></td><td>Data Pipeline Orchestration with Airflow</td><td>Automate and manage data workflows using Apache Airflow</td></tr><tr><td><strong>Week 10 &amp; 11</strong></td><td>End-to-End Project on AWS, Azure, or GCP</td><td>Complete an end-to-end project on a cloud platform of your choice</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-1-introduction--platform--pipeline-design"></a>Week 1: Introduction &amp; Platform &amp; Pipeline Design<a class="hash-link" href="#week-1-introduction--platform--pipeline-design" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="1-learn-the-basics-of-platform--pipeline-design"></a>1. Learn the Basics of Platform &amp; Pipeline Design<a class="hash-link" href="#1-learn-the-basics-of-platform--pipeline-design" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-platform-and-pipeline-design"></a>Data Platform and Pipeline Design<a class="hash-link" href="#data-platform-and-pipeline-design" title="Direct link to heading">#</a></h5><p><strong>Learn how to build data pipelines with templates and examples for Azure, GCP, and Hadoop</strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description"></a>Description<a class="hash-link" href="#description" title="Direct link to heading">#</a></h5><p>Data pipelines are the backbone of any Data Science platform. They are essential for data ingestion, processing, and machine learning workflows. This training will help you understand how to create stream and batch processing pipelines as well as machine learning pipelines by going through the most essential basics—complemented by templates and examples for useful cloud computing platforms.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-pipeline-design" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Platform &amp; Pipeline Basics</strong></td><td>The Platform Blueprint</td><td>10:11</td></tr><tr><td></td><td>Data Engineering Tools Guide</td><td>2:44</td></tr><tr><td></td><td>End-to-End Pipeline Example</td><td>6:18</td></tr><tr><td><strong>Ingestion Pipelines</strong></td><td>Push Ingestion Pipelines</td><td>3:42</td></tr><tr><td></td><td>Pull Ingestion Pipelines</td><td>3:34</td></tr><tr><td><strong>Pipeline Types</strong></td><td>Batch Pipelines</td><td>3:07</td></tr><tr><td></td><td>Streaming Pipelines</td><td>3:34</td></tr><tr><td><strong>Visualization</strong></td><td>Stream Analytics</td><td>2:26</td></tr><tr><td></td><td>Visualization Pipelines</td><td>3:47</td></tr><tr><td></td><td>Visualization with Hive &amp; Spark on Hadoop</td><td>6:21</td></tr><tr><td></td><td>Visualization Data via Spark Thrift Server</td><td>3:27</td></tr><tr><td><strong>Platform Examples</strong></td><td>AWS, Azure, GCP (Currently Slides Only)</td><td>START</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="2-get-to-know-the-different-data-stores"></a>2. Get to Know the Different Data Stores<a class="hash-link" href="#2-get-to-know-the-different-data-stores" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="choosing-data-stores"></a>Choosing Data Stores<a class="hash-link" href="#choosing-data-stores" title="Direct link to heading">#</a></h5><p><strong>Learn the different types of data storages and when to use which</strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-1"></a>Description<a class="hash-link" href="#description-1" title="Direct link to heading">#</a></h5><p>One part of creating a data platform and pipelines is to choose data stores, which is the focus of this training. You will learn about relational databases, NoSQL databases, data warehouses, and data lakes. The goal is to help you understand when to use each type of data storage and how to incorporate them into your pipeline.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/choosing-data-stores" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-1"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-1" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>What are Data Stores?</td><td>2:09</td></tr><tr><td><strong>Data Stores Basics</strong></td><td>OLTP vs OLAP</td><td>7:34</td></tr><tr><td></td><td>ETL vs ELT</td><td>5:45</td></tr><tr><td></td><td>Data Stores Ranking</td><td>4:05</td></tr><tr><td><strong>Relational Databases</strong></td><td>How to Choose Data Stores</td><td>8:11</td></tr><tr><td></td><td>Relational Databases Concepts</td><td>6:34</td></tr><tr><td><strong>NoSQL Databases</strong></td><td>NoSQL Basics</td><td>10:39</td></tr><tr><td></td><td>Document Stores</td><td>5:56</td></tr><tr><td></td><td>Time Series Databases</td><td>5:00</td></tr><tr><td></td><td>Search Engines</td><td>4:18</td></tr><tr><td></td><td>Wide Column Stores</td><td>4:22</td></tr><tr><td></td><td>Key Value Stores</td><td>4:59</td></tr><tr><td></td><td>Graph Databases</td><td>1:05</td></tr><tr><td><strong>Data Warehouses &amp; Data Lakes</strong></td><td>Data Warehouses</td><td>5:32</td></tr><tr><td></td><td>Data Lakes</td><td>7:10</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="3-see-data-modeling-examples-for-the-learned-data-stores"></a>3. See Data Modeling Examples for the Learned Data Stores<a class="hash-link" href="#3-see-data-modeling-examples-for-the-learned-data-stores" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-modeling-1"></a>Data Modeling 1<a class="hash-link" href="#data-modeling-1" title="Direct link to heading">#</a></h5><p><strong>Learn how to design schemas for SQL, NoSQL, and Data Warehouses</strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-2"></a>Description<a class="hash-link" href="#description-2" title="Direct link to heading">#</a></h5><p>Schema design is a critical skill for data engineers. This training covers schema design for different data stores using an e-commerce dataset. You will see examples of how the same dataset is modeled for relational databases, NoSQL stores, wide column stores, document stores, key-value stores, and data warehouses. This will help you understand how to create maintainable models and avoid data swamps.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-2"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-2" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>Why Data Modeling Is Important</td><td>5:44</td></tr><tr><td></td><td>A Good Dataset</td><td>1:28</td></tr><tr><td><strong>Relational Databases</strong></td><td>Schema Design</td><td>9:27</td></tr><tr><td><strong>Wide Column Stores</strong></td><td>Schema Design</td><td>7:35</td></tr><tr><td><strong>Document Stores</strong></td><td>Schema Design</td><td>7:28</td></tr><tr><td><strong>Key Value Stores</strong></td><td>Schema Design</td><td>4:49</td></tr><tr><td><strong>Data Warehouses</strong></td><td>Schema Design</td><td>4:44</td></tr><tr><td><strong>Data Modeling Workshop</strong></td><td>November 2024</td><td>101:49</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-2-relational-data-modeling"></a>Week 2: Relational Data Modeling<a class="hash-link" href="#week-2-relational-data-modeling" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="start-with-relational-data-modeling"></a>Start with Relational Data Modeling<a class="hash-link" href="#start-with-relational-data-modeling" title="Direct link to heading">#</a></h5><p><strong>Relational Data modeling</strong> is an essential skill, as even in modern &quot;big data&quot; environments, relational databases are often used for managing and serving metadata. This week focuses on building a strong foundation in relational data modeling, which is crucial for structuring data effectively and optimizing query performance.</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="relational-data-modeling"></a>Relational Data Modeling<a class="hash-link" href="#relational-data-modeling" title="Direct link to heading">#</a></h5><p><strong>Learn the most important basics to create a data model for OLTP data stores</strong></p><h6><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-3"></a>Description<a class="hash-link" href="#description-3" title="Direct link to heading">#</a></h6><p>This course covers everything you need to know about relational data modeling—from understanding entities, attributes, and relationships to normalizing data models up to the third normal form (3NF). You will learn how to design conceptual, logical, and physical data models, implement primary and foreign keys, and ensure data quality through constraints and validations. Practical exercises include setting up a MySQL server with Docker and creating ER diagrams using MySQL Workbench.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/relational-data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-3"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-3" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Basics and Prepare the Environment</strong></td><td>Relational Data Models History</td><td>3:16</td></tr><tr><td></td><td>Installing MySQL Server and MySQL Workbench</td><td>8:04</td></tr><tr><td></td><td>MySQL Workbench Introduction</td><td>4:36</td></tr><tr><td><strong>Create the Conceptual Data Model</strong></td><td>The Design Process Explained</td><td>4:14</td></tr><tr><td></td><td>Discover the Entities</td><td>10:24</td></tr><tr><td></td><td>Discover the Attributes</td><td>13:09</td></tr><tr><td></td><td>Define Entity Relationships and Normalize the Data</td><td>11:19</td></tr><tr><td><strong>Defining and Resolving Relationships</strong></td><td>Identifying vs Non-Identifying Relationships</td><td>2:01</td></tr><tr><td></td><td>How to Resolve Many-to-Many Relationships</td><td>4:00</td></tr><tr><td></td><td>How to Resolve One-to-Many Relationships</td><td>2:34</td></tr><tr><td></td><td>How to Resolve One-to-One Relationships</td><td>1:45</td></tr><tr><td><strong>Hands-On Workbench - Creating the Database</strong></td><td>Create Your ER Diagram Using Workbench</td><td>19:46</td></tr><tr><td></td><td>Create a Physical Data Model</td><td>4:13</td></tr><tr><td></td><td>Populate the MySQL DB with Data from .xls File</td><td>15:13</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-3--4-python-for-data-engineers"></a>Week 3 &amp; 4: Python for Data Engineers<a class="hash-link" href="#week-3--4-python-for-data-engineers" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-4"></a>Description<a class="hash-link" href="#description-4" title="Direct link to heading">#</a></h5><p>This course offers a comprehensive guide to using Python for data engineering tasks. You’ll learn advanced Python features, including data processing with Pandas, working with APIs, interacting with PostgreSQL databases, and handling data types like JSON. The course also covers important programming concepts like exception handling, modules, unit testing, and object-oriented programming—all within the context of data engineering.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/python-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-4"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-4" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Advanced Python</strong></td><td>Classes</td><td>4:37</td></tr><tr><td></td><td>Modules</td><td>3:06</td></tr><tr><td></td><td>Exception Handling</td><td>8:55</td></tr><tr><td></td><td>Logging</td><td>5:12</td></tr><tr><td><strong>Data Engineering</strong></td><td>Datetime</td><td>8:04</td></tr><tr><td></td><td>JSON</td><td>9:54</td></tr><tr><td></td><td>JSON Validation</td><td>15:10</td></tr><tr><td></td><td>UnitTesting</td><td>16:44</td></tr><tr><td></td><td>Pandas: Intro &amp; Data Types</td><td>8:43</td></tr><tr><td></td><td>Pandas: Appending &amp; Merging DataFrames</td><td>7:49</td></tr><tr><td></td><td>Pandas: Normalizing &amp; Lambdas</td><td>4:12</td></tr><tr><td></td><td>Pandas: Pivot &amp; Parquet Write, Read</td><td>6:17</td></tr><tr><td></td><td>Pandas: Melting &amp; JSON Normalization</td><td>8:15</td></tr><tr><td></td><td>Numpy</td><td>4:47</td></tr><tr><td><strong>Working with Data Sources/Sinks</strong></td><td>Requests (Working with APIs)</td><td>11:15</td></tr><tr><td></td><td>Working with Databases: Setup</td><td>4:06</td></tr><tr><td></td><td>Working with Databases: Tables, Bulk Load, Queries</td><td>8:12</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-5-sql-for-data-engineers"></a>Week 5: SQL for Data Engineers<a class="hash-link" href="#week-5-sql-for-data-engineers" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-5"></a>Description<a class="hash-link" href="#description-5" title="Direct link to heading">#</a></h5><p>SQL is the backbone of working with relational databases, and if you’re getting into Data Engineering, mastering SQL is a must. This course provides the essential SQL skills needed to work with databases effectively. You&#x27;ll learn how to manage data, build efficient queries, and perform advanced operations to handle real-world data challenges.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/sql-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-5"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-5" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Basics</strong></td><td>Database Management Systems &amp; SQL</td><td>3:49</td></tr><tr><td></td><td>The Chinook Database</td><td>3:03</td></tr><tr><td></td><td>SQLite Installation</td><td>7:02</td></tr><tr><td></td><td>DBeaver Installation</td><td>4:08</td></tr><tr><td></td><td>Data Types in SQLite</td><td>6:15</td></tr><tr><td><strong>Basic SQL</strong></td><td>DML &amp; DDL</td><td>15:06</td></tr><tr><td></td><td>Select Statements</td><td>6:03</td></tr><tr><td></td><td>Grouping &amp; Aggregation</td><td>10:12</td></tr><tr><td></td><td>Joins</td><td>10:05</td></tr><tr><td><strong>Advanced SQL</strong></td><td>TCP Transaction Control Language</td><td>6:42</td></tr><tr><td></td><td>Common Table Expressions &amp; Subqueries</td><td>10:26</td></tr><tr><td></td><td>Window Functions 1: Concept &amp; Syntax</td><td>5:00</td></tr><tr><td></td><td>Window Functions 2: Aggregate Functions</td><td>7:24</td></tr><tr><td></td><td>Window Functions 3: Ranking Functions</td><td>6:05</td></tr><tr><td></td><td>Window Functions 4: Analytical Functions</td><td>7:20</td></tr><tr><td><strong>Optimization</strong></td><td>Query Optimization</td><td>START</td></tr><tr><td></td><td>Indexing Best Practices</td><td>START</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-6-dimensional-data-modeling"></a>Week 6: Dimensional Data Modeling<a class="hash-link" href="#week-6-dimensional-data-modeling" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-6"></a>Description<a class="hash-link" href="#description-6" title="Direct link to heading">#</a></h5><p>Dimensional data modeling is a crucial skill for data engineers working with analytics use-cases where data needs to be structured efficiently for reporting and business insights. This course covers the basics of dimensional modeling, the medallion architecture, and how to create data models for OLAP data stores.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-modeling-3-dimensional-data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-6"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-6" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>Data Warehousing Basics</td><td>6:42</td></tr><tr><td><strong>Dimensional Modeling Basics</strong></td><td>Approaches to building a data warehouse</td><td>5:20</td></tr><tr><td></td><td>Dimension tables explained</td><td>5:34</td></tr><tr><td></td><td>Fact tables explained</td><td>6:34</td></tr><tr><td></td><td>Identifying dimensions</td><td>3:16</td></tr><tr><td><strong>Data Warehouse Setup</strong></td><td>What is DuckDB</td><td>5:58</td></tr><tr><td></td><td>First DuckDB hands-on</td><td>2:20</td></tr><tr><td></td><td>Creating tables in DuckDB</td><td>2:40</td></tr><tr><td></td><td>Installing DBeaver</td><td>6:49</td></tr><tr><td><strong>Working With The Data Warehouse</strong></td><td>Exploring SCD0 and SCD1</td><td>19:57</td></tr><tr><td></td><td>Exploring SCD2</td><td>13:52</td></tr><tr><td></td><td>Exploring transaction fact table</td><td>6:28</td></tr><tr><td></td><td>Exploring accumulating fact table</td><td>7:17</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-7-snowflake-for-data-engineers"></a>Week 7: Snowflake for Data Engineers<a class="hash-link" href="#week-7-snowflake-for-data-engineers" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-7"></a>Description<a class="hash-link" href="#description-7" title="Direct link to heading">#</a></h5><p>Snowflake is a highly popular cloud-based data warehouse that is ideal for beginners due to its simplicity and powerful features. In this course, you will learn how to set up Snowflake, load and process data, and create visualizations. The course covers both SQL and Python methods for managing data within Snowflake, and provides hands-on experience with connecting Snowflake to other tools such as PowerBI.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/snowflake-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-7"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-7" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Introduction</strong></td><td>Snowflake basics</td><td>4:16</td></tr><tr><td></td><td>Data Warehousing basics</td><td>4:13</td></tr><tr><td></td><td>How Snowflake fits into data platforms</td><td>3:14</td></tr><tr><td><strong>Setup</strong></td><td>Snowflake Account setup</td><td>4:24</td></tr><tr><td></td><td>Creating your warehouse &amp; UI overview</td><td>4:15</td></tr><tr><td><strong>Loading CSVs from your PC</strong></td><td>Our dataset &amp; goals</td><td>3:01</td></tr><tr><td></td><td>Setup Snowflake database</td><td>10:29</td></tr><tr><td></td><td>Preparing the upload file</td><td>8:31</td></tr><tr><td></td><td>Using internal stages with SnowSQL</td><td>12:37</td></tr><tr><td></td><td>Splitting a data table into two tables</td><td>6:38</td></tr><tr><td><strong>Visualizing Data</strong></td><td>Creating a visualization worksheet</td><td>7:08</td></tr><tr><td></td><td>Creating a dashboard</td><td>5:23</td></tr><tr><td></td><td>Connect PowerBI to Snowflake</td><td>6:03</td></tr><tr><td></td><td>Query data with Python</td><td>7:35</td></tr><tr><td><strong>Automation</strong></td><td>Create import task</td><td>9:18</td></tr><tr><td></td><td>Create table refresh task</td><td>3:40</td></tr><tr><td></td><td>Test our pipeline</td><td>3:14</td></tr><tr><td><strong>AWS S3 Integration</strong></td><td>Working with external stages for AWS S3</td><td>10:20</td></tr><tr><td></td><td>Implementing snowpipe with S3</td><td>6:19</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-8-dbt-for-data-engineers"></a>Week 8: dbt for Data Engineers<a class="hash-link" href="#week-8-dbt-for-data-engineers" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-8"></a>Description<a class="hash-link" href="#description-8" title="Direct link to heading">#</a></h5><p>This course introduces dbt (Data Build Tool), a SQL-first transformation workflow that allows you to transform, test, and document data directly within your data warehouse. You will learn how to set up dbt, connect it with Snowflake, create data pipelines, and implement advanced features like CI/CD and documentation generation. This training is ideal for data engineers looking to build trusted datasets for reporting, machine learning, and operational workflows.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/dbt-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-8"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-8" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>dbt Introduction &amp; Setup</strong></td><td>Modern data experience</td><td>5:42</td></tr><tr><td></td><td>Introduction to dbt</td><td>4:38</td></tr><tr><td></td><td>Goals of this course</td><td>4:50</td></tr><tr><td></td><td>Snowflake preparation</td><td>7:29</td></tr><tr><td></td><td>Loading data into Snowflake</td><td>4:48</td></tr><tr><td></td><td>Setup dbt Core</td><td>9:35</td></tr><tr><td></td><td>Preparing the GitHub repository</td><td>3:32</td></tr><tr><td><strong>Working with dbt-Core</strong></td><td>dbt models &amp; materialization explained</td><td>6:16</td></tr><tr><td></td><td>Creating your first SQL model</td><td>5:48</td></tr><tr><td></td><td>Working with custom schemas</td><td>5:28</td></tr><tr><td></td><td>Creating your first Python model</td><td>4:35</td></tr><tr><td></td><td>dbt sources</td><td>1:55</td></tr><tr><td></td><td>Configuring sources</td><td>4:03</td></tr><tr><td></td><td>Working with seed files</td><td>4:20</td></tr><tr><td><strong>Tests in dbt</strong></td><td>Generic tests</td><td>3:19</td></tr><tr><td></td><td>Tests with Great Expectations</td><td>3:25</td></tr><tr><td></td><td>Writing custom generic tests</td><td>2:49</td></tr><tr><td><strong>Working with dbt-Cloud</strong></td><td>dbt cloud setup</td><td>7:25</td></tr><tr><td></td><td>Creating dbt jobs</td><td>5:14</td></tr><tr><td></td><td>CI/CD automation with dbt cloud and GitHub</td><td>10:52</td></tr><tr><td></td><td>Documentation in dbt</td><td>7:38</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-9-apache-airflow-workflow-orchestration"></a>Week 9: Apache Airflow Workflow Orchestration<a class="hash-link" href="#week-9-apache-airflow-workflow-orchestration" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-9"></a>Description<a class="hash-link" href="#description-9" title="Direct link to heading">#</a></h5><p>Airflow is a platform-independent workflow orchestration tool that offers many possibilities to create and monitor stream and batch pipeline processes. It supports complex, multi-stage processes across major platforms and tools in the data engineering world, such as AWS or Google Cloud. Airflow is not only great for planning and organizing your processes but also provides robust monitoring capabilities, allowing you to keep track of data workflows and troubleshoot effectively.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/learn-apache-airflow" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-course-curriculum-9"></a>Detailed Course Curriculum<a class="hash-link" href="#detailed-course-curriculum-9" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Airflow Workflow Orchestration</strong></td><td>Airflow Usage</td><td>3:19</td></tr><tr><td><strong>Airflow Fundamental Concepts</strong></td><td>Fundamental Concepts</td><td>2:47</td></tr><tr><td></td><td>Airflow Architecture</td><td>3:09</td></tr><tr><td></td><td>Example Pipelines</td><td>4:49</td></tr><tr><td></td><td>Spotlight 3rd Party Operators</td><td>2:17</td></tr><tr><td></td><td>Airflow XComs</td><td>4:32</td></tr><tr><td><strong>Hands-On Setup</strong></td><td>Project Setup</td><td>1:43</td></tr><tr><td></td><td>Docker Setup Explained</td><td>2:06</td></tr><tr><td></td><td>Docker Compose &amp; Starting Containers</td><td>4:23</td></tr><tr><td></td><td>Checking Services</td><td>1:48</td></tr><tr><td></td><td>Setup WeatherAPI</td><td>1:33</td></tr><tr><td></td><td>Setup Postgres DB</td><td>1:58</td></tr><tr><td><strong>Learn Creating DAGs</strong></td><td>Airflow Webinterface</td><td>4:37</td></tr><tr><td></td><td>Creating DAG With Airflow 2.0</td><td>9:46</td></tr><tr><td></td><td>Running our DAG</td><td>4:15</td></tr><tr><td></td><td>Creating DAG With TaskflowAPI</td><td>6:59</td></tr><tr><td></td><td>Getting Data From the API With SimpleHTTPOperator</td><td>3:38</td></tr><tr><td></td><td>Writing into Postgres</td><td>4:12</td></tr><tr><td></td><td>Parallel Processing</td><td>4:15</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-10--11-end-to-end-project-on-aws-azure-or-gcp"></a>Week 10 &amp; 11: End-to-End Project on AWS, Azure, or GCP<a class="hash-link" href="#week-10--11-end-to-end-project-on-aws-azure-or-gcp" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="important-choose-one-project"></a>Important: Choose One Project<a class="hash-link" href="#important-choose-one-project" title="Direct link to heading">#</a></h5><p>Participants need to select <strong>one</strong> of the following cloud platforms to complete their end-to-end data engineering project. It is not necessary to complete all three projects.</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aws-project-introduction"></a>AWS Project Introduction<a class="hash-link" href="#aws-project-introduction" title="Direct link to heading">#</a></h5><p>The AWS project is designed for those who want to get started with cloud platforms, particularly with Amazon Web Services, the leading platform in data processing. This project will guide you through setting up an end-to-end data engineering pipeline using AWS tools like Lambda, API Gateway, Glue, Redshift, Kinesis, and DynamoDB. You will work with an e-commerce dataset, learn data modeling, and implement both stream and batch processing pipelines.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-aws" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-aws-project-curriculum"></a>Detailed AWS Project Curriculum<a class="hash-link" href="#detailed-aws-project-curriculum" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>Data Engineering</td><td>4:15</td></tr><tr><td></td><td>Data Science Platform</td><td>5:20</td></tr><tr><td><strong>The Dataset</strong></td><td>Data Types You Encounter</td><td>3:03</td></tr><tr><td></td><td>What Is A Good Dataset</td><td>2:54</td></tr><tr><td></td><td>The Dataset We Use</td><td>3:16</td></tr><tr><td></td><td>Defining The Purpose</td><td>6:27</td></tr><tr><td></td><td>Relational Storage Possibilities</td><td>3:46</td></tr><tr><td></td><td>NoSQL Storage Possibilities</td><td>6:28</td></tr><tr><td><strong>Platform Design</strong></td><td>Selecting The Tools</td><td>3:49</td></tr><tr><td></td><td>Client</td><td>3:05</td></tr><tr><td></td><td>Connect</td><td>1:18</td></tr><tr><td></td><td>Buffer</td><td>1:28</td></tr><tr><td></td><td>Process</td><td>2:42</td></tr><tr><td></td><td>Store</td><td>3:41</td></tr><tr><td></td><td>Visualize</td><td>3:00</td></tr><tr><td><strong>Data Pipelines</strong></td><td>Data Ingestion Pipeline</td><td>3:00</td></tr><tr><td></td><td>Stream To Raw Storage Pipeline</td><td>2:19</td></tr><tr><td></td><td>Stream To DynamoDB Pipeline</td><td>3:09</td></tr><tr><td></td><td>Visualization API Pipeline</td><td>2:56</td></tr><tr><td></td><td>Visualization Redshift Data Warehouse Pipeline</td><td>5:29</td></tr><tr><td></td><td>Batch Processing Pipeline</td><td>3:19</td></tr><tr><td><strong>AWS Basics</strong></td><td>Create An AWS Account</td><td>1:58</td></tr><tr><td></td><td>Things To Keep In Mind</td><td>2:45</td></tr><tr><td></td><td>IAM Identity &amp; Access Management</td><td>4:06</td></tr><tr><td></td><td>Logging</td><td>2:22</td></tr><tr><td></td><td>AWS Python API Boto3</td><td>2:57</td></tr><tr><td><strong>Data Ingestion Pipeline</strong></td><td>Development Environment</td><td>4:02</td></tr><tr><td></td><td>Create Lambda for API</td><td>2:33</td></tr><tr><td></td><td>Create API Gateway</td><td>8:30</td></tr><tr><td></td><td>Setup Kinesis</td><td>1:38</td></tr><tr><td></td><td>Setup IAM for API</td><td>5:00</td></tr><tr><td></td><td>Create Ingestion Pipeline (Code)</td><td>6:09</td></tr><tr><td></td><td>Create Script to Send Data</td><td>5:46</td></tr><tr><td></td><td>Test The Pipeline</td><td>4:53</td></tr><tr><td><strong>Stream To Raw S3 Storage Pipeline</strong></td><td>Setup S3 Bucket</td><td>3:42</td></tr><tr><td></td><td>Configure IAM For S3</td><td>3:21</td></tr><tr><td></td><td>Create Lambda For S3 Insert</td><td>7:16</td></tr><tr><td></td><td>Test The Pipeline</td><td>4:01</td></tr><tr><td><strong>Stream To DynamoDB Pipeline</strong></td><td>Setup DynamoDB</td><td>9:00</td></tr><tr><td></td><td>Setup IAM For DynamoDB Stream</td><td>3:36</td></tr><tr><td></td><td>Create DynamoDB Lambda</td><td>9:20</td></tr><tr><td><strong>Visualization API</strong></td><td>Create API &amp; Lambda For Access</td><td>6:10</td></tr><tr><td></td><td>Test The API</td><td>4:47</td></tr><tr><td><strong>Visualization Pipeline Redshift Data Warehouse</strong></td><td>Setup Redshift Data Warehouse</td><td>8:08</td></tr><tr><td></td><td>Security Group For Firehose</td><td>3:12</td></tr><tr><td></td><td>Create Redshift Tables</td><td>5:51</td></tr><tr><td></td><td>S3 Bucket &amp; jsonpaths.json</td><td>3:02</td></tr><tr><td></td><td>Configure Firehose</td><td>7:58</td></tr><tr><td></td><td>Debug Redshift Streaming</td><td>7:43</td></tr><tr><td></td><td>Bug-fixing</td><td>5:58</td></tr><tr><td></td><td>Power BI</td><td>12:16</td></tr><tr><td><strong>Batch Processing Pipeline</strong></td><td>AWS Glue Basics</td><td>5:14</td></tr><tr><td></td><td>Glue Crawlers</td><td>13:09</td></tr><tr><td></td><td>Glue Jobs</td><td>13:43</td></tr><tr><td></td><td>Redshift Insert &amp; Debugging</td><td>7:16</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="azure-project-introduction"></a>Azure Project Introduction<a class="hash-link" href="#azure-project-introduction" title="Direct link to heading">#</a></h5><p>The Azure project is designed for those who want to build a streaming data pipeline using Microsoft Azure&#x27;s robust cloud platform. This project introduces you to Azure services such as APIM, Blob Storage, Azure Functions, Cosmos DB, and Power BI. You will gain practical experience by building a pipeline that ingests, processes, stores, and visualizes data, using Python and Visual Studio Code.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/build-streaming-data-pipelines-in-azure" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-azure-project-curriculum"></a>Detailed Azure Project Curriculum<a class="hash-link" href="#detailed-azure-project-curriculum" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Project Introduction</strong></td><td>Data Engineering in Azure - Streaming Data Pipelines</td><td>2:43</td></tr><tr><td><strong>Datasets and Local Preprocessing</strong></td><td>Introduction to Datasets and Local Preprocessing</td><td>7:06</td></tr><tr><td></td><td>Deploying Code on Visual Studio to Docker Containers</td><td>5:27</td></tr><tr><td><strong>Azure Functions and Blob Storage</strong></td><td>Develop Azure Functions via Python and VS Code</td><td>5:52</td></tr><tr><td></td><td>Deploy Azure Function to Azure Function App and Test It</td><td>6:26</td></tr><tr><td></td><td>Integrate Azure Function with Blob Storage via Bindings</td><td>4:58</td></tr><tr><td><strong>Add Azure Function to Azure API Management (APIM)</strong></td><td>Expose Azure Function as a Backend</td><td>7:05</td></tr><tr><td></td><td>Securely Store Secrets in Azure Key Vault</td><td>4:41</td></tr><tr><td></td><td>Add Basic Authentication in API Management</td><td>4:35</td></tr><tr><td></td><td>Test APIM and Imported Azure Function via Local Python Program</td><td>2:34</td></tr><tr><td><strong>Create and Combine Event Hubs, Azure Function, and Cosmos DB</strong></td><td>Create Event Hubs and Test Capture Events Feature</td><td>6:59</td></tr><tr><td></td><td>Modify Existing Azure Function to Include Event Hubs Binding</td><td>6:42</td></tr><tr><td><strong>Write Tweets to Cosmos DB (Core SQL) from Event Hub</strong></td><td>Create a Cosmos DB (Core SQL)</td><td>9:03</td></tr><tr><td></td><td>Create a New Azure Function that Writes Messages to Cosmos DB</td><td>9:03</td></tr><tr><td><strong>Connect Power BI Desktop to Your Cosmos DB</strong></td><td>Connect Power BI Desktop via Connector and Create a Dashboard</td><td>6:32</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gcp-project-introduction"></a>GCP Project Introduction<a class="hash-link" href="#gcp-project-introduction" title="Direct link to heading">#</a></h5><p>The GCP project is designed for those who want to learn how to build, manage, and optimize data pipelines on Google Cloud Platform. This project focuses on building an end-to-end pipeline that extracts data from an external weather API, processes it through GCP&#x27;s data tools, and visualizes the results using Looker Studio. This project offers practical, hands-on experience with tools like Cloud SQL, Compute Engine, Cloud Functions, Pub/Sub, and Looker Studio.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-gcp" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-gcp-project-curriculum"></a>Detailed GCP Project Curriculum<a class="hash-link" href="#detailed-gcp-project-curriculum" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Introduction</strong></td><td>Introduction</td><td>1:13</td></tr><tr><td></td><td>GitHub &amp; the Team</td><td>1:30</td></tr><tr><td><strong>Data &amp; Goals</strong></td><td>Architecture of the Project</td><td>3:19</td></tr><tr><td></td><td>Introduction to Weather API</td><td>2:18</td></tr><tr><td></td><td>Setup Google Cloud Account</td><td>2:12</td></tr><tr><td><strong>Project Setup</strong></td><td>Creating the Project</td><td>2:35</td></tr><tr><td></td><td>Enabling the Required APIs</td><td>1:34</td></tr><tr><td></td><td>Configure Scheduling</td><td>2:20</td></tr><tr><td><strong>Pipeline Creation - Extract from API</strong></td><td>Setup VM for Database Interaction</td><td>2:53</td></tr><tr><td></td><td>Setup MySQL Database</td><td>2:16</td></tr><tr><td></td><td>Setup VM Client and Create Database</td><td>2:46</td></tr><tr><td></td><td>Creating Pub/Sub Message Queue</td><td>1:41</td></tr><tr><td></td><td>Create Cloud Function to Pull Data from API</td><td>4:17</td></tr><tr><td></td><td>Explanation of Code to Pull from API</td><td>4:20</td></tr><tr><td><strong>Pipeline Creation - Write to Database</strong></td><td>Create Function to Write to Database</td><td>7:47</td></tr><tr><td></td><td>Explanation of Code to Write Data to Database</td><td>5:56</td></tr><tr><td></td><td>Testing the Function</td><td>5:51</td></tr><tr><td></td><td>Create Function Write Data to DB - Pull</td><td>3:53</td></tr><tr><td></td><td>Explanation Code Write Data to DB - Pull</td><td>4:33</td></tr><tr><td><strong>Visualization</strong></td><td>Setup Looker Studio and Create Bubble Chart</td><td>2:20</td></tr><tr><td></td><td>Setup Looker Studio and Create Time Series Chart</td><td>1:57</td></tr><tr><td></td><td>Pipeline Monitoring</td><td>6:20</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="whats-next"></a>What’s Next?<a class="hash-link" href="#whats-next" title="Direct link to heading">#</a></h5><p>After completing this roadmap, you’ll have the confidence and skills to not just analyze data but to engineer and optimize it like a pro! Explore advanced topics, start contributing to projects, and showcase your new skills to potential employers.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="roadmap-for-data-analysts"></a>Roadmap for Data Analysts<a class="hash-link" href="#roadmap-for-data-analysts" title="Direct link to heading">#</a></h3><p>Start this roadmap at my Academy: <a href="https://learndataengineering.com/p/data-engineering-for-data-analysts" target="_blank" rel="noopener noreferrer">Start Today</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="go-beyond-sql-and-learn-how-to-build-automate-and-optimize-data-pipelines-like-an-engineer"></a>Go Beyond SQL and Learn How to Build, Automate, and Optimize Data Pipelines Like an Engineer<a class="hash-link" href="#go-beyond-sql-and-learn-how-to-build-automate-and-optimize-data-pipelines-like-an-engineer" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="who-is-this-10-week-roadmap-for"></a>Who Is This 10 Week Roadmap For?<a class="hash-link" href="#who-is-this-10-week-roadmap-for" title="Direct link to heading">#</a></h4><ul><li>Data Analysts who want to understand the full data lifecycle</li><li>Those looking to move beyond SQL and build real data pipelines</li><li>Professionals seeking hands-on, practical experience to boost their resumes</li><li>Anyone wanting to stay competitive in the job market</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-youll-achieve"></a>What You’ll Achieve<a class="hash-link" href="#what-youll-achieve" title="Direct link to heading">#</a></h4><p>This roadmap provides a step-by-step approach to mastering data engineering skills. You&#x27;ll start with Python and data modeling, move on to building pipelines, work with cloud platforms, and finally automate workflows using industry-standard tools.</p><p><img alt="Building blocks of your curriculum" src="/assets/images/Roadmap-From-Data-Analyst-to-Engineer-34c0e571a57c231c82f401367bb225ce.jpg"></p><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="learning-goals-1"></a>Learning Goals<a class="hash-link" href="#learning-goals-1" title="Direct link to heading">#</a></h4><table><thead><tr><th>Goal</th><th>Description</th></tr></thead><tbody><tr><td><strong>Goal #1</strong></td><td>Master Python &amp; Relational Data Modeling</td></tr><tr><td><strong>Goal #2</strong></td><td>Build Your First ETL Pipeline on AWS (or Azure/GCP)</td></tr><tr><td><strong>Goal #3</strong></td><td>Gain Hands-On Experience with Snowflake &amp; dbt</td></tr><tr><td><strong>Goal #4</strong></td><td>Connect AWS and Snowflake</td></tr><tr><td><strong>Goal #5</strong></td><td>Automate Your Data Pipeline with Airflow</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="10-week-learning-roadmap"></a>10-Week Learning Roadmap<a class="hash-link" href="#10-week-learning-roadmap" title="Direct link to heading">#</a></h4><table><thead><tr><th>Week</th><th>Topic</th><th>Key Learning Outcomes</th></tr></thead><tbody><tr><td><strong>Week 1</strong></td><td>Introduction to Data Engineering &amp; Python</td><td>Understand core concepts of data engineering and Python programming basics</td></tr><tr><td><strong>Week 2</strong></td><td>Platform &amp; Pipeline Design</td><td>Learn how to design effective data platforms and pipelines</td></tr><tr><td><strong>Week 3</strong></td><td>Relational Data Modeling</td><td>Develop skills in creating relational data models for structured data</td></tr><tr><td><strong>Week 4</strong></td><td>Dimensional Data Modeling</td><td>Master the techniques of dimensional modeling for analytics and reporting</td></tr><tr><td><strong>Week 5</strong></td><td>Docker Fundamentals &amp; APIs</td><td>Get hands-on with containerization using Docker and working with APIs</td></tr><tr><td><strong>Week 8</strong></td><td>Working with Snowflake</td><td>Gain practical experience using Snowflake as a data warehouse</td></tr><tr><td><strong>Week 9</strong></td><td>Transforming Data With dbt</td><td>Learn to transform and model data efficiently using dbt</td></tr><tr><td><strong>Week 10</strong></td><td>Pipeline Orchestration with Airflow</td><td>Automate and manage data workflows using Apache Airflow</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-weekly-content"></a>Detailed Weekly Content<a class="hash-link" href="#detailed-weekly-content" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-1-introduction-to-data-engineering--python"></a>Week 1: Introduction to Data Engineering &amp; Python<a class="hash-link" href="#week-1-introduction-to-data-engineering--python" title="Direct link to heading">#</a></h4><p>If you want to take your data engineering skills to the next level, you are in the right place. Python has become the go-to language for data analysis and machine learning, and with our training, you will learn how to successfully use Python to build robust data pipelines and manipulate data efficiently.</p><p>This comprehensive training program is designed for data engineers of all levels. Whether you are just starting out in data engineering or you are an experienced engineer looking to expand your skill set, our Python for Data Engineers training will give you the tools you need to excel in your field.</p><p>At the end of the training, you will have a strong foundation in Python and data engineering and be ready to tackle complex data engineering projects with ease.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/python-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum"></a>Course Curriculum<a class="hash-link" href="#course-curriculum" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Classes</td><td>4:37</td></tr><tr><td>Modules</td><td>3:06</td></tr><tr><td>Exception Handling</td><td>8:55</td></tr><tr><td>Logging</td><td>5:12</td></tr><tr><td>Datetime</td><td>8:04</td></tr><tr><td>JSON</td><td>9:54</td></tr><tr><td>JSON Validation</td><td>15:10</td></tr><tr><td>UnitTesting</td><td>16:44</td></tr><tr><td>Pandas: Intro &amp; data types</td><td>8:43</td></tr><tr><td>Pandas: Appending &amp; Merging DataFrames</td><td>7:49</td></tr><tr><td>Pandas: Normalizing &amp; Lambdas</td><td>4:12</td></tr><tr><td>Pandas: Pivot &amp; Parquet write, read</td><td>6:17</td></tr><tr><td>Pandas: Melting &amp; JSON normalization</td><td>8:15</td></tr><tr><td>Numpy</td><td>4:47</td></tr><tr><td>Requests (Working with APIs)</td><td>11:15</td></tr><tr><td>Working with Databases: Setup</td><td>4:06</td></tr><tr><td>Working with Databases: Tables, bulk load, queries</td><td>8:12</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-2-platform--pipeline-design"></a>Week 2: Platform &amp; Pipeline Design<a class="hash-link" href="#week-2-platform--pipeline-design" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-10"></a>Description<a class="hash-link" href="#description-10" title="Direct link to heading">#</a></h5><p>Data pipelines are the number one thing within the Data Science platform. Without them, data ingestion or machine learning processing, for example, would not be possible.</p><p>This 110-minute long training will help you understand how to create stream and batch processing pipelines as well as machine learning pipelines by going through some of the most essential basics - complemented by templates and examples for useful cloud computing platforms.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-pipeline-design" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-1"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-1" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Platform Blueprint &amp; End to End Pipeline Example</td><td>10:11</td></tr><tr><td>Data Engineering Tools Guide</td><td>2:44</td></tr><tr><td>End to End Pipeline Example</td><td>6:18</td></tr><tr><td>Push Ingestion Pipelines</td><td>3:42</td></tr><tr><td>Pull Ingestion Pipelines</td><td>3:34</td></tr><tr><td>Batch Pipelines</td><td>3:07</td></tr><tr><td>Streaming Pipelines</td><td>3:34</td></tr><tr><td>Stream Analytics</td><td>2:26</td></tr><tr><td>Lambda Architecture</td><td>4:02</td></tr><tr><td>Visualization Pipelines</td><td>3:47</td></tr><tr><td>Visualization with Hive &amp; Spark on Hadoop</td><td>6:21</td></tr><tr><td>Visualization Data via Spark Thrift Server</td><td>3:27</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-3-relational-data-modeling"></a>Week 3: Relational Data Modeling<a class="hash-link" href="#week-3-relational-data-modeling" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-11"></a>Description<a class="hash-link" href="#description-11" title="Direct link to heading">#</a></h5><p>Relational modeling is often used for building transactional databases. You might say, &#x27;But I&#x27;m not planning to become a back-end engineer&#x27;. Apart from knowing how to move data, you should also know how to store it effectively which involves designing a scalable data model optimized to drive faster query response time and efficiently retrieve data.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/relational-data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-2"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-2" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Relational Data Models History</td><td>3:16</td></tr><tr><td>Installing MySQL Server and MySQL Workbench</td><td>8:04</td></tr><tr><td>MySQL Workbench Introduction</td><td>4:36</td></tr><tr><td>The Design Process Explained</td><td>4:14</td></tr><tr><td>Discover the Entities</td><td>10:24</td></tr><tr><td>Discover the Attributes</td><td>13:09</td></tr><tr><td>Define Entity Relationships and Normalize the Data</td><td>11:19</td></tr><tr><td>Identifying vs Non-identifying Relationships</td><td>2:01</td></tr><tr><td>Resolve Many-to-Many Relationships</td><td>4:00</td></tr><tr><td>Resolve One-to-Many Relationships</td><td>2:34</td></tr><tr><td>Resolve One-to-One Relationships</td><td>1:45</td></tr><tr><td>Create ER Diagram Using Workbench</td><td>19:46</td></tr><tr><td>Create a Physical Data Model</td><td>4:13</td></tr><tr><td>Populate MySQL DB with Data from .xls File</td><td>15:13</td></tr><tr><td>Course Conclusion</td><td>1:28</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-4-dimensional-data-modeling"></a>Week 4: Dimensional Data Modeling<a class="hash-link" href="#week-4-dimensional-data-modeling" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-12"></a>Description<a class="hash-link" href="#description-12" title="Direct link to heading">#</a></h5><p>In today’s data-driven world, efficient data organization is key to enabling insightful analysis and reporting. Dimensional data modeling is a crucial technique that helps structure your data for faster querying and better decision-making.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-modeling-3-dimensional-data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-3"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-3" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Intro to Data Warehousing</td><td>6:42</td></tr><tr><td>Approaches to Building a Data Warehouse</td><td>5:20</td></tr><tr><td>Dimension Tables Explained</td><td>5:34</td></tr><tr><td>Fact Tables Explained</td><td>6:34</td></tr><tr><td>Identifying Dimensions</td><td>3:16</td></tr><tr><td>What is DuckDB</td><td>5:58</td></tr><tr><td>First DuckDB Hands-on</td><td>2:20</td></tr><tr><td>Creating Tables in DuckDB</td><td>2:40</td></tr><tr><td>Installing DBeaver</td><td>6:49</td></tr><tr><td>Exploring SCD0 and SCD1</td><td>19:57</td></tr><tr><td>Exploring SCD2</td><td>13:52</td></tr><tr><td>Exploring Transaction Fact Table</td><td>6:28</td></tr><tr><td>Exploring Accumulating Fact Table</td><td>7:17</td></tr><tr><td>Course Conclusion</td><td>0:52</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-5-docker-fundamentals--apis"></a>Week 5: Docker Fundamentals &amp; APIs<a class="hash-link" href="#week-5-docker-fundamentals--apis" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-13"></a>Description<a class="hash-link" href="#description-13" title="Direct link to heading">#</a></h5><p>Week 5 covers two crucial topics: containerization using Docker and building APIs with FastAPI. Docker is essential for creating lightweight, self-sustained containers, while APIs are the backbone of data platforms.</p><p>Check out Docker Fundamentals in my Academy: <a href="https://learndataengineering.com/p/docker-fundamentals" target="_blank" rel="noopener noreferrer">Learn More</a></p><p>Check out Building APIs with FastAPI in my Academy: <a href="https://learndataengineering.com/p/apis-with-fastapi-course" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-4"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-4" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="docker-fundamentals"></a>Docker Fundamentals<a class="hash-link" href="#docker-fundamentals" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Docker vs Virtual Machines</td><td>6:23</td></tr><tr><td>Docker Terminology</td><td>5:56</td></tr><tr><td>Installing Docker Desktop</td><td>4:09</td></tr><tr><td>Pulling Images &amp; Running Containers</td><td>6:34</td></tr><tr><td>Docker Compose</td><td>6:34</td></tr><tr><td>Build &amp; Run Simple Image</td><td>6:28</td></tr><tr><td>Build Image with Dependencies</td><td>5:05</td></tr><tr><td>Using DockerHub Image Registry</td><td>4:24</td></tr><tr><td>Image Layers &amp; Security Best Practices</td><td>7:55</td></tr><tr><td>Managing Docker with Portainer</td><td>4:04</td></tr></tbody></table><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="building-apis-with-fastapi"></a>Building APIs with FastAPI<a class="hash-link" href="#building-apis-with-fastapi" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>What are APIs?</td><td>8:29</td></tr><tr><td>Hosting vs Using APIs</td><td>4:08</td></tr><tr><td>HTTP Methods &amp; Media Types</td><td>6:56</td></tr><tr><td>API Parameters &amp; Response Codes</td><td>9:40</td></tr><tr><td>Setting up FastAPI</td><td>4:55</td></tr><tr><td>Creating APIs: POST, GET, PUT</td><td>16:18</td></tr><tr><td>Testing APIs with Postman</td><td>4:22</td></tr><tr><td>Deploying FastAPI with Docker</td><td>6:01</td></tr><tr><td>API Security Best Practices</td><td>3:48</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-6--7-end-to-end-project-on-aws-azure-or-gcp"></a>Week 6 &amp; 7: End-to-End Project on AWS, Azure, or GCP<a class="hash-link" href="#week-6--7-end-to-end-project-on-aws-azure-or-gcp" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="important-choose-one-project-1"></a>Important: Choose One Project<a class="hash-link" href="#important-choose-one-project-1" title="Direct link to heading">#</a></h5><p>Participants need to select <strong>one</strong> of the following cloud platforms to complete their end-to-end data engineering project. It is not necessary to complete all three projects.</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aws-project-introduction-1"></a>AWS Project Introduction<a class="hash-link" href="#aws-project-introduction-1" title="Direct link to heading">#</a></h5><p>The AWS project is designed for those who want to get started with cloud platforms, particularly with Amazon Web Services, the leading platform in data processing. This project will guide you through setting up an end-to-end data engineering pipeline using AWS tools like Lambda, API Gateway, Glue, Redshift, Kinesis, and DynamoDB. You will work with an e-commerce dataset, learn data modeling, and implement both stream and batch processing pipelines.</p><p>Check out this project in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-aws" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-aws-project-curriculum-1"></a>Detailed AWS Project Curriculum<a class="hash-link" href="#detailed-aws-project-curriculum-1" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>Data Engineering</td><td>4:15</td></tr><tr><td></td><td>Data Science Platform</td><td>5:20</td></tr><tr><td><strong>The Dataset</strong></td><td>Data Types You Encounter</td><td>3:03</td></tr><tr><td></td><td>What Is A Good Dataset</td><td>2:54</td></tr><tr><td></td><td>The Dataset We Use</td><td>3:16</td></tr><tr><td></td><td>Defining The Purpose</td><td>6:27</td></tr><tr><td></td><td>Relational Storage Possibilities</td><td>3:46</td></tr><tr><td></td><td>NoSQL Storage Possibilities</td><td>6:28</td></tr><tr><td><strong>Platform Design</strong></td><td>Selecting The Tools</td><td>3:49</td></tr><tr><td></td><td>Client</td><td>3:05</td></tr><tr><td></td><td>Connect</td><td>1:18</td></tr><tr><td></td><td>Buffer</td><td>1:28</td></tr><tr><td></td><td>Process</td><td>2:42</td></tr><tr><td></td><td>Store</td><td>3:41</td></tr><tr><td></td><td>Visualize</td><td>3:00</td></tr><tr><td><strong>Data Pipelines</strong></td><td>Data Ingestion Pipeline</td><td>3:00</td></tr><tr><td></td><td>Stream To Raw Storage Pipeline</td><td>2:19</td></tr><tr><td></td><td>Stream To DynamoDB Pipeline</td><td>3:09</td></tr><tr><td></td><td>Visualization API Pipeline</td><td>2:56</td></tr><tr><td></td><td>Visualization Redshift Data Warehouse Pipeline</td><td>5:29</td></tr><tr><td></td><td>Batch Processing Pipeline</td><td>3:19</td></tr><tr><td><strong>AWS Basics</strong></td><td>Create An AWS Account</td><td>1:58</td></tr><tr><td></td><td>Things To Keep In Mind</td><td>2:45</td></tr><tr><td></td><td>IAM Identity &amp; Access Management</td><td>4:06</td></tr><tr><td></td><td>Logging</td><td>2:22</td></tr><tr><td></td><td>AWS Python API Boto3</td><td>2:57</td></tr><tr><td><strong>Data Ingestion Pipeline</strong></td><td>Development Environment</td><td>4:02</td></tr><tr><td></td><td>Create Lambda for API</td><td>2:33</td></tr><tr><td></td><td>Create API Gateway</td><td>8:30</td></tr><tr><td></td><td>Setup Kinesis</td><td>1:38</td></tr><tr><td></td><td>Setup IAM for API</td><td>5:00</td></tr><tr><td></td><td>Create Ingestion Pipeline (Code)</td><td>6:09</td></tr><tr><td></td><td>Create Script to Send Data</td><td>5:46</td></tr><tr><td></td><td>Test The Pipeline</td><td>4:53</td></tr><tr><td><strong>Stream To Raw S3 Storage Pipeline</strong></td><td>Setup S3 Bucket</td><td>3:42</td></tr><tr><td></td><td>Configure IAM For S3</td><td>3:21</td></tr><tr><td></td><td>Create Lambda For S3 Insert</td><td>7:16</td></tr><tr><td></td><td>Test The Pipeline</td><td>4:01</td></tr><tr><td><strong>Stream To DynamoDB Pipeline</strong></td><td>Setup DynamoDB</td><td>9:00</td></tr><tr><td></td><td>Setup IAM For DynamoDB Stream</td><td>3:36</td></tr><tr><td></td><td>Create DynamoDB Lambda</td><td>9:20</td></tr><tr><td><strong>Visualization API</strong></td><td>Create API &amp; Lambda For Access</td><td>6:10</td></tr><tr><td></td><td>Test The API</td><td>4:47</td></tr><tr><td><strong>Visualization Pipeline Redshift Data Warehouse</strong></td><td>Setup Redshift Data Warehouse</td><td>8:08</td></tr><tr><td></td><td>Security Group For Firehose</td><td>3:12</td></tr><tr><td></td><td>Create Redshift Tables</td><td>5:51</td></tr><tr><td></td><td>S3 Bucket &amp; jsonpaths.json</td><td>3:02</td></tr><tr><td></td><td>Configure Firehose</td><td>7:58</td></tr><tr><td></td><td>Debug Redshift Streaming</td><td>7:43</td></tr><tr><td></td><td>Bug-fixing</td><td>5:58</td></tr><tr><td></td><td>Power BI</td><td>12:16</td></tr><tr><td><strong>Batch Processing Pipeline</strong></td><td>AWS Glue Basics</td><td>5:14</td></tr><tr><td></td><td>Glue Crawlers</td><td>13:09</td></tr><tr><td></td><td>Glue Jobs</td><td>13:43</td></tr><tr><td></td><td>Redshift Insert &amp; Debugging</td><td>7:16</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="azure-project-introduction-1"></a>Azure Project Introduction<a class="hash-link" href="#azure-project-introduction-1" title="Direct link to heading">#</a></h5><p>The Azure project is designed for those who want to build a streaming data pipeline using Microsoft Azure&#x27;s robust cloud platform. This project introduces you to Azure services such as APIM, Blob Storage, Azure Functions, Cosmos DB, and Power BI. You will gain practical experience by building a pipeline that ingests, processes, stores, and visualizes data, using Python and Visual Studio Code.</p><p>Check out this project in my Academy: <a href="https://learndataengineering.com/p/build-streaming-data-pipelines-in-azure" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-azure-project-curriculum-1"></a>Detailed Azure Project Curriculum<a class="hash-link" href="#detailed-azure-project-curriculum-1" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Project Introduction</strong></td><td>Data Engineering in Azure - Streaming Data Pipelines</td><td>2:43</td></tr><tr><td><strong>Datasets and Local Preprocessing</strong></td><td>Introduction to Datasets and Local Preprocessing</td><td>7:06</td></tr><tr><td></td><td>Deploying Code on Visual Studio to Docker Containers</td><td>5:27</td></tr><tr><td><strong>Azure Functions and Blob Storage</strong></td><td>Develop Azure Functions via Python and VS Code</td><td>5:52</td></tr><tr><td></td><td>Deploy Azure Function to Azure Function App and Test It</td><td>6:26</td></tr><tr><td></td><td>Integrate Azure Function with Blob Storage via Bindings</td><td>4:58</td></tr><tr><td><strong>Add Azure Function to Azure API Management (APIM)</strong></td><td>Expose Azure Function as a Backend</td><td>7:05</td></tr><tr><td></td><td>Securely Store Secrets in Azure Key Vault</td><td>4:41</td></tr><tr><td></td><td>Add Basic Authentication in API Management</td><td>4:35</td></tr><tr><td></td><td>Test APIM and Imported Azure Function via Local Python Program</td><td>2:34</td></tr><tr><td><strong>Create and Combine Event Hubs, Azure Function, and Cosmos DB</strong></td><td>Create Event Hubs and Test Capture Events Feature</td><td>6:59</td></tr><tr><td></td><td>Modify Existing Azure Function to Include Event Hubs Binding</td><td>6:42</td></tr><tr><td><strong>Write Tweets to Cosmos DB (Core SQL) from Event Hub</strong></td><td>Create a Cosmos DB (Core SQL)</td><td>9:03</td></tr><tr><td></td><td>Create a New Azure Function that Writes Messages to Cosmos DB</td><td>9:03</td></tr><tr><td><strong>Connect Power BI Desktop to Your Cosmos DB</strong></td><td>Connect Power BI Desktop via Connector and Create a Dashboard</td><td>6:32</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gcp-project-introduction-1"></a>GCP Project Introduction<a class="hash-link" href="#gcp-project-introduction-1" title="Direct link to heading">#</a></h5><p>The GCP project is designed for those who want to learn how to build, manage, and optimize data pipelines on Google Cloud Platform. This project focuses on building an end-to-end pipeline that extracts data from an external weather API, processes it through GCP&#x27;s data tools, and visualizes the results using Looker Studio. This project offers practical, hands-on experience with tools like Cloud SQL, Compute Engine, Cloud Functions, Pub/Sub, and Looker Studio.</p><p>Check out this project in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-gcp" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="detailed-gcp-project-curriculum-1"></a>Detailed GCP Project Curriculum<a class="hash-link" href="#detailed-gcp-project-curriculum-1" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Introduction</strong></td><td>Introduction</td><td>1:13</td></tr><tr><td></td><td>GitHub &amp; the Team</td><td>1:30</td></tr><tr><td><strong>Data &amp; Goals</strong></td><td>Architecture of the Project</td><td>3:19</td></tr><tr><td></td><td>Introduction to Weather API</td><td>2:18</td></tr><tr><td></td><td>Setup Google Cloud Account</td><td>2:12</td></tr><tr><td><strong>Project Setup</strong></td><td>Creating the Project</td><td>2:35</td></tr><tr><td></td><td>Enabling the Required APIs</td><td>1:34</td></tr><tr><td></td><td>Configure Scheduling</td><td>2:20</td></tr><tr><td><strong>Pipeline Creation - Extract from API</strong></td><td>Setup VM for Database Interaction</td><td>2:53</td></tr><tr><td></td><td>Setup MySQL Database</td><td>2:16</td></tr><tr><td></td><td>Setup VM Client and Create Database</td><td>2:46</td></tr><tr><td></td><td>Creating Pub/Sub Message Queue</td><td>1:41</td></tr><tr><td></td><td>Create Cloud Function to Pull Data from API</td><td>4:17</td></tr><tr><td></td><td>Explanation of Code to Pull from API</td><td>4:20</td></tr><tr><td><strong>Pipeline Creation - Write to Database</strong></td><td>Create Function to Write to Database</td><td>7:47</td></tr><tr><td></td><td>Explanation of Code to Write Data to Database</td><td>5:56</td></tr><tr><td></td><td>Testing the Function</td><td>5:51</td></tr><tr><td></td><td>Create Function Write Data to DB - Pull</td><td>3:53</td></tr><tr><td></td><td>Explanation Code Write Data to DB - Pull</td><td>4:33</td></tr><tr><td><strong>Visualization</strong></td><td>Setup Looker Studio and Create Bubble Chart</td><td>2:20</td></tr><tr><td></td><td>Setup Looker Studio and Create Time Series Chart</td><td>1:57</td></tr><tr><td></td><td>Pipeline Monitoring</td><td>6:20</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-8-working-with-snowflake"></a>Week 8: Working with Snowflake<a class="hash-link" href="#week-8-working-with-snowflake" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-14"></a>Description<a class="hash-link" href="#description-14" title="Direct link to heading">#</a></h5><p>Currently, Snowflake is the analytics store/data warehouse everybody is talking about. It is a 100% cloud-based platform that offers many advantages, including flexible data access and the ability to scale services as needed. Snowflake is widely used in the industry, and learning it will enhance your data engineering skill set.</p><p>This training covers everything from the basics of Snowflake and data warehousing to advanced integration and automation techniques. By the end, you will have the knowledge to prepare, integrate, manage data on Snowflake, and connect other systems to the platform.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/snowflake-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-5"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-5" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td></td><td>Snowflake Basics</td><td>4:16</td></tr><tr><td></td><td>Data Warehousing Basics</td><td>4:13</td></tr><tr><td></td><td>How Snowflake Fits into Data Platforms</td><td>3:14</td></tr><tr><td><strong>Setup</strong></td><td>Snowflake Account Setup</td><td>4:24</td></tr><tr><td></td><td>Creating Your Warehouse &amp; UI Overview</td><td>4:15</td></tr><tr><td><strong>Loading CSVs from Your PC</strong></td><td>Our Dataset &amp; Goals</td><td>3:01</td></tr><tr><td></td><td>Setup Snowflake Database</td><td>10:29</td></tr><tr><td></td><td>Preparing the Upload File</td><td>8:31</td></tr><tr><td></td><td>Using Internal Stages with SnowSQL</td><td>12:37</td></tr><tr><td></td><td>Splitting a Data Table into Two Tables</td><td>6:38</td></tr><tr><td><strong>Visualizing Data</strong></td><td>Creating a Visualization Worksheet</td><td>7:08</td></tr><tr><td></td><td>Creating a Dashboard</td><td>5:23</td></tr><tr><td></td><td>Connect PowerBI to Snowflake</td><td>6:03</td></tr><tr><td></td><td>Query Data with Python</td><td>7:35</td></tr><tr><td><strong>Automation</strong></td><td>Create Import Task</td><td>9:18</td></tr><tr><td></td><td>Create Table Refresh Task</td><td>3:40</td></tr><tr><td></td><td>Test Our Pipeline</td><td>3:14</td></tr><tr><td><strong>AWS S3 Integration</strong></td><td>Working with External Stages for AWS S3</td><td>10:20</td></tr><tr><td></td><td>Implementing Snowpipe with S3</td><td>6:19</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-9-transforming-data-with-dbt"></a>Week 9: Transforming Data With dbt<a class="hash-link" href="#week-9-transforming-data-with-dbt" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-15"></a>Description<a class="hash-link" href="#description-15" title="Direct link to heading">#</a></h5><p>dbt is a SQL-first transformation workflow that simplifies the process of transforming, testing, and documenting data. It allows teams to work directly within the data warehouse, creating trusted datasets for reporting, machine learning, and operational workflows. This training is the perfect starting point to get hands-on experience with dbt Core, dbt Cloud, and Snowflake.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/dbt-for-data-engineers" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-6"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-6" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>dbt Introduction &amp; Setup</strong></td><td>Modern Data Experience</td><td>5:42</td></tr><tr><td></td><td>Introduction to dbt</td><td>4:38</td></tr><tr><td></td><td>Goals of this Course</td><td>4:50</td></tr><tr><td></td><td>Snowflake Preparation</td><td>7:29</td></tr><tr><td></td><td>Loading Data into Snowflake</td><td>4:48</td></tr><tr><td></td><td>Setup dbt Core</td><td>9:35</td></tr><tr><td></td><td>Preparing the GitHub Repository</td><td>3:32</td></tr><tr><td><strong>Working with dbt-Core</strong></td><td>dbt Models &amp; Materialization Explained</td><td>6:16</td></tr><tr><td></td><td>Creating Your First SQL Model</td><td>5:48</td></tr><tr><td></td><td>Working with Custom Schemas</td><td>5:28</td></tr><tr><td></td><td>Creating Your First Python Model</td><td>4:35</td></tr><tr><td></td><td>dbt Sources</td><td>1:55</td></tr><tr><td></td><td>Configuring Sources</td><td>4:03</td></tr><tr><td></td><td>Working with Seed Files</td><td>4:20</td></tr><tr><td><strong>Tests in dbt</strong></td><td>Generic Tests</td><td>3:19</td></tr><tr><td></td><td>Tests with Great Expectations</td><td>3:25</td></tr><tr><td></td><td>Writing Custom Generic Tests</td><td>2:49</td></tr><tr><td><strong>Working with dbt-Cloud</strong></td><td>dbt Cloud Setup</td><td>7:25</td></tr><tr><td></td><td>Creating dbt Jobs</td><td>5:14</td></tr><tr><td></td><td>CI/CD Automation with dbt Cloud and GitHub</td><td>10:52</td></tr><tr><td></td><td>Documentation in dbt</td><td>7:38</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-10-pipeline-orchestration-with-airflow"></a>Week 10: Pipeline Orchestration with Airflow<a class="hash-link" href="#week-10-pipeline-orchestration-with-airflow" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-16"></a>Description<a class="hash-link" href="#description-16" title="Direct link to heading">#</a></h5><p>Apache Airflow is a powerful, platform-independent workflow orchestration tool widely used in the data engineering world. It allows you to create and monitor both stream and batch pipeline processes with ease. Airflow supports integration with major platforms and tools such as AWS, Google Cloud, and many more.</p><p>Airflow not only helps in planning and organizing workflows but also offers robust monitoring features, allowing you to troubleshoot and maintain complex ETL pipelines effectively. As one of the most popular tools for workflow orchestration, mastering Airflow is highly valuable for data engineers.</p><p>Check out this course in my Academy: <a href="https://learndataengineering.com/p/learn-apache-airflow" target="_blank" rel="noopener noreferrer">Learn More</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-7"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-7" title="Direct link to heading">#</a></h5><table><thead><tr><th>Module</th><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td><strong>Airflow Workflow Orchestration</strong></td><td>Airflow Usage</td><td>3:19</td></tr><tr><td><strong>Airflow Fundamental Concepts</strong></td><td>Fundamental Concepts</td><td>2:47</td></tr><tr><td></td><td>Airflow Architecture</td><td>3:09</td></tr><tr><td></td><td>Example Pipelines</td><td>4:49</td></tr><tr><td></td><td>Spotlight 3rd Party Operators</td><td>2:17</td></tr><tr><td></td><td>Airflow XComs</td><td>4:32</td></tr><tr><td><strong>Hands-On Setup</strong></td><td>Project Setup</td><td>1:43</td></tr><tr><td></td><td>Docker Setup Explained</td><td>2:06</td></tr><tr><td></td><td>Docker Compose &amp; Starting Containers</td><td>4:23</td></tr><tr><td></td><td>Checking Services</td><td>1:48</td></tr><tr><td></td><td>Setup WeatherAPI</td><td>1:33</td></tr><tr><td></td><td>Setup Postgres DB</td><td>1:58</td></tr><tr><td><strong>Learn Creating DAGs</strong></td><td>Airflow Webinterface</td><td>4:37</td></tr><tr><td></td><td>Creating DAG With Airflow 2.0</td><td>9:46</td></tr><tr><td></td><td>Running our DAG</td><td>4:15</td></tr><tr><td></td><td>Creating DAG With TaskflowAPI</td><td>6:59</td></tr><tr><td></td><td>Getting Data From the API With SimpleHTTPOperator</td><td>3:38</td></tr><tr><td></td><td>Writing into Postgres</td><td>4:12</td></tr><tr><td></td><td>Parallel Processing</td><td>4:15</td></tr><tr><td><strong>Recap</strong></td><td>Recap &amp; Outlook</td><td>4:38</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="whats-next-1"></a>What’s Next?<a class="hash-link" href="#whats-next-1" title="Direct link to heading">#</a></h4><p>After completing this roadmap, you’ll have the confidence and skills to not just analyze data but to engineer and optimize it like a pro! Explore advanced topics, start contributing to projects, and showcase your new skills to potential employers.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="roadmap-for-data-scientists"></a>Roadmap for Data Scientists<a class="hash-link" href="#roadmap-for-data-scientists" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="14-week-data-engineering-roadmap-for-data-scientists"></a>14-Week Data Engineering Roadmap for Data Scientists<a class="hash-link" href="#14-week-data-engineering-roadmap-for-data-scientists" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="from-notebooks-to-production-build-deploy-and-scale-your-ml-workflows"></a>From Notebooks to Production: Build, Deploy, and Scale Your ML Workflows<a class="hash-link" href="#from-notebooks-to-production-build-deploy-and-scale-your-ml-workflows" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="start-this-roadmap-at-my-academy-start-today"></a>Start this roadmap at my Academy: <a href="https://learndataengineering.com/p/data-engineering-for-data-scientists" target="_blank" rel="noopener noreferrer">Start Today</a><a class="hash-link" href="#start-this-roadmap-at-my-academy-start-today" title="Direct link to heading">#</a></h4><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="who-is-this-roadmap-for"></a>Who Is This Roadmap For?<a class="hash-link" href="#who-is-this-roadmap-for" title="Direct link to heading">#</a></h4><ul><li>Data Scientists who want to deploy and maintain ML models in production</li><li>ML practitioners struggling with real-time data, CI/CD, and orchestration</li><li>Data professionals looking to expand their engineering toolkit</li><li>Anyone ready to go beyond notebooks and automate their ML workflows</li></ul><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-youll-achieve-1"></a>What You’ll Achieve<a class="hash-link" href="#what-youll-achieve-1" title="Direct link to heading">#</a></h4><p>This roadmap provides a step-by-step approach to gaining production-grade data engineering skills. You&#x27;ll start with pipelines and containerization, move on to deployment and orchestration, and finish with big data and monitoring.</p><p><img alt="Building blocks of your curriculum" src="/assets/images/Roadmap-Data-Engineering-For-Data-Scientists-b37b196294b3e4687f9b1383d0efa625.jpg"></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="learning-goals-2"></a>Learning Goals<a class="hash-link" href="#learning-goals-2" title="Direct link to heading">#</a></h4><table><thead><tr><th>Goal #</th><th>Description</th></tr></thead><tbody><tr><td>Goal #1</td><td>Build an End-to-End ML Pipeline on AWS</td></tr><tr><td>Goal #2</td><td>Add CI/CD &amp; Containerization to Your Platform</td></tr><tr><td>Goal #3</td><td>Implement the Lakehouse Architecture in AWS or GCP</td></tr><tr><td>Goal #4</td><td>Orchestrate Your Pipelines with Airflow</td></tr><tr><td>Goal #5</td><td>Process Big Data with Apache Spark &amp; Streaming</td></tr><tr><td>Goal #6</td><td>Analyze Your ML Training Logs with Elasticsearch</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="14-week-learning-roadmap"></a>14-Week Learning Roadmap<a class="hash-link" href="#14-week-learning-roadmap" title="Direct link to heading">#</a></h4><table><thead><tr><th>Week</th><th>Topic</th></tr></thead><tbody><tr><td>Week 1</td><td>Platform &amp; Pipeline Design</td></tr><tr><td>Week 2</td><td>Docker Fundamentals</td></tr><tr><td>Week 3</td><td>Relational Data Modeling</td></tr><tr><td>Week 4</td><td>Working &amp; Designing APIs</td></tr><tr><td>Week 5 &amp; 6</td><td>ML &amp; Containerization on AWS</td></tr><tr><td>Week 7</td><td>ETL &amp; CI/CD on AWS</td></tr><tr><td>Week 8</td><td>Building a Lakehouse on AWS or GCP</td></tr><tr><td>Week 9</td><td>Orchestrate with Airflow</td></tr><tr><td>Week 10</td><td>Pre-Process Data with Apache Spark</td></tr><tr><td>Week 11-13</td><td>Build a Streaming Pipeline (AWS, Azure, GCP)</td></tr><tr><td>Week 14</td><td>Analyze Training Logs with Elasticsearch</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-1-platform--pipeline-design"></a>Week 1: Platform &amp; Pipeline Design<a class="hash-link" href="#week-1-platform--pipeline-design" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-17"></a>Description<a class="hash-link" href="#description-17" title="Direct link to heading">#</a></h5><p>Data pipelines are the foundation of any data platform. In this 110-minute training, you&#x27;ll learn about stream, batch, and ML pipelines. You&#x27;ll also explore platform blueprints, architecture components, and Lambda architecture.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/data-pipeline-design" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-8"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-8" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Platform Blueprint &amp; End to End Pipeline Example</td><td>10:11</td></tr><tr><td>Data Engineering Tools Guide</td><td>2:44</td></tr><tr><td>End to End Pipeline Example</td><td>6:18</td></tr><tr><td>Push Ingestion Pipelines</td><td>3:42</td></tr><tr><td>Pull Ingestion Pipelines</td><td>3:34</td></tr><tr><td>Batch Pipelines</td><td>3:07</td></tr><tr><td>Streaming Pipelines</td><td>3:34</td></tr><tr><td>Stream Analytics</td><td>2:26</td></tr><tr><td>Lambda Architecture</td><td>4:02</td></tr><tr><td>Visualization Pipelines</td><td>3:47</td></tr><tr><td>Visualization with Hive &amp; Spark on Hadoop</td><td>6:21</td></tr><tr><td>Visualization Data via Spark Thrift Server</td><td>3:27</td></tr><tr><td>Platform Examples (AWS, Azure, GCP, Hadoop)</td><td>Slides Only</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-2-docker-fundamentals"></a>Week 2: Docker Fundamentals<a class="hash-link" href="#week-2-docker-fundamentals" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-18"></a>Description<a class="hash-link" href="#description-18" title="Direct link to heading">#</a></h5><p>Docker is the go-to container platform for engineers. This training covers key concepts, hands-on Docker usage, building and running containers, and how Docker fits into production workflows.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/docker-fundamentals" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-9"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-9" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Docker vs Virtual Machines</td><td>6:23</td></tr><tr><td>Docker Terminology</td><td>5:56</td></tr><tr><td>Installing Docker Desktop</td><td>4:09</td></tr><tr><td>Pulling Images &amp; Running Containers</td><td>6:34</td></tr><tr><td>CLI Cheat Sheet</td><td>3:38</td></tr><tr><td>Docker Compose Explained</td><td>6:34</td></tr><tr><td>Build &amp; Run Hello World Image</td><td>6:28</td></tr><tr><td>Build Image with Dependencies</td><td>5:05</td></tr><tr><td>Using DockerHub</td><td>4:24</td></tr><tr><td>Image Layers</td><td>7:55</td></tr><tr><td>Deployment in Production</td><td>5:47</td></tr><tr><td>Security Best Practices</td><td>4:09</td></tr><tr><td>Managing Docker with Portainer</td><td>4:04</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-3-relational-data-modeling-1"></a>Week 3: Relational Data Modeling<a class="hash-link" href="#week-3-relational-data-modeling-1" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-19"></a>Description<a class="hash-link" href="#description-19" title="Direct link to heading">#</a></h5><p>Learn how to design efficient and scalable relational models. You&#x27;ll go through conceptual to physical modeling and normalize your schema. You&#x27;ll use MySQL and MySQL Workbench for hands-on practice.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/relational-data-modeling" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-10"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-10" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>History of Relational Models</td><td>3:16</td></tr><tr><td>Installing MySQL &amp; Workbench</td><td>8:04</td></tr><tr><td>Workbench Introduction</td><td>4:36</td></tr><tr><td>The Design Process Explained</td><td>4:14</td></tr><tr><td>Discover Entities</td><td>10:24</td></tr><tr><td>Discover Attributes</td><td>13:09</td></tr><tr><td>Normalize &amp; Define Relationships</td><td>11:19</td></tr><tr><td>Identifying vs Non-identifying</td><td>2:01</td></tr><tr><td>Resolve Many-to-Many</td><td>4:00</td></tr><tr><td>Resolve One-to-Many</td><td>2:34</td></tr><tr><td>Resolve One-to-One</td><td>1:45</td></tr><tr><td>Create ER Diagram</td><td>19:46</td></tr><tr><td>Create Physical Data Model</td><td>4:13</td></tr><tr><td>Populate from XLS</td><td>15:13</td></tr><tr><td>Course Conclusion</td><td>1:28</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-4-working--designing-apis"></a>Week 4: Working &amp; Designing APIs<a class="hash-link" href="#week-4-working--designing-apis" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-20"></a>Description<a class="hash-link" href="#description-20" title="Direct link to heading">#</a></h5><p>APIs are the backbone of modern data platforms. You&#x27;ll learn how to build and test APIs using FastAPI, design schemas, and deploy them in Docker. Postman and Docker are used for testing and deployment.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/apis-with-fastapi-course" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-11"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-11" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>What are APIs?</td><td>8:29</td></tr><tr><td>Hosting vs Using APIs</td><td>4:08</td></tr><tr><td>HTTP Methods &amp; Media Types</td><td>6:56</td></tr><tr><td>Response Codes &amp; Parameters</td><td>9:40</td></tr><tr><td>FastAPI Setup</td><td>4:55</td></tr><tr><td>POST, GET, PUT API Methods</td><td>16:18</td></tr><tr><td>Testing with Postman</td><td>4:22</td></tr><tr><td>Deploying FastAPI with Docker</td><td>6:01</td></tr><tr><td>API Security Best Practices</td><td>3:48</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-5--6-ml--containerization-on-aws"></a>Week 5 &amp; 6: ML &amp; Containerization on AWS<a class="hash-link" href="#week-5--6-ml--containerization-on-aws" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-21"></a>Description<a class="hash-link" href="#description-21" title="Direct link to heading">#</a></h5><p>This hands-on project teaches you how to build a real-time ML pipeline on AWS. You&#x27;ll pull data from the Twitter API (or The Guardian API), apply sentiment analysis with NLTK in a Lambda function, store results in a Postgres database via RDS, and build a Streamlit dashboard. Finally, you’ll containerize and deploy the dashboard using AWS ECS and ECR.</p><p><strong>Check out this project in my Academy: <a href="https://learndataengineering.com/p/ml-on-aws" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-12"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-12" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Introduction</td><td>2:38</td></tr><tr><td>Project Architecture Explained</td><td>2:06</td></tr><tr><td>RDS Setup</td><td>2:37</td></tr><tr><td>VPC Inbound Rules</td><td>2:12</td></tr><tr><td>PG Admin Installation &amp; S3 Config</td><td>4:05</td></tr><tr><td>Lambda Intro &amp; IAM Setup</td><td>3:11</td></tr><tr><td>Create Lambda Function</td><td>1:24</td></tr><tr><td>Lambda Code Explained</td><td>8:22</td></tr><tr><td>Insert Code Into Lambda</td><td>0:56</td></tr><tr><td>Add Layers from Klayers</td><td>5:32</td></tr><tr><td>Create Custom Layers</td><td>4:40</td></tr><tr><td>Test Lambda &amp; Set Env Variables</td><td>4:53</td></tr><tr><td>Schedule Lambda with EventBridge</td><td>3:15</td></tr><tr><td>Setup Virtual Conda Environment</td><td>4:07</td></tr><tr><td>Install Dependencies with Poetry</td><td>5:57</td></tr><tr><td>Streamlit App Code Walkthrough</td><td>7:52</td></tr><tr><td>Setup ECR Container Registry</td><td>1:52</td></tr><tr><td>AWS CLI Install &amp; Login</td><td>5:19</td></tr><tr><td>Dockerfile Build &amp; Push</td><td>2:52</td></tr><tr><td>Create ECS Fargate Cluster</td><td>1:34</td></tr><tr><td>ECS Task Configuration &amp; Deployment</td><td>4:59</td></tr><tr><td>Fixing ECS Task</td><td>5:14</td></tr><tr><td>Stop ECS Task</td><td>0:59</td></tr><tr><td>Project Conclusion</td><td>5:06</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-7-etl--cicd-on-aws"></a>Week 7: ETL &amp; CI/CD on AWS<a class="hash-link" href="#week-7-etl--cicd-on-aws" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-22"></a>Description<a class="hash-link" href="#description-22" title="Direct link to heading">#</a></h5><p>In this project, you&#x27;ll build a lightweight ETL job that pulls data from a public weather API and writes it into a time series database. You’ll dockerize the job, schedule it using AWS Lambda and EventBridge, and visualize the data using Grafana.</p><p><strong>Check out this project in my Academy: <a href="https://learndataengineering.com/p/timeseries-etl-with-aws-tdengine-grafana" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-13"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-13" title="Direct link to heading">#</a></h3><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Quick Note from Andreas</td><td>0:43</td></tr><tr><td>Project Introduction</td><td>1:26</td></tr><tr><td>Setup of the Project</td><td>2:52</td></tr><tr><td>Time Series Data Basics</td><td>2:20</td></tr><tr><td>Big Pros of Time Series Databases</td><td>2:06</td></tr><tr><td>About TDengine</td><td>1:22</td></tr><tr><td>Setup Weather API</td><td>1:04</td></tr><tr><td>Code Query API</td><td>2:41</td></tr><tr><td>TDengine Setup</td><td>3:04</td></tr><tr><td>Connect Python to TDengine</td><td>1:50</td></tr><tr><td>Lambda Docker Container &amp; Push to ECR</td><td>1:55</td></tr><tr><td>AWS Setup</td><td>1:36</td></tr><tr><td>Create Lambda Function Using Docker Image</td><td>1:04</td></tr><tr><td>Schedule Function with EventBridge</td><td>1:25</td></tr><tr><td>CloudWatch Lambda Events</td><td>0:27</td></tr><tr><td>Grafana Setup</td><td>3:01</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-8-building-a-lakehouse-on-aws-or-gcp"></a>Week 8: Building a Lakehouse on AWS or GCP<a class="hash-link" href="#week-8-building-a-lakehouse-on-aws-or-gcp" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-23"></a>Description<a class="hash-link" href="#description-23" title="Direct link to heading">#</a></h5><p>This week, you’ll learn how to combine data lakes and warehouses into a Lakehouse architecture. You’ll implement a full data analytics stack using tools like S3, Athena, BigQuery, Glue, Quicksight, and Data Studio.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/modern-data-warehouses" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-14"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-14" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Introduction</td><td>2:13</td></tr><tr><td>Data Science Platform Overview</td><td>4:10</td></tr><tr><td>ETL &amp; ELT in Warehouses</td><td>6:22</td></tr><tr><td>Data Lake &amp; Warehouse Integration</td><td>3:29</td></tr><tr><td>GCP Pipelines Overview</td><td>3:13</td></tr><tr><td>Cloud Storage &amp; BigQuery Hands-on</td><td>8:35</td></tr><tr><td>Create Dashboard in Data Studio</td><td>7:33</td></tr><tr><td>GCP Recap &amp; AWS Goals</td><td>2:12</td></tr><tr><td>Upload Data to S3</td><td>2:12</td></tr><tr><td>Athena Manual Table Configuration</td><td>3:48</td></tr><tr><td>Create Dashboard in Quicksight</td><td>5:05</td></tr><tr><td>Athena via Glue Catalog</td><td>3:29</td></tr><tr><td>Course Recap</td><td>2:36</td></tr><tr><td>BONUS: Redshift Spectrum with S3</td><td>2:57</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-9-orchestrate-with-airflow"></a>Week 9: Orchestrate with Airflow<a class="hash-link" href="#week-9-orchestrate-with-airflow" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-24"></a>Description<a class="hash-link" href="#description-24" title="Direct link to heading">#</a></h5><p>This training will guide you through installing and running Apache Airflow in Docker, creating DAGs, using the Taskflow API, and monitoring workflow execution.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/learn-apache-airflow" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-15"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-15" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Introduction</td><td>1:36</td></tr><tr><td>Airflow Usage</td><td>3:19</td></tr><tr><td>Fundamental Concepts</td><td>2:47</td></tr><tr><td>Airflow Architecture</td><td>3:09</td></tr><tr><td>Example Pipelines</td><td>4:49</td></tr><tr><td>Spotlight on 3rd Party Operators</td><td>2:17</td></tr><tr><td>Airflow XComs</td><td>4:32</td></tr><tr><td>Project Setup</td><td>1:43</td></tr><tr><td>Docker Setup Explained</td><td>2:06</td></tr><tr><td>Docker Compose &amp; Starting Containers</td><td>4:23</td></tr><tr><td>Checking Services</td><td>1:48</td></tr><tr><td>Weather API Setup</td><td>1:33</td></tr><tr><td>Postgres DB Setup</td><td>1:58</td></tr><tr><td>Airflow Web Interface</td><td>4:37</td></tr><tr><td>Create DAG with Airflow 2.0</td><td>9:46</td></tr><tr><td>Run Your DAG</td><td>4:15</td></tr><tr><td>Create DAG with Taskflow API</td><td>6:59</td></tr><tr><td>Get Data via SimpleHTTP Operator</td><td>3:38</td></tr><tr><td>Write to Postgres</td><td>4:12</td></tr><tr><td>Parallel Processing</td><td>4:15</td></tr><tr><td>Recap &amp; Outlook</td><td>4:38</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-10-pre-process-data-with-apache-spark"></a>Week 10: Pre-Process Data with Apache Spark<a class="hash-link" href="#week-10-pre-process-data-with-apache-spark" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-25"></a>Description<a class="hash-link" href="#description-25" title="Direct link to heading">#</a></h5><p>This training introduces Apache Spark fundamentals, showing you how to process large datasets using Spark DataFrames, RDDs, and SparkSQL inside Docker and Jupyter Notebooks.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/learning-apache-spark-fundamentals" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-16"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-16" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Introduction &amp; Contents</td><td>3:30</td></tr><tr><td>Vertical vs Horizontal Scaling</td><td>3:55</td></tr><tr><td>What Spark Is Good For</td><td>4:45</td></tr><tr><td>Driver, Context &amp; Executors</td><td>4:11</td></tr><tr><td>Cluster Types</td><td>1:59</td></tr><tr><td>Client vs Cluster Deployment</td><td>6:11</td></tr><tr><td>Where to Run Spark</td><td>3:38</td></tr><tr><td>Tools in Spark Course</td><td>2:35</td></tr><tr><td>Dataset Overview</td><td>4:11</td></tr><tr><td>Docker Setup</td><td>2:52</td></tr><tr><td>Jupyter Notebook Setup &amp; Run</td><td>5:31</td></tr><tr><td>RDDs</td><td>3:57</td></tr><tr><td>DataFrames</td><td>1:40</td></tr><tr><td>Transformations &amp; Actions Overview</td><td>2:59</td></tr><tr><td>Transformations</td><td>2:22</td></tr><tr><td>Actions</td><td>3:06</td></tr><tr><td>JSON Transformations</td><td>9:52</td></tr><tr><td>Working with Schemas</td><td>8:23</td></tr><tr><td>Working with DataFrames</td><td>10:09</td></tr><tr><td>SparkSQL</td><td>5:04</td></tr><tr><td>Working with RDDs</td><td>12:52</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-1113-build-a-streaming-pipeline-on-aws-azure-or-gcp"></a>Week 11–13: Build a Streaming Pipeline on AWS, Azure, or GCP<a class="hash-link" href="#week-1113-build-a-streaming-pipeline-on-aws-azure-or-gcp" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-26"></a>Description<a class="hash-link" href="#description-26" title="Direct link to heading">#</a></h5><p>In this 3-week section, you&#x27;ll complete an end-to-end streaming data project on the cloud platform of your choice: AWS, Azure, or GCP. Each project teaches you how to ingest real-time data, process it, store it, and create visualizations.</p><p>You only need to complete one of the following three options:</p><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="option-1-streaming-pipeline-on-aws"></a>Option 1: Streaming Pipeline on AWS<a class="hash-link" href="#option-1-streaming-pipeline-on-aws" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-27"></a>Description<a class="hash-link" href="#description-27" title="Direct link to heading">#</a></h5><p>You&#x27;ll use AWS services like API Gateway, Kinesis, DynamoDB, Redshift, Lambda, Glue, and Power BI to create a complete streaming solution. You&#x27;ll work with e-commerce data and build multiple ingestion and batch pipelines.</p><p><strong>Check out this project in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-aws" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-17"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-17" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Data Engineering</td><td>4:15</td></tr><tr><td>Data Science Platform</td><td>5:20</td></tr><tr><td>Dataset Introduction</td><td>3:16</td></tr><tr><td>Relational Storage Possibilities</td><td>3:46</td></tr><tr><td>NoSQL Storage Possibilities</td><td>6:28</td></tr><tr><td>Platform Design &amp; Pipeline Planning</td><td>3:49</td></tr><tr><td>Client to Visualization Design</td><td>3:00</td></tr><tr><td>Data Ingestion to Kinesis</td><td>3:00</td></tr><tr><td>Stream to S3 and DynamoDB</td><td>5:28</td></tr><tr><td>Visualization API &amp; Redshift</td><td>5:29</td></tr><tr><td>AWS Setup &amp; IAM</td><td>4:06</td></tr><tr><td>Create Lambda Functions</td><td>2:33</td></tr><tr><td>Configure Firehose &amp; Debugging</td><td>7:43</td></tr><tr><td>Power BI Setup</td><td>12:16</td></tr><tr><td>Glue Crawlers and Jobs</td><td>26:52</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="option-2-streaming-pipeline-on-azure"></a>Option 2: Streaming Pipeline on Azure<a class="hash-link" href="#option-2-streaming-pipeline-on-azure" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-28"></a>Description<a class="hash-link" href="#description-28" title="Direct link to heading">#</a></h5><p>You’ll build a Twitter-like JSON stream pipeline using Azure Functions, Event Hub, Cosmos DB, and Power BI. You’ll learn how to set up API management, key vaults, and authentication.</p><p><strong>Check out this project in my Academy: <a href="https://learndataengineering.com/p/build-streaming-data-pipelines-in-azure" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-18"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-18" title="Direct link to heading">#</a></h4><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Project Introduction</td><td>2:43</td></tr><tr><td>Local Preprocessing &amp; Docker Setup</td><td>7:06</td></tr><tr><td>Develop &amp; Deploy Azure Functions</td><td>5:52</td></tr><tr><td>Test Functions &amp; Integrate with Blob Storage</td><td>6:26</td></tr><tr><td>Add Functions to Azure API Management (APIM)</td><td>7:05</td></tr><tr><td>Key Vault &amp; Authentication</td><td>4:41</td></tr><tr><td>Create Event Hubs and Bindings</td><td>6:59</td></tr><tr><td>Write to Cosmos DB</td><td>9:03</td></tr><tr><td>Power BI Connection and Dashboard Creation</td><td>6:32</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="option-3-streaming-pipeline-on-gcp"></a>Option 3: Streaming Pipeline on GCP<a class="hash-link" href="#option-3-streaming-pipeline-on-gcp" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-29"></a>Description<a class="hash-link" href="#description-29" title="Direct link to heading">#</a></h5><p>This project shows how to extract weather data via API, stream it with Pub/Sub, write it into Cloud SQL, and visualize it with Looker Studio. You&#x27;ll also learn function deployment and VM/database setup.</p><p><strong>Check out this project in my Academy: <a href="https://learndataengineering.com/p/data-engineering-on-gcp" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-19"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-19" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Introduction &amp; Setup</td><td>2:43</td></tr><tr><td>Architecture &amp; Weather API</td><td>5:31</td></tr><tr><td>Enable APIs &amp; Configure Scheduling</td><td>4:00</td></tr><tr><td>Setup MySQL Database &amp; Compute Engine</td><td>4:40</td></tr><tr><td>Create Cloud Functions for Data Ingestion</td><td>8:37</td></tr><tr><td>Use Pub/Sub for Messaging</td><td>1:41</td></tr><tr><td>Write Data to Cloud SQL</td><td>13:43</td></tr><tr><td>Test and Monitor Data Flow</td><td>5:51</td></tr><tr><td>Setup Looker Studio &amp; Build Dashboards</td><td>4:17</td></tr><tr><td>Monitor Pipelines</td><td>6:20</td></tr></tbody></table><hr><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="week-14-analyze-training-logs-with-elasticsearch"></a>Week 14: Analyze Training Logs with Elasticsearch<a class="hash-link" href="#week-14-analyze-training-logs-with-elasticsearch" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="description-30"></a>Description<a class="hash-link" href="#description-30" title="Direct link to heading">#</a></h5><p>Wrap up your roadmap by learning how to monitor pipelines using Elasticsearch. You’ll deploy Elasticsearch with Docker, send logs from your training pipelines, and visualize them in Kibana dashboards.</p><p><strong>Check out this course in my Academy: <a href="https://learndataengineering.com/p/log-analysis-with-elasticsearch" target="_blank" rel="noopener noreferrer">Learn More</a></strong></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="course-curriculum-20"></a>Course Curriculum<a class="hash-link" href="#course-curriculum-20" title="Direct link to heading">#</a></h5><table><thead><tr><th>Lesson</th><th>Duration</th></tr></thead><tbody><tr><td>Course Introduction</td><td>2:07</td></tr><tr><td>Elasticsearch vs Relational Databases</td><td>5:43</td></tr><tr><td>ETL Log Analysis &amp; Debugging</td><td>3:54</td></tr><tr><td>Streaming Log Analysis &amp; Debugging</td><td>2:48</td></tr><tr><td>Solving Problems with Elasticsearch</td><td>4:37</td></tr><tr><td>ELK Stack Overview</td><td>2:03</td></tr><tr><td>Setup Limiting RAM &amp; Environment Config</td><td>4:26</td></tr><tr><td>Running Elasticsearch</td><td>4:07</td></tr><tr><td>Elasticsearch APIs &amp; Python Index Creation</td><td>7:31</td></tr><tr><td>Write Logs (JSON) to Elasticsearch</td><td>4:46</td></tr><tr><td>Create Kibana Visualizations &amp; Dashboards</td><td>9:27</td></tr><tr><td>Search Logs in Elasticsearch</td><td>4:57</td></tr><tr><td>Course Recap</td><td>—</td></tr></tbody></table><hr><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="whats-next-2"></a>What’s Next?<a class="hash-link" href="#whats-next-2" title="Direct link to heading">#</a></h4><p>After 14 weeks, you’ll have built scalable, production-ready data pipelines and ML workflows. You can now explore more advanced projects, optimize performance, and contribute to production systems with confidence. Need help showcasing your skills or getting hired? Reach out to my coaching program!</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="roadmap-for-software-engineers"></a>Roadmap for Software Engineers<a class="hash-link" href="#roadmap-for-software-engineers" title="Direct link to heading">#</a></h3><p><img alt="Building blocks of your curriculum" src="/assets/images/Data-Engineering-Roadmap-for-Software-Engineers-a7164de96fd4a0f515b2863497a98237.jpg"></p><p>if you&#x27;re transitioning from a background in computer science or software engineering into data engineering, you&#x27;re already equipped with a solid foundation. Your existing knowledge in coding, familiarity with SQL databases, understanding of computer networking, and experience with operating systems like Linux, provide you with a considerable advantage. These skills form the cornerstone of data engineering and can significantly streamline your learning curve as you embark on this new journey.</p><p>Here&#x27;s a refined roadmap, incorporating your prior expertise, to help you excel in data engineering:</p><ul><li><strong>Deepen Your Python Skills:</strong> Python is crucial in data engineering for processing and handling various data formats, such as APIs, CSV, and JSON. Given your coding background, focusing on Python for data engineering will enhance your ability to manipulate and process data effectively.</li><li><strong>Master Docker:</strong> Docker is essential for deploying code and managing containers, streamlining the software distribution process. Your understanding of operating systems and networking will make mastering Docker more intuitive, as you&#x27;ll appreciate the importance of containerization in today&#x27;s development and deployment workflows.</li><li><strong>Platform and Pipeline Design:</strong> Leverage your knowledge of computer networking and operating systems to grasp the architecture of data platforms. Understanding how to design data pipelines, including considerations for stream and batch processing, and emphasizing security, will be key. Your background will provide a solid foundation for understanding how different components integrate within a data platform.</li><li><strong>Choosing the Right Data Stores:</strong> Dive into the specifics of data stores, understanding the nuances between transactional and analytical databases, and when to use relational vs. NoSQL vs. document stores vs. time-series databases. Your experience with SQL databases will serve as a valuable baseline for exploring these various data storage options.</li><li><strong>Explore Cloud Platforms:</strong> Get hands-on with cloud services such as AWS, GCP, and Azure. Projects or courses that offer practical experience with these platforms will be invaluable. Your tasks might include building pipelines to process data from APIs, using message queues, or delving into data warehousing and lakes, capitalizing on your foundational skills.</li><li><strong>Optional Deep Dives:</strong> For those interested in advanced data processing, exploring technologies like Spark or Kafka for stream processing can be enriching. Additionally, learning how to build APIs and work with MongoDB for document storage can open new avenues, especially through practical projects.</li><li><strong>Log Analysis and Data Observability:</strong> Familiarize yourself with tools like Elasticsearch, Grafana, and InfluxDB to monitor and analyze your data pipelines effectively. This area leverages your comprehensive understanding of how systems communicate and operate, enhancing your ability to maintain and optimize data flows.</li></ul><p>As you embark on this path, remember that your journey is unique. Your existing knowledge not only serves as a strong foundation but also as a catalyst for accelerating your growth in the realm of data engineering. Keep leveraging your strengths, explore areas of interest deeply, and continually adapt to the evolving landscape of data technology.</p><p>| Live Stream -&gt; Data Engineering Roadmap for Computer Scientists / Developers
|------------------|
|In this live stream you&#x27;ll find even more details how to read this roadmap for Data Scientists, why I chose these tools and why I think this is the right way to do it.
| <a href="https://youtube.com/live/0e4WfIUixRw" target="_blank" rel="noopener noreferrer">Watch on YouTube</a>|</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-engineers-skills-matrix"></a>Data Engineers Skills Matrix<a class="hash-link" href="#data-engineers-skills-matrix" title="Direct link to heading">#</a></h2><p><img alt="Data Engineer Skills Matrix" src="/assets/images/Data-Engineer-Skills-Matrix-6345a4d7f6ee6bb3ac8d6004a3d6d526.jpg"></p><p>If you&#x27;re diving into the world of data engineering or looking to climb the ladder within this field, you&#x27;re in for a treat with this enlightening YouTube video. Andreas kicks things off by introducing us to a very handy tool they&#x27;ve developed: the Data Engineering Skills Matrix. This isn&#x27;t just any chart; it&#x27;s a roadmap designed to navigate the complex landscape of data engineering roles, ranging from a Junior Data Engineer to the lofty heights of a Data Architect and Machine Learning Engineer.</p><p>| Live Stream -&gt; Data Engineering Skills Matrix
|------------------|
|In this live stream you&#x27;ll find even more details how to read this skills matrix for Data Engineers.<br>
| <a href="https://youtube.com/live/5E0UiBy0Kwo" target="_blank" rel="noopener noreferrer">Watch on YouTube</a>|</p><p>Andreas takes us through the intricacies of this matrix, layer by layer. Starting with the basics, they discuss the minimum experience needed for each role. It&#x27;s an eye-opener, especially when you see how experience requirements evolve from a beginner to senior levels. But it&#x27;s not just about how many years you&#x27;ve spent in the field; it&#x27;s about the skills you&#x27;ve honed during that time.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="challenges--responsibilities"></a>Challenges &amp; Responsibilities<a class="hash-link" href="#challenges--responsibilities" title="Direct link to heading">#</a></h3><p>As the conversation progresses, Andreas delves into the core responsibilities and main tasks associated with each role. You&#x27;ll learn what sets a Junior Data Engineer apart from a Senior Data Engineer, the unique challenges a Data Architect faces, and the critical skills a Machine Learning Engineer must possess. This part of the video is golden for anyone trying to understand where they fit in the data engineering ecosystem or plotting their next career move.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sql--soft-skills"></a>SQL &amp; Soft Skills<a class="hash-link" href="#sql--soft-skills" title="Direct link to heading">#</a></h3><p>Then there&#x27;s the talk on SQL knowledge and its relevance across different roles. This segment sheds light on how foundational SQL is, irrespective of your position. But it&#x27;s not just about technical skills; the video also emphasizes soft skills, like leadership and collaboration, painting a holistic picture of what it takes to succeed in data engineering.</p><p>For those who love getting into the weeds, Andreas doesn&#x27;t disappoint. They discuss software development skills, debugging, and even dive into how data engineers work with SQL and databases. This part is particularly insightful for understanding the technical depth required at various stages of your career.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="qa"></a>Q&amp;A<a class="hash-link" href="#qa" title="Direct link to heading">#</a></h3><p>And here&#x27;s the cherry on top: Andreas encourages interaction, inviting viewers to share their experiences and questions. This makes the video not just a one-way learning experience but a dynamic conversation that enriches everyone involved.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary-1"></a>Summary<a class="hash-link" href="#summary-1" title="Direct link to heading">#</a></h3><p>By the end of this video, you&#x27;ll walk away with a clear understanding of the data engineering field&#x27;s diverse roles. You&#x27;ll know the skills needed to excel in each role and have a roadmap for your career progression. Whether you&#x27;re a recent graduate looking to break into data engineering or a seasoned professional aiming for a senior position, Andreas&#x27;s video is a must-watch. It&#x27;s not just a lecture; it&#x27;s a guide to navigating the exciting world of data engineering, tailored by someone who&#x27;s taken the time to lay out the journey for you.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-become-a-senior-data-engineer"></a>How to Become a Senior Data Engineer<a class="hash-link" href="#how-to-become-a-senior-data-engineer" title="Direct link to heading">#</a></h2><p>Becoming a senior data engineer is a goal many in the tech industry aspire to. It&#x27;s a role that demands a deep understanding of data architecture, advanced programming skills, and the ability to lead and communicate effectively within an organization. In this live stream series, I dove into what it takes to climb the ladder to a senior data engineering position. Here are the key takeaways. You can find the links to the videos and the shown images below.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="understanding-the-role"></a>Understanding the Role<a class="hash-link" href="#understanding-the-role" title="Direct link to heading">#</a></h3><p>The journey to becoming a senior data engineer starts with a clear understanding of what the role entails. Senior data engineers are responsible for designing, implementing, and maintaining an organization&#x27;s data architecture. They ensure data accuracy, accessibility, and security, often taking the lead on complex projects that require advanced technical skills and strategic thinking.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="key-skills-and-knowledge-areas"></a>Key Skills and Knowledge Areas<a class="hash-link" href="#key-skills-and-knowledge-areas" title="Direct link to heading">#</a></h3><p>Based on insights from the live stream and consultations with industry experts, including GPT-3, here are the critical areas where aspiring senior data engineers should focus their development:</p><ul><li><strong>Advanced Data Modeling and Architecture:</strong> Mastery of data modeling techniques and architecture best practices is crucial. This includes understanding of dimensional and Data Vault modeling, as well as expertise in SQL and NoSQL databases.</li><li><strong>Big Data Technologies:</strong> Familiarity with distributed computing frameworks (like Apache Spark), streaming technologies (such as Apache Kafka), and cloud-based big data technologies is essential.
Advanced ETL Techniques: Skills in incremental loading, data merging, and transformation are vital for efficiently processing large datasets.</li><li><strong>Data Warehousing and Data Lake Implementation:</strong> Building and maintaining scalable and performant data warehouses and lakes are fundamental responsibilities.</li><li><strong>Cloud Computing:</strong> Proficiency in cloud services from AWS, Azure, or GCP, along with platforms like Snowflake and Databricks, is increasingly important.</li><li><strong>Programming and Scripting:</strong> Advanced coding skills in languages relevant to data engineering, such as Python, Scala, or Java, are non-negotiable.</li><li><strong>Data Governance and Compliance:</strong> Understanding data governance frameworks and compliance requirements is critical, especially in highly regulated industries.</li><li><strong>Leadership and Communication:</strong> Beyond technical skills, the ability to lead projects, communicate effectively with both technical and non-technical team members, and mentor junior engineers is what differentiates a senior engineer.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="learning-pathways"></a>Learning Pathways<a class="hash-link" href="#learning-pathways" title="Direct link to heading">#</a></h3><p>Becoming a senior data engineer requires continuous learning and real-world experience. Here are a few steps to guide your journey:</p><ul><li><strong>Educational Foundation:</strong> Start with a strong foundation in computer science or a related field. This can be through formal education or self-study courses.</li><li><strong>Gain Practical Experience:</strong> Apply your skills in real-world projects. This could be in a professional setting, contributions to open-source projects, or personal projects.</li><li><strong>Specialize and Certify:</strong> Consider specializing in areas particularly relevant to your interests or industry needs. Obtaining certifications in specific technologies or platforms can also bolster your credentials.</li><li><strong>Develop Soft Skills:</strong> Work on your communication, project management, and leadership skills. These are as critical as your technical abilities.</li><li><strong>Seek Feedback and Mentorship:</strong> Learn from the experiences of others. Seek out mentors who can provide guidance and feedback on your progress.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="video-1"></a>Video 1<a class="hash-link" href="#video-1" title="Direct link to heading">#</a></h3><p>| Live Stream -&gt; How to become a Senior Data Engineer - Part 1
|------------------|
| In this part one I talked about Data Modeling, Big Data, ETL, Data Warehousing &amp; Data Lakes as well as Cloud computing
| <a href="https://youtube.com/live/M-6xkTCKQQc" target="_blank" rel="noopener noreferrer">Watch on YouTube</a>|</p><p><img alt="Watch on YouTube" src="/assets/images/Becoming-a-Senior-Data-Engineer-Video-1-71ef9af722e156b7f3deb7de508fc819.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="video-2"></a>Video 2<a class="hash-link" href="#video-2" title="Direct link to heading">#</a></h3><p>| Live Stream -&gt; How to become a Senior Data Engineer - Part 2
|------------------|
| In part two I talked about real time data processing, programming &amp; scripting, data governance, compliance and data security
| <a href="https://youtube.com/live/po96pzpjxvA" target="_blank" rel="noopener noreferrer">Watch on YouTube</a>|</p><p><img alt="Watch on YouTube" src="/assets/images/Becoming-a-Senior-Data-Engineer-Video-2-69a5bfc2615f2edadbc1e68770ee40d1.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="video-3"></a>Video 3<a class="hash-link" href="#video-3" title="Direct link to heading">#</a></h3><p>| Live Stream -&gt; How to become a Senior Data Engineer - Part 3
|------------------|
| In part 3 I focused on everything regarding Leadership and Communication: team management, project management, collaboration, problem solving, strategic thinking, communication and leadership
| <a href="https://youtube.com/live/DMumpzSyRjI" target="_blank" rel="noopener noreferrer">Watch on YouTube</a>|</p><p><img alt="Watch on YouTube" src="/assets/images/Becoming-a-Senior-Data-Engineer-Video-3-7ccc30e8bf245524a44691adb1ac0b33.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="final-thoughts"></a>Final Thoughts<a class="hash-link" href="#final-thoughts" title="Direct link to heading">#</a></h3><p>The path to becoming a senior data engineer is both challenging and rewarding. It requires a blend of technical prowess, continuous learning, and the development of soft skills that enable you to lead and innovate. Whether you&#x27;re just starting out or looking to advance your career, focusing on the key areas outlined above will set you on the right path.</p></div></article><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/02-BasicSkills"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">02-BasicSkills »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#contents" class="table-of-contents__link">Contents</a></li><li><a href="#what-is-this-cookbook" class="table-of-contents__link">What is this Cookbook</a></li><li><a href="#if-you-like-this-book--need-more-help" class="table-of-contents__link">If You Like This Book &amp; Need More Help:</a></li><li><a href="#support-this-book-for-free" class="table-of-contents__link">Support This Book For Free!</a></li><li><a href="#how-to-contribute" class="table-of-contents__link">How To Contribute</a></li><li><a href="#data-engineers" class="table-of-contents__link">Data Engineers</a></li><li><a href="#my-data-science-platform-blueprint" class="table-of-contents__link">My Data Science Platform Blueprint</a><ul><li><a href="#connect" class="table-of-contents__link">Connect</a></li><li><a href="#buffer" class="table-of-contents__link">Buffer</a></li><li><a href="#processing-framework" class="table-of-contents__link">Processing Framework</a></li><li><a href="#store" class="table-of-contents__link">Store</a></li><li><a href="#visualize" class="table-of-contents__link">Visualize</a></li></ul></li><li><a href="#who-companies-need" class="table-of-contents__link">Who Companies Need</a></li><li><a href="#how-to-learn-data-engineering" class="table-of-contents__link">How to Learn Data Engineering</a><ul><li><a href="#interview-with-andreas-on-the-super-data-science-podcast" class="table-of-contents__link">Interview with Andreas on the Super Data Science Podcast</a></li><li><a href="#building-blocks-to-learn-data-engineering" class="table-of-contents__link">Building Blocks to Learn Data Engineering</a></li><li><a href="#roadmap-for-beginners" class="table-of-contents__link">Roadmap for Beginners</a></li><li><a href="#roadmap-for-data-analysts" class="table-of-contents__link">Roadmap for Data Analysts</a></li><li><a href="#roadmap-for-data-scientists" class="table-of-contents__link">Roadmap for Data Scientists</a></li><li><a href="#course-curriculum-13" class="table-of-contents__link">Course Curriculum</a></li><li><a href="#roadmap-for-software-engineers" class="table-of-contents__link">Roadmap for Software Engineers</a></li></ul></li><li><a href="#data-engineers-skills-matrix" class="table-of-contents__link">Data Engineers Skills Matrix</a><ul><li><a href="#challenges--responsibilities" class="table-of-contents__link">Challenges &amp; Responsibilities</a></li><li><a href="#sql--soft-skills" class="table-of-contents__link">SQL &amp; Soft Skills</a></li><li><a href="#qa" class="table-of-contents__link">Q&amp;A</a></li><li><a href="#summary-1" class="table-of-contents__link">Summary</a></li></ul></li><li><a href="#how-to-become-a-senior-data-engineer" class="table-of-contents__link">How to Become a Senior Data Engineer</a><ul><li><a href="#understanding-the-role" class="table-of-contents__link">Understanding the Role</a></li><li><a href="#key-skills-and-knowledge-areas" class="table-of-contents__link">Key Skills and Knowledge Areas</a></li><li><a href="#learning-pathways" class="table-of-contents__link">Learning Pathways</a></li><li><a href="#video-1" class="table-of-contents__link">Video 1</a></li><li><a href="#video-2" class="table-of-contents__link">Video 2</a></li><li><a href="#video-3" class="table-of-contents__link">Video 3</a></li><li><a href="#final-thoughts" class="table-of-contents__link">Final Thoughts</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Andreas Kretz. Built by Kristijan Bakaric with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/styles.4aacfb65.js"></script>
<script src="/assets/js/runtime~main.6087afab.js"></script>
<script src="/assets/js/main.4c5ae3c4.js"></script>
<script src="/assets/js/1.4743d043.js"></script>
<script src="/assets/js/2.d8ee9dd6.js"></script>
<script src="/assets/js/18.28e25878.js"></script>
<script src="/assets/js/19.f0bdf1f6.js"></script>
<script src="/assets/js/935f2afb.cff03dc6.js"></script>
<script src="/assets/js/17896441.aacbb830.js"></script>
<script src="/assets/js/f5648033.2c628efe.js"></script>
</body>
</html>