<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.fd17476c3">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="THE DATA ENGINEERING COOKBOOK Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="THE DATA ENGINEERING COOKBOOK Blog Atom Feed"><title data-react-helmet="true">03-AdvancedSkills | THE DATA ENGINEERING COOKBOOK</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="03-AdvancedSkills | THE DATA ENGINEERING COOKBOOK"><meta data-react-helmet="true" name="description" content="Advanced Data Engineering Skills"><meta data-react-helmet="true" property="og:description" content="Advanced Data Engineering Skills"><meta data-react-helmet="true" property="og:url" content="http://cookbook.learndataengineering.com/docs/03-AdvancedSkills"><link data-react-helmet="true" rel="shortcut icon" href="/images/CookbookCover.jpg"><link data-react-helmet="true" rel="alternate" href="http://cookbook.learndataengineering.com/docs/03-AdvancedSkills" hreflang="x-default"><link data-react-helmet="true" rel="canonical" href="http://cookbook.learndataengineering.com/docs/03-AdvancedSkills"><link rel="stylesheet" href="/assets/css/styles.5ba6ab3f.css">
<link rel="preload" href="/assets/js/styles.4aacfb65.js" as="script">
<link rel="preload" href="/assets/js/runtime~main.23366044.js" as="script">
<link rel="preload" href="/assets/js/main.4c5ae3c4.js" as="script">
<link rel="preload" href="/assets/js/1.4743d043.js" as="script">
<link rel="preload" href="/assets/js/2.d8ee9dd6.js" as="script">
<link rel="preload" href="/assets/js/18.28e25878.js" as="script">
<link rel="preload" href="/assets/js/19.f0bdf1f6.js" as="script">
<link rel="preload" href="/assets/js/935f2afb.cff03dc6.js" as="script">
<link rel="preload" href="/assets/js/17896441.aacbb830.js" as="script">
<link rel="preload" href="/assets/js/0a15610b.cd0a632d.js" as="script">
</head>
<body>
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_1oUP">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a><a class="navbar__item navbar__link" href="/docs/01-Introduction">Cookbook</a></div><div class="navbar__items navbar__items--right"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Data Engineering Academy</a><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Plumbers Of Data Science</a><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/01-Introduction">Cookbook</a></li><li class="menu__list-item"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="menu__link">Data Engineering Academy</a></li><li class="menu__list-item"><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="menu__link">Plumbers Of Data Science</a></li><li class="menu__list-item"><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Data Engineering</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/01-Introduction">01-Introduction</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/02-BasicSkills">02-BasicSkills</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/03-AdvancedSkills">03-AdvancedSkills</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/04-HandsOnCourse">04-HandsOnCourse</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/05-CaseStudies">05-CaseStudies</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/06-BestPracticesCloud">06-BestPracticesCloud</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/07-DataSources">07-DataSources</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/08-InterviewQuestions">08-InterviewQuestions</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/09-BooksAndCourses">09-BooksAndCourses</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/10-Updates">10-Updates</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">03-AdvancedSkills</h1></header><div class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="advanced-data-engineering-skills"></a>Advanced Data Engineering Skills<a class="hash-link" href="#advanced-data-engineering-skills" title="Direct link to heading">#</a></h1><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="contents"></a>Contents<a class="hash-link" href="#contents" title="Direct link to heading">#</a></h2><ul><li><a href="/docs/03-AdvancedSkills#data-science-platform">Data Science Platform</a><ul><li><a href="/docs/03-AdvancedSkills#why-a-good-data-platform-is-important">Why a Good Data Platform Is Important</a></li><li><a href="/docs/03-AdvancedSkills#Big-Data-vs-Data-Science-and-Analytics">Big Data vs Data Science and Analytics</a></li><li><a href="/docs/03-AdvancedSkills#the-4-vs-of-big-data">The 4 Vs of Big Data</a></li><li><a href="/docs/03-AdvancedSkills#why-big-data">Why Big Data</a><ul><li><a href="/docs/03-AdvancedSkills#planning-is-everything">Planning is Everything</a></li><li><a href="/docs/03-AdvancedSkills#the-problem-with-etl">The Problem with ETL</a></li><li><a href="/docs/03-AdvancedSkills#scaling-up">Scaling Up</a></li><li><a href="/docs/03-AdvancedSkills#scaling-out">Scaling Out</a></li><li><a href="/docs/03-AdvancedSkills#please-dont-go-big-data">When not to Do Big Data</a></li></ul></li></ul></li><li><a href="/docs/03-AdvancedSkills#81-platform-and-pipeline-design-questions">81 Platform &amp; Pipeline Design Questions</a><ul><li><a href="/docs/03-AdvancedSkills#data-source-questions">Data Source Questions</a></li><li><a href="/docs/03-AdvancedSkills#goals-and-destination-questions">Goals and Destination Questions</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#connect">Connect</a><ul><li><a href="/docs/03-AdvancedSkills#rest-apis">REST APIs</a><ul><li><a href="/docs/03-AdvancedSkills#api-design">API Design</a></li><li><a href="/docs/03-AdvancedSkills#implementation-frameworks">Implementation Frameworks</a></li><li><a href="/docs/03-AdvancedSkills#security">Security</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#apache-nifi">Apache Nifi</a></li><li><a href="/docs/03-AdvancedSkills#logstash">Logstash</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#buffer">Buffer</a><ul><li><a href="/docs/03-AdvancedSkills#apache-kafka">Apache Kafka</a><ul><li><a href="/docs/03-AdvancedSkills#why-a-message-queue-tool">Why a Message Queue Tool?</a></li><li><a href="/docs/03-AdvancedSkills#kafka-architecture">Kafka Architecture</a></li><li><a href="/docs/03-AdvancedSkills#what-are-topics">Kafka Topics</a></li><li><a href="/docs/03-AdvancedSkills#what-does-zookeeper-have-to-do-with-kafka">Kafka and Zookeeper</a></li><li><a href="/docs/03-AdvancedSkills#how-to-produce-and-consume-messages">How to Produce and Consume Messages</a></li><li><a href="/docs/03-AdvancedSkills#kafka-commands">Kafka Commands</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#redis-pub-sub">Apache Redis Pub-Sub</a></li><li><a href="/docs/03-AdvancedSkills#apache-kafka">AWS Kinesis</a></li><li><a href="/docs/03-AdvancedSkills#google-cloud-pubsub">Google Cloud PubSub</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#processing-frameworks">Processing Frameworks</a><ul><li><a href="/docs/03-AdvancedSkills#lambda-and-kappa-architecture">Lambda and Kappa Architecture</a></li><li><a href="/docs/03-AdvancedSkills#batch-processing">Batch Processing</a></li><li><a href="/docs/03-AdvancedSkills#stream-processing">Stream Processing</a><ul><li><a href="/docs/03-AdvancedSkills#three-methods-of-streaming">Three Methods of Streaming</a></li><li><a href="/docs/03-AdvancedSkills#at-least-once">At Least Once</a></li><li><a href="/docs/03-AdvancedSkills#at-most-once">At Most Once</a></li><li><a href="/docs/03-AdvancedSkills#exactly-once">Exactly Once</a></li><li><a href="/docs/03-AdvancedSkills#check-the-tools">Check The Tools</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#should-you-do-stream-or-batch-processing">Should You do Stream or Batch Processing</a></li><li><a href="/docs/03-AdvancedSkills#is-etl-still-relevant-for-analytics">Is ETL still relevant for Analytics?</a></li><li><a href="/docs/03-AdvancedSkills#mapreduce">MapReduce</a><ul><li><a href="/docs/03-AdvancedSkills#How-does-mapreduce-work">How Does MapReduce Work</a></li><li><a href="/docs/03-AdvancedSkills#mapreduce">MapReduce</a></li><li><a href="/docs/03-AdvancedSkills#example">MapReduce Example</a></li><li><a href="/docs/03-AdvancedSkills#What-is-the-limitation-of-mapreduce">MapReduce Limitations</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#apache-spark">Apache Spark</a><ul><li><a href="/docs/03-AdvancedSkills#what-is-the-difference-to-MapReduce">What is the Difference to MapReduce?</a></li><li><a href="/docs/03-AdvancedSkills#how-does-spark-fit-to-hadoop">How Spark Fits to Hadoop</a></li><li><a href="/docs/03-AdvancedSkills#wheres-the-difference">Spark vs Hadoop</a></li><li><a href="/docs/03-AdvancedSkills#spark-and-hadoop-is-a-perfect-fit">Spark and Hadoop a Perfect Fit</a></li><li><a href="/docs/03-AdvancedSkills#spark-on-yarn">Spark on YARn</a></li><li><a href="/docs/03-AdvancedSkills#my-simple-rule-of-thumb">My Simple Rule of Thumb</a></li><li><a href="/docs/03-AdvancedSkills#available-languages">Available Languages</a></li><li><a href="/docs/03-AdvancedSkills#how-spark-works-driver-executor-sparkcontext">Spark Driver Executor and SparkContext</a></li><li><a href="/docs/03-AdvancedSkills#spark-batch-vs-stream-processing">Spark Batch vs Stream processing</a></li><li><a href="/docs/03-AdvancedSkills#How-does-spark-use-data-from-hadoop">How Spark uses Data From Hadoop</a></li><li><a href="/docs/03-AdvancedSkills#what-are-rdds-and-how-to-use-them">What are RDDs and How to Use Them</a></li><li><a href="/docs/03-AdvancedSkills#available-languages">SparkSQL How and Why to Use It</a></li><li><a href="/docs/03-AdvancedSkills#what-are-dataframes-how-to-use-them">What are Dataframes and How to Use Them</a></li><li><a href="/docs/03-AdvancedSkills#machine-learning-on-spark-tensor-flow">Machine Learning on Spark (TensorFlow)</a></li><li><a href="/docs/03-AdvancedSkills#mllib">MLlib</a></li><li><a href="/docs/03-AdvancedSkills#spark-setup">Spark Setup</a></li><li><a href="/docs/03-AdvancedSkills#spark-resource-management">Spark Resource Management</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#apache-flink">AWS Lambda</a>  </li><li><a href="/docs/03-AdvancedSkills#apache-flink">Apache Flink</a></li><li><a href="/docs/03-AdvancedSkills#elasticsearch">Elasticsearch</a></li><li><a href="/docs/03-AdvancedSkills#apache-drill">Apache Drill</a></li><li><a href="/docs/03-AdvancedSkills#streamsets">StreamSets</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#store">Store</a><ul><li><a href="/docs/03-AdvancedSkills#analytical-data-stores">Analytical Data Stores</a><ul><li><a href="/docs/03-AdvancedSkills#data-warehouse-vs-data-lake">Data Warehouse vs Data Lake</a></li><li><a href="/docs/03-AdvancedSkills#snowflake-and-dbt">Snowflake and dbt</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#transactional-data-stores">Transactional Data Stores</a><ul><li><a href="/docs/03-AdvancedSkills#sql-databases">SQL Databases</a><ul><li><a href="/docs/03-AdvancedSkills#postgresql-db">PostgreSQL DB</a></li><li><a href="/docs/03-AdvancedSkills#database-design">Database Design</a></li><li><a href="/docs/03-AdvancedSkills#sql-queries">SQL Queries</a></li><li><a href="/docs/03-AdvancedSkills#stored-procedures">Stored Procedures</a></li><li><a href="/docs/03-AdvancedSkills#odbc-jdbc-server-connections">ODBC/JDBC Server Connections</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#nosql-stores">NoSQL Stores</a><ul><li><a href="/docs/03-AdvancedSkills#keyvalue-stores-hbase">HBase KeyValue Store</a></li><li><a href="/docs/03-AdvancedSkills#document-stores-hdfs">HDFS Document Store</a></li><li><a href="/docs/03-AdvancedSkills#document-stores-mongodb">MongoDB Document Store</a></li><li><a href="/docs/03-AdvancedSkills#Elasticsearch-search-engine-and-document-store">Elasticsearch Document Store</a></li><li><a href="/docs/03-AdvancedSkills#graph-db-neo4j">Graph Databases (Neo4j)</a></li><li><a href="/docs/03-AdvancedSkills#impala">Impala</a></li><li><a href="/docs/03-AdvancedSkills#kudu">Kudu</a></li><li><a href="/docs/03-AdvancedSkills#apache-druid">Apache Druid</a></li><li><a href="/docs/03-AdvancedSkills#influxdb-time-series-database">InfluxDB Time Series Database</a></li><li><a href="/docs/03-AdvancedSkills#mpp-databases-greenplum">Greenplum MPP Database</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#nosql-data-warehouses">NoSQL Data Warehouses</a><ul><li><a href="/docs/03-AdvancedSkills#hive-warehouse">Hive Warehouse</a></li><li><a href="/docs/03-AdvancedSkills#impala">Impala</a></li></ul></li></ul></li></ul></li><li><a href="/docs/03-AdvancedSkills#visualize">Visualize</a><ul><li><a href="/docs/03-AdvancedSkills#android-and-ios">Android and IOS</a></li><li><a href="/docs/03-AdvancedSkills#how-to-design-apis-for-mobile-apps">API Design for Mobile Apps</a></li><li><a href="/docs/03-AdvancedSkills#dashboards">Dashboards</a><ul><li><a href="/docs/03-AdvancedSkills#grafana">Grafana</a></li><li><a href="/docs/03-AdvancedSkills#kibana">Kibana</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#how-to-use-webservers-to-display-content">Webservers</a><ul><li><a href="/docs/03-AdvancedSkills#tomcat">Tomcat</a></li><li><a href="/docs/03-AdvancedSkills#jetty">Jetty</a></li><li><a href="/docs/03-AdvancedSkills#nodered">NodeRED</a></li><li><a href="/docs/03-AdvancedSkills#react">React</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#business-intelligence-tools">Business Intelligence Tools</a><ul><li><a href="/docs/03-AdvancedSkills#tableau">Tableau</a></li><li><a href="/docs/03-AdvancedSkills#power-bi">Power BI</a></li><li><a href="/docs/03-AdvancedSkills#quliksense">Quliksense</a></li></ul></li><li><a href="/docs/03-AdvancedSkills#Identity-and-device-management">Identity &amp; Device Management</a><ul><li><a href="/docs/03-AdvancedSkills#what-is-a-digital-twin">What Is A Digital Twin</a></li><li><a href="/docs/03-AdvancedSkills#active-directory">Active Directory</a></li></ul></li></ul></li><li><a href="/docs/03-AdvancedSkills#machine-learning">Machine Learning</a><ul><li><a href="/docs/03-AdvancedSkills#how-to-domachine-learning-in-production">How to do Machine Learning in production</a></li><li><a href="/docs/03-AdvancedSkills#why-machine-learning-in-production-is-harder-then-you-think">Why machine learning in production is harder then you think</a></li><li><a href="/docs/03-AdvancedSkills#models-do-not-work-forever">Models Do Not Work Forever</a></li><li><a href="/docs/03-AdvancedSkills#where-are-the-platforms-that-support-this">Where are The Platforms That Support Machine Learning</a></li><li><a href="/docs/03-AdvancedSkills#training-parameter-management">Training Parameter Management</a></li><li><a href="/docs/03-AdvancedSkills#how-to-convince-people-machine-learning-works">How to Convince People That Machine Learning Works</a></li><li><a href="/docs/03-AdvancedSkills#no-rules-no-physical-models">No Rules No Physical Models</a></li><li><a href="/docs/03-AdvancedSkills#you-have-the-data-use-it">You Have The Data. Use It!</a></li><li><a href="/docs/03-AdvancedSkills#data-is-stronger-than-opinions">Data is Stronger Than Opinions</a></li><li><a href="/docs/03-AdvancedSkills#aws-sagemaker">AWS Sagemaker</a></li></ul></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-science-platform"></a>Data Science Platform<a class="hash-link" href="#data-science-platform" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-a-good-data-platform-is-important"></a>Why a Good Data Platform Is Important<a class="hash-link" href="#why-a-good-data-platform-is-important" title="Direct link to heading">#</a></h3><p>| Podcast Episode: #066 How To Do Data Science From A Data Engineers Perspective<br>
|------------------|
|A simple introduction how to do data science in the context of the internet of things.
| <a href="https://youtu.be/yp_cc4R0mGQ" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/066-How-To-Do-Data-Science-From-A-Data-Engineers-Perspective-e45imt" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="big-data-vs-data-science-and-analytics"></a>Big Data vs Data Science and Analytics<a class="hash-link" href="#big-data-vs-data-science-and-analytics" title="Direct link to heading">#</a></h3><p>I talked about the difference in this podcast:
<a href="https://anchor.fm/andreaskayy/embed/episodes/BI-vs-Data-Science-vs-Big-Data-e199hq" target="_blank" rel="noopener noreferrer">https://anchor.fm/andreaskayy/embed/episodes/BI-vs-Data-Science-vs-Big-Data-e199hq</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="the-4-vs-of-big-data"></a>The 4 Vs of Big Data<a class="hash-link" href="#the-4-vs-of-big-data" title="Direct link to heading">#</a></h3><p>It is a complete misconception. Volume is only one part of the often
called four V&#x27;s of big data: Volume, velocity, variety and veracity.</p><p><strong>Volume</strong> is about the size - How much data you have</p><p><strong>Velocity</strong> is about the speed - How fast data is getting to you</p><p>How much data in a specific time needs to get processed or is coming
into the system. This is where the whole concept of streaming data and
real-time processing comes in to play.</p><p><strong>Variety</strong> is about the variety - How different your data is</p><p>Like CSV files, PDFs that you have and stuff in XML. That you also have
JSON logfiles, or data in some kind of a key-value store.</p><p>It&#x27;s about the variety of data types from different sources that you
basically want to join together. All to make an analysis based on that
data.</p><p><strong>Veracity</strong> is about the credibility - How reliable your data is</p><p>The issue with big data is, that it is very unreliable.</p><p>You cannot really trust the data. Especially when you&#x27;re coming from the
Internet of Things (IoT) side. Devices use sensors for measurement of
temperature, pressure, acceleration and so on.</p><p>You cannot always be hundred percent sure that the actual measurement is
right.</p><p>When you have data that is from for instance SAP and it contains data
that is created by hand you also have problems. As you know we humans
are bad at inputting stuff.</p><p>Everybody articulates differently. We make mistakes, down to the spelling
and that can be a very difficult issue for analytics.</p><p>I talked about the 4Vs in this podcast:
<a href="https://anchor.fm/andreaskayy/embed/episodes/4-Vs-Of-Big-Data-Are-Enough-e1h2ra" target="_blank" rel="noopener noreferrer">https://anchor.fm/andreaskayy/embed/episodes/4-Vs-Of-Big-Data-Are-Enough-e1h2ra</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-big-data"></a>Why Big Data?<a class="hash-link" href="#why-big-data" title="Direct link to heading">#</a></h3><p>What I always emphasize is that the four V&#x27;s are quite nice. They give you a
general direction.</p><p>There is a much more important issue: Catastrophic Success.</p><p>What I mean by catastrophic success is, that your project, your startup
or your platform has more growth that you anticipated. Exponential
growth is what everybody is looking for.</p><p>Because with exponential growth there is the money. It starts small and
gets very big very fast. The classic hockey stick curve:</p><p>1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384,
.... BOOM!</p><p>Think about it. It starts small and quite slow, but gets very big very
fast.</p><p>You get a lot of users or customers who are paying money to use your
service, the platform or whatever. If you have a system that is not
equipped to scale and process the data the whole system breaks down.</p><p>That&#x27;s catastrophic success. You are so successful and grow so fast that
you cannot fulfill the demand anymore. And so you fail and it&#x27;s all
over.</p><p>It&#x27;s now like you just can make that up while you go. That you can
foresee in a few months or weeks the current system doesn&#x27;t work
anymore.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="planning-is-everything"></a>Planning is Everything<a class="hash-link" href="#planning-is-everything" title="Direct link to heading">#</a></h3><p>It&#x27;s all happens very very fast and you cannot react anymore. There&#x27;s a
necessary type of planning and analyzing the potential of your business
case necessary.</p><p>Then you need to decide if you actually have big data or not.</p><p>You need to decide if you use big data tools. This means when you
conceptualize the whole infrastructure it might look ridiculous to
actually focus on big data tools.</p><p>But in the long run it will help you a lot. Good planning will get a lot
of problems out of the way, especially if you think about streaming data
and real-time analytics.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="the-problem-with-etl"></a>The problem with ETL<a class="hash-link" href="#the-problem-with-etl" title="Direct link to heading">#</a></h3><p>A typical old-school platform deployment would look like the picture
below. Devices use a data API to upload data that gets stored in a SQL
database. An external analytics tool is querying data and uploading the
results back to the SQL DB. Users then use the user interface to display
data stored in the database.</p><p><img alt="Common SQL Platform Architecture" src="/assets/images/Common-SQL-Architecture-af9716a79908e48a922ea0cd2963e4de.jpg"></p><p>Now, when the front end queries data from the SQL database the following
three steps happen:</p><p>- The database extracts all the needed rows from the storage. (E) - The
extracted data gets transformed, for instance sorted by timestamp or
something a lot more complex. (T) - The transformed data is loaded to
the destination (the user interface) for chart creation. (L)</p><p>With exploding amounts of stored data the ETL process starts being a
real problem.</p><p>Analytics is working with large data sets, for instance whole days,
weeks, months or more. Data sets are very big like 100GB or Terabytes.
That means Billions or Trillions of rows.</p><p>This has the result that the ETL process for large data sets takes
longer and longer. Very quickly the ETL performance gets so bad it won&#x27;t
deliver results to analytics anymore.</p><p>A traditional solution to overcome these performance issues is trying to
increase the performance of the database server. That&#x27;s what&#x27;s called
scaling up.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="scaling-up"></a>Scaling Up<a class="hash-link" href="#scaling-up" title="Direct link to heading">#</a></h3><p>To scale up the system and therefore increase ETL speeds administrators
resort to more powerful hardware by:</p><p>Speeding up the extract performance by adding faster disks to physically
read the data faster. Increasing RAM for row caching. What is already in
memory does not have to be read by slow disk drives. Using more powerful
CPU&#x27;s for better transform performance (more RAM helps here as well).
Increasing or optimising networking performance for faster data delivery
to the front end and analytics.</p><p>In summary: Scaling up the system is fairly easy.</p><p><img alt="Scaling up a SQL Database" src="/assets/images/SQL-Scaling-UP-38143fb9a3483b1665084bd9e195e4d1.jpg"></p><p>But with exponential growth it is obvious that sooner or later (more
sooner than later) you will run into the same problems again. At some
point you simply cannot scale up anymore because you already have a
monster system, or you cannot afford to buy more expensive hardware.</p><p>The next step you could take would be scaling out.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="scaling-out"></a>Scaling Out<a class="hash-link" href="#scaling-out" title="Direct link to heading">#</a></h3><p>Scaling out is the opposite of scaling up. Instead of building bigger
systems the goal is to distribute the load between many smaller systems.</p><p>The easiest way of scaling out an SQL database is using a storage area
network (SAN) to store the data. You can then use up to eight SQL
servers (explain), attach them to the SAN and let them handle queries.
This way load gets distributed between those eight servers.</p><p><img alt="Scaling out a SQL Database" src="/assets/images/SQL-Scaling-Out-8febba9e5eed6c87d2ca8cb1ace10f0d.jpg"></p><p>One major downside of this setup is that, because the storage is shared
between the SQL servers, it can only be used as an read only database.
Updates have to be done periodically, for instance once a day. To do
updates all SQL servers have to detach from the database. Then, one is
attaching the DB in read-write mode and refreshing the data. This
procedure can take a while if a lot of data needs to be uploaded.</p><p>This Link (missing) to a Microsoft MSDN page has more options of scaling
out an SQL database for you.</p><p>I deliberately don&#x27;t want to get into details about possible scaling out
solutions. The point I am trying to make is that while it is possible to
scale out SQL databases it is very complicated.</p><p>There is no perfect solution. Every option has its up- and downsides.
One common major issue is the administrative effort that you need to
take to implement and maintain a scaled out solution.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="please-dont-go-big-data"></a>Please don&#x27;t go Big Data<a class="hash-link" href="#please-dont-go-big-data" title="Direct link to heading">#</a></h3><p>If you don&#x27;t run into scaling issues please, do not use big data tools!</p><p>Big data is an expensive thing. A Hadoop cluster for instance needs at
least five servers to work properly. More is better.</p><p>Believe me this stuff costs a lot of money.</p><p>Especially when you are talking about maintenance and development on top
big data tools into account.</p><p>If you don&#x27;t need it it&#x27;s making absolutely no sense at all!</p><p>On the other side: If you really need big data tools they will save your
ass :)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="81-platform-and-pipeline-design-questions"></a>81 Platform and Pipeline Design Questions<a class="hash-link" href="#81-platform-and-pipeline-design-questions" title="Direct link to heading">#</a></h2><p>Many people ask: &quot;How do you select the platform, tools and design the pipelines?&quot;
The options seem infinite. Technology however should never dictate the decisions.</p><p>Here are 81 questions that you should answer when starting a project</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-source-questions"></a>Data Source Questions<a class="hash-link" href="#data-source-questions" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-origin-and-structure"></a>Data Origin and Structure<a class="hash-link" href="#data-origin-and-structure" title="Direct link to heading">#</a></h4><ul><li><strong>What is the source?</strong> Understand the &quot;device.&quot;</li><li><strong>What is the format of the incoming data?</strong> (e.g., JSON, CSV, Avro, Parquet)</li><li><strong>What’s the schema?</strong></li><li><strong>Is the data structured, semi-structured, or unstructured?</strong></li><li><strong>What is the data type?</strong> Understand the content of the data.</li><li><strong>Is the schema well-defined, or is it dynamic?</strong></li><li><strong>How are changes in the data structure from the source (schema evolution) handled?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-volume--velocity"></a>Data Volume &amp; Velocity<a class="hash-link" href="#data-volume--velocity" title="Direct link to heading">#</a></h4><ul><li><strong>How much data is transmitted per transmission?</strong></li><li><strong>How fast is the data coming in?</strong> (e.g., messages per minute)</li><li><strong>What is the maximum data volume expected per source per day?</strong></li><li><strong>What scaling of sources/data is expected?</strong></li><li><strong>Are there peaks for incoming data?</strong></li><li><strong>How much data is posted per day across all sources?</strong></li><li><strong>How does the data volume fluctuate?</strong> (e.g., seasonal peaks, hourly/daily variations)</li><li><strong>How will the system handle bursts of data?</strong> (e.g., throttling or buffering)</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="source-reliability--redundancy"></a>Source Reliability &amp; Redundancy<a class="hash-link" href="#source-reliability--redundancy" title="Direct link to heading">#</a></h4><ul><li><strong>Is there data arriving late?</strong></li><li><strong>Is there a risk of duplicate data from the source?</strong> How will we handle de-duplication?</li><li><strong>How reliable are the sources?</strong> What’s the expected failure rate?</li><li><strong>How do we handle data corruption or loss during transmission?</strong></li><li><strong>What happens if a source goes offline?</strong> Is there a fallback or failover source?</li><li><strong>Do we need to retry failed transmissions or have fault-tolerance mechanisms in place?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-extraction--new-sources"></a>Data Extraction &amp; New Sources<a class="hash-link" href="#data-extraction--new-sources" title="Direct link to heading">#</a></h4><ul><li><strong>Do we need to extract the data from the sources?</strong></li><li><strong>How many sources are there?</strong></li><li><strong>Will new sources be implemented?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-source-connectivity--authentication"></a>Data Source Connectivity &amp; Authentication<a class="hash-link" href="#data-source-connectivity--authentication" title="Direct link to heading">#</a></h4><ul><li><strong>How is the data arriving?</strong> (API, bucket, etc.)</li><li><strong>How is the authentication done?</strong></li><li><strong>What kind of connection is required for the data source?</strong> (e.g., streaming, batch, API)</li><li><strong>What protocols are used for data ingestion?</strong> (e.g., REST, WebSocket, FTP)</li><li><strong>Are there any rate limits or quotas imposed by the data source?</strong></li><li><strong>How do we handle credentials?</strong> Is there an API?</li><li><strong>What is the retry strategy if data fails to be processed or transmitted?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-security--compliance"></a>Data Security &amp; Compliance<a class="hash-link" href="#data-security--compliance" title="Direct link to heading">#</a></h4><ul><li><strong>Does the data need to be encrypted at the source before being transmitted?</strong></li><li><strong>Are there any compliance frameworks (e.g., GDPR, HIPAA) that the source data must adhere to?</strong></li><li><strong>Is there a requirement for data masking or obfuscation at the source?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metadata--audit"></a>Metadata &amp; Audit<a class="hash-link" href="#metadata--audit" title="Direct link to heading">#</a></h4><ul><li><strong>Is there metadata for the client transmission stored somewhere?</strong></li><li><strong>What metadata should be captured for each transmission?</strong> (e.g., record counts, latency)</li><li><strong>How do we track and log data ingestion events for audit purposes?</strong></li><li><strong>Is there a need for tracking data lineage?</strong> (i.e., source origin and changes over time)</li></ul><hr><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="goals-and-destination-questions"></a>Goals and Destination Questions<a class="hash-link" href="#goals-and-destination-questions" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="use-case--data-consumption"></a>Use Case &amp; Data Consumption<a class="hash-link" href="#use-case--data-consumption" title="Direct link to heading">#</a></h4><ul><li><strong>What kind of use case is this?</strong> (Analytics, BI, ML, Transactional processing, Visualization, User Interfaces, APIs)</li><li><strong>What are the typical use cases that require this data?</strong> (e.g., predictive analytics, operational dashboards)</li><li><strong>What are the downstream systems or platforms that will consume this data?</strong></li><li><strong>How critical is real-time data versus historical data in this use case?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-query--delivery"></a>Data Query &amp; Delivery<a class="hash-link" href="#data-query--delivery" title="Direct link to heading">#</a></h4><ul><li><strong>How is the data visualized?</strong> (raw data, aggregated data)</li><li><strong>How much raw data is processed at once?</strong></li><li><strong>How much data is cold data, and how often is cold data queried?</strong></li><li><strong>How fast do the results need to appear?</strong></li><li><strong>How much data is going to be queried at once?</strong></li><li><strong>How fresh does the data need to be?</strong></li><li><strong>How often is the data queried?</strong> (frequency)</li><li><strong>What are the SLAs for delivering data to downstream systems or applications?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aggregation--modeling"></a>Aggregation &amp; Modeling<a class="hash-link" href="#aggregation--modeling" title="Direct link to heading">#</a></h4><ul><li><strong>How is the data aggregated?</strong> (by devices, topic, time)</li><li><strong>When does the aggregation happen?</strong> (on query, on schedule, while streaming)</li><li><strong>What kind of data models are needed for this use case?</strong> (e.g., star schema, snowflake schema)</li><li><strong>Is there a need for pre-aggregations to speed up queries?</strong></li><li><strong>Should partitioning or indexing strategies be implemented to optimize query performance?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="performance--availability"></a>Performance &amp; Availability<a class="hash-link" href="#performance--availability" title="Direct link to heading">#</a></h4><ul><li><strong>What is the processing time requirement?</strong></li><li><strong>What is the availability of analytics output?</strong> (input vs output delay)</li><li><strong>How fresh does the data need to be?</strong></li><li><strong>What are the performance expectations for query speed?</strong></li><li><strong>What is the acceptable query response time for end-users?</strong></li><li><strong>How will the system handle an increase in concurrent queries from multiple users?</strong></li><li><strong>What is the expected lag between data ingestion and availability for querying?</strong></li><li><strong>Do we need horizontal scaling for query engines or databases?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-lifecycle--retention"></a>Data Lifecycle &amp; Retention<a class="hash-link" href="#data-lifecycle--retention" title="Direct link to heading">#</a></h4><ul><li><strong>What’s the data retention time?</strong></li><li><strong>How often is data archived or moved to lower-cost storage?</strong></li><li><strong>Will old data need to be transformed or reprocessed for new use cases?</strong></li><li><strong>What are the data retention policies?</strong> (e.g., hot vs cold storage)</li><li><strong>How will the use case evolve as the data grows?</strong> Will this affect how data is consumed or visualized?</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="monitoring--debugging"></a>Monitoring &amp; Debugging<a class="hash-link" href="#monitoring--debugging" title="Direct link to heading">#</a></h4><ul><li><strong>How will data delivery to the destination be monitored?</strong> (e.g., time-to-load, query failures)</li><li><strong>How will we monitor data pipeline health at the destination?</strong> (e.g., throughput, latency)</li><li><strong>What tools or methods will be used for debugging data delivery failures or performance bottlenecks?</strong></li><li><strong>What metrics should be tracked to ensure data pipeline health?</strong> (e.g., latency, throughput)</li><li><strong>How do we handle issues such as data corruption or incomplete data at the destination?</strong></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-access--permissions"></a>Data Access &amp; Permissions<a class="hash-link" href="#data-access--permissions" title="Direct link to heading">#</a></h4><ul><li><strong>Who is working with the platform, and who has access to query or visualize the data?</strong></li><li><strong>Which tools are used to query the data?</strong></li><li><strong>What kind of data export capabilities are required?</strong> (e.g., CSV, API, direct database access)</li><li><strong>Is role-based access control (RBAC) needed to segment data views for different users?</strong></li><li><strong>How will access to sensitive data be managed?</strong> (e.g., row-level security, encryption)</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="scaling--future-requirements"></a>Scaling &amp; Future Requirements<a class="hash-link" href="#scaling--future-requirements" title="Direct link to heading">#</a></h4><ul><li><strong>What are the scalability requirements for the data platform as data volume grows?</strong></li><li><strong>How will future business goals or scalability needs affect the design of data aggregation and retention strategies?</strong></li><li><strong>How will the system handle an increasing load as more users query data or as data volume grows?</strong></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="connect"></a>Connect<a class="hash-link" href="#connect" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="rest-apis"></a>REST APIs<a class="hash-link" href="#rest-apis" title="Direct link to heading">#</a></h3><p>APIs or Application Programming Interfaces are the cornerstones of any
great data platform.</p><p>| Podcast Episode: #033 How APIs Rule The World
|------------------|
|Strong APIs make a good platform. In this episode I talk about why you need APIs and why Twitter is a great example. Especially JSON APIs are my personal favorite. Because JSON is also important in the Big Data world, for instance in log analytics. How? Check out this episode!<br>
| <a href="https://anchor.fm/andreaskayy/episodes/How-APIs-Rule-The-World--PoDS-033-e24ttq" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="api-design"></a>API Design<a class="hash-link" href="#api-design" title="Direct link to heading">#</a></h4><p>In this podcast episode we look into the Twitter API. It&#x27;s a great
example how to build an API</p><p>| Podcast Episode: #081 Twitter API Research Data Engineering Course Part 5
|------------------|
|In this episode we look into the Twitter API documentation, which I love by the way. How can we get old tweets for a certain hashtags and how to get current live tweets for these hashtags?
| <a href="https://youtu.be/UnAXKxeIlyg" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/081-How-to-get-tweets-from-the-Twitter-API-e45j32" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="payload-compression-attacks"></a>Payload compression attacks<a class="hash-link" href="#payload-compression-attacks" title="Direct link to heading">#</a></h4><p>How to defend your Server with zip Bombs
<a href="https://www.sitepoint.com/how-to-defend-your-website-with-zip-bombs/" target="_blank" rel="noopener noreferrer">https://www.sitepoint.com/how-to-defend-your-website-with-zip-bombs/</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="implementation-frameworks"></a>Implementation Frameworks<a class="hash-link" href="#implementation-frameworks" title="Direct link to heading">#</a></h4><p>Jersey:</p><p><a href="https://eclipse-ee4j.github.io/jersey.github.io/documentation/latest/getting-started.html" target="_blank" rel="noopener noreferrer">https://eclipse-ee4j.github.io/jersey.github.io/documentation/latest/getting-started.html</a></p><p>Tutorial – REST API design and implementation in Java with Jersey and Spring:
<a href="https://www.codepedia.org/ama/tutorial-rest-api-design-and-implementation-in-java-with-jersey-and-spring/" target="_blank" rel="noopener noreferrer">https://www.codepedia.org/ama/tutorial-rest-api-design-and-implementation-in-java-with-jersey-and-spring/</a></p><p>Swagger:</p><p><a href="https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Getting-started" target="_blank" rel="noopener noreferrer">https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Getting-started</a></p><p>Jersey vs Swagger:</p><p><a href="https://stackoverflow.com/questions/36997865/what-is-the-difference-between-swagger-api-and-jax-rs" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/36997865/what-is-the-difference-between-swagger-api-and-jax-rs</a></p><p>Spring Framework:</p><p><a href="https://spring.io/" target="_blank" rel="noopener noreferrer">https://spring.io/</a></p><p>When to use Spring or Jersey:</p><p><a href="https://stackoverflow.com/questions/26824423/what-is-the-difference-among-spring-rest-service-and-jersey-rest-service-and-spr" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/26824423/what-is-the-difference-among-spring-rest-service-and-jersey-rest-service-and-spr</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="oauth-security"></a>OAuth security<a class="hash-link" href="#oauth-security" title="Direct link to heading">#</a></h4><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-nifi"></a>Apache Nifi<a class="hash-link" href="#apache-nifi" title="Direct link to heading">#</a></h3><p>Nifi is one of these tools that I identify as high potential tools. It
allows you to create a data pipeline very easily.</p><p>Read data from a RestAPI and post it to Kafka? No problem Read data from
Kafka and put it into a database? No problem</p><p>It&#x27;s super versatile and you can do everything on the UI.</p><p>I use it in Part 3 of this Document. Check it out.</p><p>Check out the Apache Nifi FAQ website. Also look into the documentation
to find all possible data sources and sinks of Nifi:</p><p><a href="https://nifi.apache.org/faq.html" target="_blank" rel="noopener noreferrer">https://nifi.apache.org/faq.html</a></p><p>Here&#x27;s a great blog about Nifi:</p><p><a href="https://www.datainmotion.dev" target="_blank" rel="noopener noreferrer">https://www.datainmotion.dev</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="logstash"></a>Logstash<a class="hash-link" href="#logstash" title="Direct link to heading">#</a></h3><p><a href="https://www.elastic.co/products/logstash" target="_blank" rel="noopener noreferrer">https://www.elastic.co/products/logstash</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="fluentd"></a>FluentD<a class="hash-link" href="#fluentd" title="Direct link to heading">#</a></h3><p>Data Collector</p><p><a href="https://www.fluentd.org/" target="_blank" rel="noopener noreferrer">https://www.fluentd.org/</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-flume"></a>Apache Flume<a class="hash-link" href="#apache-flume" title="Direct link to heading">#</a></h3><p><a href="https://flume.apache.org/" target="_blank" rel="noopener noreferrer">https://flume.apache.org/</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sqoop"></a>Sqoop<a class="hash-link" href="#sqoop" title="Direct link to heading">#</a></h3><p><a href="https://sqoop.apache.org/" target="_blank" rel="noopener noreferrer">https://sqoop.apache.org/</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="azure-iothub"></a>Azure IoTHub<a class="hash-link" href="#azure-iothub" title="Direct link to heading">#</a></h3><p><a href="https://azure.microsoft.com/en-us/services/iot-hub/" target="_blank" rel="noopener noreferrer">https://azure.microsoft.com/en-us/services/iot-hub/</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="buffer"></a>Buffer<a class="hash-link" href="#buffer" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-kafka"></a>Apache Kafka<a class="hash-link" href="#apache-kafka" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-a-message-queue-tool"></a>Why a message queue tool?<a class="hash-link" href="#why-a-message-queue-tool" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="kafka-architecture"></a>Kafka architecture<a class="hash-link" href="#kafka-architecture" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-are-topics"></a>What are topics<a class="hash-link" href="#what-are-topics" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-does-zookeeper-have-to-do-with-kafka"></a>What does Zookeeper have to do with Kafka<a class="hash-link" href="#what-does-zookeeper-have-to-do-with-kafka" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-produce-and-consume-messages"></a>How to produce and consume messages<a class="hash-link" href="#how-to-produce-and-consume-messages" title="Direct link to heading">#</a></h4><p>My YouTube video how to set up Kafka at home:
<a href="https://youtu.be/7F9tBwTUSeY" target="_blank" rel="noopener noreferrer">https://youtu.be/7F9tBwTUSeY</a></p><p>My YouTube video how to write to Kafka: <a href="https://youtu.be/RboQBZvZCh0" target="_blank" rel="noopener noreferrer">https://youtu.be/RboQBZvZCh0</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="kafka-commands"></a>KAFKA Commands<a class="hash-link" href="#kafka-commands" title="Direct link to heading">#</a></h4><p>Start Zookeeper container for Kafka:</p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">docker run -d --name zookeeper-server   \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    --network app-tier   \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    -e ALLOW_ANONYMOUS_LOGIN=yes    \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    bitnami/zookeeper:latest</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>Start Kafka container:</p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">docker run -d --name kafka-server  \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    --network app-tier  \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    -e KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper-server:2181  \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    -e ALLOW_PLAINTEXT_LISTENER=yes  \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    bitnami/kafka:latest</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="redis-pub-sub"></a>Redis Pub-Sub<a class="hash-link" href="#redis-pub-sub" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aws-kinesis"></a>AWS Kinesis<a class="hash-link" href="#aws-kinesis" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="google-cloud-pubsub"></a>Google Cloud PubSub<a class="hash-link" href="#google-cloud-pubsub" title="Direct link to heading">#</a></h3><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="processing-frameworks"></a>Processing Frameworks<a class="hash-link" href="#processing-frameworks" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="lambda-and-kappa-architecture"></a>Lambda and Kappa Architecture<a class="hash-link" href="#lambda-and-kappa-architecture" title="Direct link to heading">#</a></h3><p>| Podcast Episode: #077 Lambda Architecture and Kappa Architecture
|------------------|
|In this stream we talk about the lambda architecture with stream and batch processing as well as a alternative the Kappa Architecture that consists only of streaming. Also Data engineer vs data scientist and we discuss Andrew Ng’s AI Transformation Playbook.<br>
| <a href="https://youtu.be/iUOQPyHN9-0" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/077-Lambda--Kappa-Architecture-e45j0r" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="batch-processing"></a>Batch Processing<a class="hash-link" href="#batch-processing" title="Direct link to heading">#</a></h3><p>Ask the big questions. Remember your last yearly tax statement?</p><p>You break out the folders. You run around the house searching for the
receipts.</p><p>All that fun stuff.</p><p>When you finally found everything you fill out the form and send it on
its way.</p><p>Doing the tax statement is a prime example of a batch process.</p><p>Data comes in and gets stored, analytics loads the data from storage and
creates an output (insight):</p><p><img alt="Batch Processing Pipeline" src="/assets/images/Simple-Batch-Processing-Workflow-b5170c12d68563b3b53bcdbe17a6a043.jpg"></p><p>Batch processing is something you do either without a schedule or on a
schedule (tax statement). It is used to ask the big questions and gain
the insights by looking at the big picture.</p><p>To do so, batch processing jobs use large amounts of data. This data is
provided by storage systems like Hadoop HDFS.</p><p>They can store lots of data (petabytes) without a problem.</p><p>Results from batch jobs are very useful, but the execution time is high.
Because the amount of used data is high.</p><p>It can take minutes or sometimes hours until you get your results.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="stream-processing"></a>Stream Processing<a class="hash-link" href="#stream-processing" title="Direct link to heading">#</a></h3><p>Gain instant insight into your data.</p><p>Streaming allows users to make quick decisions and take actions based on
&quot;real-time&quot; insight. Contrary to batch processing, streaming processes
data on the fly, as it comes in.</p><p>With streaming you don&#x27;t have to wait minutes or hours to get results.
You gain instant insight into your data.</p><p>In the batch processing pipeline, the analytics was after the data
storage. It had access to all the available data.</p><p>Stream processing creates insight before the data storage. It has only
access to fragments of data as it comes in.</p><p>As a result the scope of the produced insight is also limited. Because
the big picture is missing.</p><p><img alt="Stream Processing Pipeline" src="/assets/images/Simple-Stream-Processing-Workflow-16a79d3da46b9f6c256d7a42effcd3ff.jpg"></p><p>Only with streaming analytics you are able to create advanced services
for the customer. Netflix for instance incorporated stream processing
into Chuckwa V2.0 and the new Keystone pipeline.</p><p>One example of advanced services through stream processing is the
Netflix &quot;Trending Now&quot; feature. Check out the Netflix case study.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="three-methods-of-streaming"></a>Three methods of streaming<a class="hash-link" href="#three-methods-of-streaming" title="Direct link to heading">#</a></h4><p>In stream processing sometimes it is ok to drop messages, other times it
is not. Sometimes it is fine to process a message multiple times, other
times that needs to be avoided like hell.</p><p>Today&#x27;s topic are the different methods of streaming: At most once, at
least once and exactly once.</p><p>What this means and why it is so important to keep them in mind when
creating a solution. That is what you will find out in this article.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="at-least-once"></a>At Least Once<a class="hash-link" href="#at-least-once" title="Direct link to heading">#</a></h4><p>At least once, means a message gets processed in the system once or
multiple times. So with at least once it&#x27;s not possible that a message
gets into the system and is not getting processed.</p><p>It&#x27;s not getting dropped or lost somewhere in the system.</p><p>One example where at least once processing can be used is when you think
about a fleet management of cars. You get GPS data from cars and that
data is transmitted with a timestamp and the GPS coordinates.</p><p>It&#x27;s important that you get the GPS data at least once, so you know
where the car is. If you&#x27;re processing this data multiple times, it
always has the the timestamp with it.</p><p>Because of that it does not matter that it gets processed multiple
times, because of the timestamp. Or that it would be stored multiple
times, because it would just override the existing one.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="at-most-once"></a>At Most Once<a class="hash-link" href="#at-most-once" title="Direct link to heading">#</a></h4><p>The second streaming method is at most once. At most once means that
it&#x27;s okay to drop some information, to drop some messages.</p><p>But it&#x27;s important that a message is only processed once as a
maximum.</p><p>A example for this is event processing. Some event is happening and that
event is not important enough, so it can be dropped. It doesn&#x27;t have any
consequences when it gets dropped.</p><p>But when that event happens it&#x27;s important that it does not get
processed multiple times. Then it would look as if the event happened
five or six times instead of only one.</p><p>Think about engine misfires. If it happens once, no big deal. But if the
system tells you it happens a lot you will think you have a problem with
your engine.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="exactly-once"></a>Exactly Once<a class="hash-link" href="#exactly-once" title="Direct link to heading">#</a></h4><p>Another thing is exactly once, this means it&#x27;s not okay to drop data,
it&#x27;s not okay to lose data and it&#x27;s also not okay to process data
multiple times.</p><p>An example for this is banking. When you think about credit card
transactions it&#x27;s not okay to drop a transaction.</p><p>When dropped, your payment is not going through. It&#x27;s also not okay to
have a transaction processed multiple times, because then you are paying
multiple times.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="check-the-tools"></a>Check The Tools!<a class="hash-link" href="#check-the-tools" title="Direct link to heading">#</a></h4><p>All of this sounds very simple and logical. What kind of processing is
done has to be a requirement for your use case.</p><p>It needs to be thought about in the design process, because not every
tool is supporting all three methods. Very often you need to code your
application very differently based on the streaming method.</p><p>Especially exactly once is very hard to do.</p><p>So, the tool of data processing needs to be chosen based on if you need
exactly once, at least once or if you need at most once.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="should-you-do-stream-or-batch-processing"></a>Should you do stream or batch processing?<a class="hash-link" href="#should-you-do-stream-or-batch-processing" title="Direct link to heading">#</a></h3><p>It is a good idea to start with batch processing. Batch processing is
the foundation of every good big data platform.</p><p>A batch processing architecture is simple, and therefore quick to set
up. Platform simplicity means, it will also be relatively cheap to run.</p><p>A batch processing platform will enable you to quickly ask the big
questions. They will give you invaluable insight into your data and
customers.</p><p>When the time comes and you also need to do analytics on the fly, then
add a streaming pipeline to your batch processing big data platform.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="is-etl-still-relevant-for-analytics"></a>Is ETL still relevant for Analytics?<a class="hash-link" href="#is-etl-still-relevant-for-analytics" title="Direct link to heading">#</a></h3><p>| Podcast Episode: #039 Is ETL Dead For Data Science &amp; Big Data?
|------------------|
|Is ETL dead in Data Science and Big Data? In today’s podcast I share with you my views on your questions regarding ETL (extract, transform, load). Is ETL still practiced or did pre-processing &amp; cleansing replace it. What would replace ETL in Data Engineering.
| <a href="https://youtu.be/leSOWPaNkl4" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/Is-ETL-Dead-For-Data-Science--Big-Data---PoDS-039-e2b604" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="mapreduce"></a>MapReduce<a class="hash-link" href="#mapreduce" title="Direct link to heading">#</a></h3><p>Since the early days of the Hadoop eco system, the MapReduce framework
is one of the main components of Hadoop alongside HDFS.</p><p>Google for instance used MapReduce to analyse stored HTML content of
websites through counting all the HTML tags and all the words and
combinations of them (for instance headlines). The output was then used
to create the page ranking for Google Search.</p><p>That was when everybody started to optimise his website for the google
search. Serious search engine optimisation was born. That was the year
2004.</p><p>How MapReduce is working is, that it processes data in two phases: The
map phase and the reduce phase.</p><p>In the map phase, the framework is reading data from HDFS. Each dataset
is called an input record.</p><p>Then there is the reduce phase. In the reduce phase, the actual
computation is done and the results are stored. The storage target can
either be a database or back HDFS or something else.</p><p>After all it&#x27;s Java -- so you can implement what you like.</p><p>The magic of MapReduce is how the map and reduce phase are implemented
and how both phases are working together.</p><p>The map and reduce phases are parallelised. What that means is, that you
have multiple map phases (mappers) and reduce phases (reducers) that can
run in parallel on your cluster machines.</p><p>Here&#x27;s an example how such a map and reduce process works with data:</p><p><img alt="Mapping of input files and reducing of mapped records" src="/assets/images/MapReduce-Process-Detailed-6d1e75a3058740de2c56cbb2c558316b.jpg"></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-mapreduce-work"></a>How does MapReduce work<a class="hash-link" href="#how-does-mapreduce-work" title="Direct link to heading">#</a></h4><p>First of all, the whole map and reduce process relies heavily on using
key-value pairs. That&#x27;s what the mappers are for.</p><p>In the map phase input data, for instance a file, gets loaded and
transformed into key-value pairs.</p><p>When each map phase is done it sends the created key-value pairs to the
reducers where they are getting sorted by key. This means, that an input
record for the reduce phase is a list of values from the mappers that
all have the same key.</p><p>Then the reduce phase is doing the computation of that key and its
values and outputting the results.</p><p>How many mappers and reducers can you use in parallel? The number of
parallel map and reduce processes depends on how many CPU cores you have
in your cluster. Every mapper and every reducer is using one core.</p><p>This means that the more CPU cores you actually have, the more mappers
you can use, the faster the extraction process can be done. The more
reducers you are using the faster the actual computation is being done.</p><p>To make this more clear, I have prepared an example:</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example"></a>Example<a class="hash-link" href="#example" title="Direct link to heading">#</a></h4><p>As I said before, MapReduce works in two stages, map and reduce. Often
these stages are explained with a word count task.</p><p>Personally, I hate this example because counting stuff is to trivial and
does not really show you what you can do with MapReduce. Therefore, we
are going to use a more real world use-case from the IoT world.</p><p>IoT applications create an enormous amount of data that has to be
processed. This data is generated by physical sensors who take
measurements, like room temperature at 8 o&#x27;clock.</p><p>Every measurement consists of a key (the timestamp when the measurement
has been taken) and a value (the actual value measured by the sensor).</p><p>Because you usually have more than one sensor on your machine, or
connected to your system, the key has to be a compound key. Compound
keys contain in addition to the measurement time information about the
source of the signal.</p><p>But, let&#x27;s forget about compound keys for now. Today we have only one
sensor. Each measurement outputs key-value pairs like: Timestamp-Value.</p><p>The goal of this exercise is to create average daily values of that
sensor&#x27;s data.</p><p>The image below shows how the map and reduce process works.</p><p>First, the map stage loads unsorted data (input records) from the source
(e.g. HDFS) by key and value (key:2016-05-01 01:02:03, value:1).</p><p>Then, because the goal is to get daily averages, the hour:minute:second
information is cut from the timestamp.</p><p>That is all that happens in the map phase, nothing more.</p><p>After all parallel map phases are done, each key-value pair gets sent to
the one reducer who is handling all the values for this particular key.</p><p>Every reducer input record then has a list of values and you can
calculate (1+5+9)/3, (2+6+7)/3 and (3+4+8)/3. That&#x27;s all.</p><p><img alt="MapReduce Example of Time Series Data" src="/assets/images/MapReduce-Time-Series-example-18f17d9f48d69162cc9c6e651b88fb34.jpg"></p><p>What do you think you need to do to generate minute averages?</p><p>Yes, you need to cut the key differently. You then would need to cut it
like this: &quot;2016-05-01 01:02&quot;, keeping the hour and minute information
in the key.</p><p>What you can also see is, why map reduce is so great for doing parallel
work. In this case, the map stage could be done by nine mappers in
parallel because each map is independent from all the others.</p><p>The reduce stage could still be done by three tasks in parallel. One for
orange, one for blue and one for green.</p><p>That means, if your dataset would be 10 times as big and you&#x27;d have 10
times the machines, the time to do the calculation would be the same.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-the-limitation-of-mapreduce"></a>What is the limitation of MapReduce?<a class="hash-link" href="#what-is-the-limitation-of-mapreduce" title="Direct link to heading">#</a></h4><p>MapReduce is awesome for simpler analytics tasks, like counting stuff.
It just has one flaw: It has only two stages Map and Reduce.</p><p><img alt="The Map Reduce Process" src="/assets/images/MapReduce-Process-4e2d0eed4225d2285ac7b163022ef4ed.jpg"></p><p>First MapReduce loads the data from HDFS into the mapping function.
There you prepare the input data for the processing in the reducer.
After the reduce is finished the results get written to the data store.</p><p>The problem with MapReduce is that there is no simple way to chain
multiple map and reduce processes together. At the end of each reduce
process the data must be stored somewhere.</p><p>This fact makes it very hard to do complicated analytics processes. You
would need to chain MapReduce jobs together.</p><p>Chaining jobs with storing and loading intermediate results just makes
no sense.</p><p>Another issue with MapReduce is that it is not capable of streaming
analytics. Jobs take some time to spin up, do the analytics and shut
down. Basically Minutes of wait time are totally normal.</p><p>This is a big negative point in a more and more real time data
processing world.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-spark"></a>Apache Spark<a class="hash-link" href="#apache-spark" title="Direct link to heading">#</a></h3><p>I talked about the three methods of data streaming in this podcast:
<a href="https://anchor.fm/andreaskayy/embed/episodes/Three-Methods-of-Streaming-Data-e15r6o" target="_blank" rel="noopener noreferrer">https://anchor.fm/andreaskayy/embed/episodes/Three-Methods-of-Streaming-Data-e15r6o</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-the-difference-to-mapreduce"></a>What is the difference to MapReduce?<a class="hash-link" href="#what-is-the-difference-to-mapreduce" title="Direct link to heading">#</a></h4><p>Spark is a complete in-memory framework. Data gets loaded from, for
instance HDFS, into the memory of workers.</p><p>There is no longer a fixed map and reduce stage. Your code can be as
complex as you want.</p><p>Once in memory, the input data and the intermediate results stay in
memory (until the job finishes). They do not get written to a drive like
with MapReduce.</p><p>This makes Spark the optimal choice for doing complex analytics. It
allows you for instance to do iterative processes. Modifying a dataset
multiple times in order to create an output is totally easy.</p><p>Streaming analytics capability is also what makes Spark so great. Spark
has natively the option to schedule a job to run every X seconds or X
milliseconds.</p><p>As a result, Spark can deliver you results from streaming data in &quot;real
time&quot;.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-spark-fit-to-hadoop"></a>How does Spark fit to Hadoop?<a class="hash-link" href="#how-does-spark-fit-to-hadoop" title="Direct link to heading">#</a></h4><p>There are some very misleading articles out there titled \&quot;Spark or
Hadoop\&quot;, \&quot;Spark is better than Hadoop\&quot; or even \&quot;Spark is replacing
Hadoop\&quot;.</p><p>So, it&#x27;s time to show you the differences between Spark and Hadoop.
After this you will know when and for what you should use Spark and
Hadoop.</p><p>You&#x27;ll also understand why \&quot;Hadoop or Spark\&quot; is the totally wrong
question.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="wheres-the-difference"></a>Where&#x27;s the difference?<a class="hash-link" href="#wheres-the-difference" title="Direct link to heading">#</a></h4><p>To make it clear how Hadoop differs from Spark I created this simple
feature table:</p><p><img alt="Hadoop vs Spark capabilities" src="/assets/images/Table-Hadoop-and-Spark-94a14b867dd6350f83d2cca4ba4c4bac.jpg"></p><p>Hadoop is used to store data in the Hadoop Distributed File System
(HDFS). It can analyse the stored data with MapReduce and manage
resources with YARN.</p><p>However, Hadoop is more than just storage, analytics and resource
management. There&#x27;s a whole eco system of tools around the Hadoop core.
I&#x27;ve written about its eco system in this article: <a href="/docs/missing">missing</a>
What is Hadoop and why is it so freakishly popular. You should check it
out as well.</p><p>Compared to Hadoop, Spark is &quot;just&quot; an analytics framework. It has no
storage capability. Although it has a standalone resource management,
you usually don&#x27;t use that feature.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spark-and-hadoop-is-a-perfect-fit"></a>Spark and Hadoop is a perfect fit<a class="hash-link" href="#spark-and-hadoop-is-a-perfect-fit" title="Direct link to heading">#</a></h4><p>So, if Hadoop and Spark are not the same things, can they work together?</p><p>Absolutely! Here&#x27;s how the first picture will look if you combine Hadoop
with Spark:</p><p>missing</p><p>As Storage you use HDFS. Analytics is done with Apache Spark and YARN is
taking care of the resource management.</p><p>Why does that work so well together?</p><p>From a platform architecture perspective, Hadoop and Spark are usually
managed on the same cluster. This means on each server where a HDFS data
node is running, a Spark worker thread runs as well.</p><p>In distributed processing, network transfer between machines is a large
bottle neck. Transferring data within a machine reduces this traffic
significantly.</p><p>Spark is able to determine on which data node the needed data is stored.
This allows a direct load of the data from the local storage into the
memory of the machine.</p><p>This reduces network traffic a lot.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spark-on-yarn"></a>Spark on YARN:<a class="hash-link" href="#spark-on-yarn" title="Direct link to heading">#</a></h4><p>You need to make sure that your physical resources are distributed
perfectly between the services. This is especially the case when you run
Spark workers with other Hadoop services on the same machine.</p><p>It just would not make sense to have two resource managers managing the
same server&#x27;s resources. Sooner or later they will get in each others
way.</p><p>That&#x27;s why the Spark standalone resource manager is seldom used.</p><p>So, the question is not Spark or Hadoop. The question has to be: Should
you use Spark or MapReduce alongside Hadoop&#x27;s HDFS and YARN.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="my-simple-rule-of-thumb"></a>My simple rule of thumb:<a class="hash-link" href="#my-simple-rule-of-thumb" title="Direct link to heading">#</a></h4><p>If you are doing simple batch jobs like counting values or doing
calculating averages: Go with MapReduce.</p><p>If you need more complex analytics like machine learning or fast stream
processing: Go with Apache Spark.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="available-languages"></a>Available Languages<a class="hash-link" href="#available-languages" title="Direct link to heading">#</a></h4><p>Spark jobs can be programmed in a variety of languages. That makes
creating analytic processes very user-friendly for data scientists.</p><p>Spark supports Python, Scala and Java. With the help of SparkR you can
even connect your R program to a Spark cluster.</p><p>If you are a data scientist who is very familiar with Python just use
Python, its great. If you know how to code Java I suggest you start
using Scala.</p><p>Spark jobs are easier to code in Scala than in Java. In Scala you can
use anonymous functions to do processing.</p><p>This results in less overhead, it is a much cleaner, simpler code.</p><p>With Java 8 simplified function calls were introduced with lambda
expressions. Still, a lot of people, including me prefer Scala over
Java.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-spark-works-driver-executor-sparkcontext"></a>How Spark works: Driver, Executor, Sparkcontext<a class="hash-link" href="#how-spark-works-driver-executor-sparkcontext" title="Direct link to heading">#</a></h4><p>| Podcast Episode: #100 Apache Spark Week Day 1
|------------------|
|Is ETL dead in Data Science and Big Data? In today’s podcast I share with you my views on your questions regarding ETL (extract, transform, load). Is ETL still practiced or did pre-processing &amp; cleansing replace it. What would replace ETL in Data Engineering.
| <a href="https://youtu.be/qD6Wi2pfCx0" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spark-batch-vs-stream-processing"></a>Spark batch vs stream processing<a class="hash-link" href="#spark-batch-vs-stream-processing" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-spark-use-data-from-hadoop"></a>How does Spark use data from Hadoop<a class="hash-link" href="#how-does-spark-use-data-from-hadoop" title="Direct link to heading">#</a></h4><p>Another thing is data locality. I always make the point, that processing
data locally where it is stored is the most efficient thing to do.</p><p>That&#x27;s exactly what Spark is doing. You can and should run Spark workers
directly on the data nodes of your Hadoop cluster.</p><p>Spark can then natively identify on what data node the needed data is
stored. This enables Spark to use the worker running on the machine
where the data is stored to load the data into the memory.</p><p><img alt="Spark Using Hadoop Data Locality" src="/assets/images/Spark-Data-Locality-dfdb28895a846d38173cbc258f3f8084.jpg"></p><p>The downside of this setup is that you need more expensive servers.
Because Spark processing needs stronger servers with more RAM and CPUs
than a &quot;pure&quot; Hadoop setup.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-are-rdds-and-how-to-use-them"></a>What are RDDs and how to use them<a class="hash-link" href="#what-are-rdds-and-how-to-use-them" title="Direct link to heading">#</a></h4><p>RDDs are the core part of Spark. I learned and used RDDs first. It felt
familiar coming from MapReduce. Nowadays you use Dataframes or Datasets.</p><p>I still find it valuable to learn how RDDs and therefore Spark works at
a lower level.</p><p>| Podcast Episode: #101 Apache Spark Week Day 2
|------------------|
|On day two of the Apache Spark week we take a look at major Apache Spark concepts: RDDs, transformations and actions, caching and broadcast variables.
| <a href="https://youtu.be/9I6mA2W6_HU" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-and-why-to-use-sparksql"></a>How and why to use SparkSQL?<a class="hash-link" href="#how-and-why-to-use-sparksql" title="Direct link to heading">#</a></h4><p>When you use Apache Zeppelin notebooks to learn Spark you automatically
come across SparkSQL. SparkSQL allows you to access Dataframes with SQL
like queries.</p><p>Especially when you work with notebooks it is very handy to create
charts from your data. You can learn from mistakes easier than just
deploying Spark applications.</p><p>| Podcast Episode: #102 Apache Spark Week Day 3
|------------------|
| We continue the Spark week, hands on. We do a full example from reading a csv, doing maps and ﬂatmaps, to writing to disk. We also use SparkSQL to visualize the data.
| <a href="https://youtu.be/Fk-s8eKD4ZI" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-are-dataframes-how-to-use-them"></a>What are DataFrames how to use them<a class="hash-link" href="#what-are-dataframes-how-to-use-them" title="Direct link to heading">#</a></h4><p>As I said before. Dataframes are the successors to RDDs. It&#x27;s the new
Spark API.</p><p>Dataframes are basically lake Tables in a SQL Database or like an Excel
sheet. This makes them very simple to use and manipulate with SparkSQL.
I highly recommend to go this route.</p><p>Processing with Dataframes is even faster then with RDDs, because it
uses optimization alogrithms for the data processing.</p><p>| Podcast Episode: #103 Apache Spark Week Day 4
|------------------|
|We look into Dataframes, Dataframes and Dataframes.
| <a href="https://youtu.be/9I6mA2W6_HU" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="machine-learning-on-spark-tensor-flow"></a>Machine Learning on Spark? (Tensor Flow)<a class="hash-link" href="#machine-learning-on-spark-tensor-flow" title="Direct link to heading">#</a></h4><p>Wouldn&#x27;t it be great to use your deep learning TensorFlow applications
on Spark? Yes, it is already possible. Check out these Links:</p><p>Why do people integrate Spark with TensorFlow even if there is a
distributed TensorFlow framework?
<a href="https://www.quora.com/Why-do-people-integrate-Spark-with-TensorFlow-even-if-there-is-a-distributed-TensorFlow-framework" target="_blank" rel="noopener noreferrer">https://www.quora.com/Why-do-people-integrate-Spark-with-TensorFlow-even-if-there-is-a-distributed-TensorFlow-framework</a></p><p>TensorFlow On Spark: Scalable TensorFlow Learning on Spark Clusters:
<a href="https://databricks.com/session/tensorflow-on-spark-scalable-tensorflow-learning-on-spark-clusters" target="_blank" rel="noopener noreferrer">https://databricks.com/session/tensorflow-on-spark-scalable-tensorflow-learning-on-spark-clusters</a></p><p>Deep Learning with Apache Spark and TensorFlow:
<a href="https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html" target="_blank" rel="noopener noreferrer">https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="mllib"></a>MLlib:<a class="hash-link" href="#mllib" title="Direct link to heading">#</a></h4><p>The machine learning library MLlib is included in Spark so there is
often no need to import another library.</p><p>I have to admit because I am not a data scientist I am not an expert in
machine learning.</p><p>From what I have seen and read though the machine learning framework
MLlib is a nice treat for data scientists wanting to train and apply
models with Spark.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spark-setup"></a>Spark Setup<a class="hash-link" href="#spark-setup" title="Direct link to heading">#</a></h4><p>From a solution architect&#x27;s point of view Spark is a perfect fit for
Hadoop big data platforms. This has a lot to do with cluster deployment
and management.</p><p>Companies like Cloudera, MapR or Hortonworks include Spark into their
Hadoop distributions. Because of that, Spark can be deployed and managed
with the clusters Hadoop management web fronted.</p><p>This makes the process for deploying and configuring a Spark cluster
very quick and admin friendly.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spark-resource-management"></a>Spark Resource Management<a class="hash-link" href="#spark-resource-management" title="Direct link to heading">#</a></h4><p>When running a computing framework you need resources to do computation:
CPU time, RAM, I/O and so on. Out of the box Spark can manage resources
with it&#x27;s stand-alone resource manager.</p><p>If Spark is running in an Hadoop environment you don&#x27;t have to use
Spark&#x27;s own stand-alone resource manager. You can configure Spark to use
Hadoop&#x27;s YARN resource management.</p><p>Why would you do that? It allows YARN to efficiently allocate resources
to your Hadoop and Spark processes.</p><p>Having a single resource manager instead of two independent ones makes
it a lot easier to configure the resource management.</p><p><img alt="Spark Resource Management With YARN" src="/assets/images/Spark-Yarn-91004b6299c87a9c6a3aaea70745ef89.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="samza"></a>Samza<a class="hash-link" href="#samza" title="Direct link to heading">#</a></h3><p><a href="http://samza.apache.org/" target="_blank" rel="noopener noreferrer">Link to Apache Samza Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aws-lambda"></a>AWS Lambda<a class="hash-link" href="#aws-lambda" title="Direct link to heading">#</a></h3><p><a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Link to AWS Lambda Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-flink"></a>Apache Flink<a class="hash-link" href="#apache-flink" title="Direct link to heading">#</a></h3><p><a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Link to Apache Flink Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="elasticsearch"></a>Elasticsearch<a class="hash-link" href="#elasticsearch" title="Direct link to heading">#</a></h3><p><a href="https://www.elastic.co/products/elastic-stack" target="_blank" rel="noopener noreferrer">Link to Elatsicsearch Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="graph-db"></a>Graph DB<a class="hash-link" href="#graph-db" title="Direct link to heading">#</a></h3><p>Graph databases store data in terms of nodes and relationships.
Each node represents an entity (people, movies, things and other
data points) and a relationship represents how the nodes are related.
They are designed to store and treat the relationships with the same
importance of that of the data (or nodes in this case). This
relationship-first approach makes a lot of difference as the relationship
between data need not be inferred anymore with foreign and primary keys.</p><p>Graph databases are especially useful when applications require
navigating through multiple and multi-level relationships between
various data points.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="neo4j"></a>Neo4j<a class="hash-link" href="#neo4j" title="Direct link to heading">#</a></h4><p>Neo4j is currently the most popular graph database management system.
It is ACID compliant and provides its own implementation of a graph database.
In addition to nodes and relationships, neo4j has the following components
to enrich the data model with information.</p><p>• Labels. These are used to group nodes, and each node can be assigned
multiple labels. Labels are indexed to speed up finding nodes in a graph.
• Properties. These are attributes of both nodes and relationships.
Neo4j allows for storing data as key-value pairs, which means properties
can have any value (string, number, or boolean).</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="advantages"></a>Advantages<a class="hash-link" href="#advantages" title="Direct link to heading">#</a></h5><p>• Neo4j is schema-free
• Highly available and provides transactional guarantees
• Cypher is a declarative query language which makes it very easy to navigate the graph
• Neo4j is fast and easily traversible because the data is connected and is very easy to query, retrieve and navigate the graph
• For the same reason as above, there are no joins in Neo4j</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="disadvantages"></a>Disadvantages<a class="hash-link" href="#disadvantages" title="Direct link to heading">#</a></h5><p>• Neo4j is not the best for any kind of aggregations or sorting, in comparison with a relational database
• While doable, they are not the best to handle transactional data like accounting
• Sharding is currently not supported</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="use-cases"></a>Use Cases<a class="hash-link" href="#use-cases" title="Direct link to heading">#</a></h5><p><a href="https://neo4j.com/use-cases/" target="_blank" rel="noopener noreferrer">https://neo4j.com/use-cases/</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-solr"></a>Apache Solr<a class="hash-link" href="#apache-solr" title="Direct link to heading">#</a></h3><p><a href="https://solr.apache.org" target="_blank" rel="noopener noreferrer">Link to Solr Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-drill"></a>Apache Drill<a class="hash-link" href="#apache-drill" title="Direct link to heading">#</a></h3><p><a href="https://drill.apache.org" target="_blank" rel="noopener noreferrer">Link to Apache Drill Homepage</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-storm"></a>Apache Storm<a class="hash-link" href="#apache-storm" title="Direct link to heading">#</a></h3><p><a href="https://storm.apache.org/" target="_blank" rel="noopener noreferrer">https://storm.apache.org/</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="streamsets"></a>StreamSets<a class="hash-link" href="#streamsets" title="Direct link to heading">#</a></h3><p><a href="https://youtu.be/djt8532UWow" target="_blank" rel="noopener noreferrer">https://youtu.be/djt8532UWow</a></p><p><a href="https://www.youtube.com/watch?v=Qm5e574WoCU&amp;t=2s" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=Qm5e574WoCU&amp;t=2s</a></p><p><a href="https://streamsets.com/blog/streaming-data-twitter-analysis-spark/" target="_blank" rel="noopener noreferrer">https://streamsets.com/blog/streaming-data-twitter-analysis-spark/</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="store"></a>Store<a class="hash-link" href="#store" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="analytical-data-stores"></a>Analytical Data Stores<a class="hash-link" href="#analytical-data-stores" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-warehouse-vs-data-lake"></a>Data Warehouse vs Data Lake<a class="hash-link" href="#data-warehouse-vs-data-lake" title="Direct link to heading">#</a></h4><p>| Podcast Episode: #055 Data Warehouse vs Data Lake
|------------------|
|On this podcast we are going to talk about data warehouses and data lakes? When do people use which? What are the pros and cons of both? Architecture examples for both Does it make sense to completely move to a data lake?
| <a href="https://youtu.be/8gNQTrUUwMk" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/055-Data-Warehouse-vs-Data-Lake-e45iem" target="_blank" rel="noopener noreferrer">Listen on Anchor</a>|</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="snowflake-and-dbt"></a>Snowflake and dbt<a class="hash-link" href="#snowflake-and-dbt" title="Direct link to heading">#</a></h4><p><img alt="Snowlfake thumb" src="/assets/images/Snowflake-dbt-thumbnail-e99e09727b6ca6cf3ae65884a309708e.jpeg"></p><p>In the rapidly evolving landscape of data engineering, staying ahead means continuously expanding your skill set with the latest tools and technologies. Among the myriad of options available, dbt (data build tool) and Snowflake have emerged as indispensable for modern data engineering workflows. Understanding and leveraging these tools can significantly enhance your ability to manage and transform data, making you a more effective and valuable data engineer. Let&#x27;s dive into why dbt and Snowflake should be at the top of your learning list and explore how the &quot;dbt for Data Engineers&quot; and &quot;Snowflake for Data Engineers&quot; courses from the Learn Data Engineering Academy can help you achieve mastery in these tools.</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="the-power-of-snowflake-in-data-engineering"></a>The Power of Snowflake in Data Engineering<a class="hash-link" href="#the-power-of-snowflake-in-data-engineering" title="Direct link to heading">#</a></h5><p>Snowflake has revolutionized the data warehousing space with its cloud-native architecture. It offers a scalable, flexible, and highly performant platform that simplifies data management and analytics. Here’s why Snowflake is a critical skill for data engineers:</p><ol><li><strong>Cloud-Native Flexibility:</strong> Snowflake’s architecture allows you to scale resources up or down based on your needs, ensuring optimal performance and cost-efficiency.</li><li><strong>Unified Data Platform:</strong> It unifies data silos, enabling seamless data sharing and collaboration across the organization.</li><li><strong>Integration Capabilities:</strong> Snowflake integrates with various data tools and platforms, enhancing its versatility in different data workflows.</li><li><strong>Advanced Analytics:</strong> With its robust support for data querying, transformation, and integration, Snowflake is ideal for complex analytical workloads.</li></ol><p>The &quot;Snowflake for Data Engineers&quot; course in my Learn Data Engineering Academy provides comprehensive training on Snowflake. From the basics of setting up your Snowflake environment to advanced data automation with Snowpipes, the course equips you with practical skills to leverage Snowflake effectively in your data projects.</p><p>Learn more about the course <a href="https://learndataengineering.com/p/snowflake-for-data-engineers" target="_blank" rel="noopener noreferrer">here</a>.</p><p><img alt="Snowlfake thumb" src="/assets/images/Snowflake-ui-fb1cf5e3f7581bdb8a6c89c7723da800.jpeg"></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-dbt-is-a-game-changer-for-data-engineers"></a>Why dbt is a Game-Changer for Data Engineers<a class="hash-link" href="#why-dbt-is-a-game-changer-for-data-engineers" title="Direct link to heading">#</a></h5><p>dbt is a powerful transformation tool that allows data engineers to transform, test, and document data directly within their data warehouse using simple SQL. Unlike traditional ETL tools, dbt operates on the principle of ELT (Extract, Load, Transform), which aligns perfectly with modern cloud data warehousing paradigms. Here are a few reasons why dbt is a must-have skill for data engineers:</p><ol><li><strong>SQL-First Approach:</strong> dbt allows you to write transformations in SQL, the lingua franca of data manipulation, making it accessible to a broad range of data professionals.</li><li><strong>Collaboration:</strong> Teams can collaborate seamlessly, creating trusted datasets for reporting, machine learning, and operational workflows.</li><li><strong>Ease of Use:</strong> With dbt, you can transform, test, and document your data with ease, streamlining the data pipeline process.</li><li><strong>Integration:</strong> dbt integrates effortlessly with your existing data warehouse, such as Snowflake, making it a versatile addition to your toolkit.</li></ol><p>In my Learn Data Engineering Academy you find the perfect starting point for mastering dbt with the course &quot;dbt for Data Engineers&quot;. The course covers everything from the basics of ELT processes to advanced features like continuous integration and deployment (CI/CD) pipelines. With hands-on training, you&#x27;ll learn to create data pipelines, configure dbt materializations, test dbt models, and much more.</p><p>Learn more about the course <a href="https://learndataengineering.com/p/dbt-for-data-engineers" target="_blank" rel="noopener noreferrer">here</a>.</p><p><img alt="Snowlfake thumb" src="/assets/images/dbt-ui-eddc02aa7967ba3a1f94ad992ab57d48.jpeg"></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="dbt-and-snowflake-a-winning-combination"></a>dbt and Snowflake: A Winning Combination<a class="hash-link" href="#dbt-and-snowflake-a-winning-combination" title="Direct link to heading">#</a></h5><p>When used together, dbt and Snowflake offer a powerful combination for data engineering. Here’s why:</p><ol><li><strong>Seamless Integration:</strong> dbt’s SQL-first transformation capabilities integrate perfectly with Snowflake’s scalable data warehousing, creating a streamlined ELT workflow.</li><li><strong>Efficiency:</strong> Together, they enhance the efficiency of data transformation and analytics, reducing the time and effort required to prepare data for analysis.</li><li><strong>Scalability:</strong> The combined power of dbt’s model management and Snowflake’s dynamic scaling ensures that your data pipelines can handle large and complex datasets with ease.</li><li><strong>Collaboration and Documentation:</strong> dbt’s ability to document and test data transformations directly within Snowflake ensures that data workflows are transparent, reliable, and collaborative.
Get right into it with our Academy!</li></ol><p>By integrating Snowflake and dbt into your skill set, you position yourself at the forefront of data engineering innovation. These tools not only simplify and enhance your data workflows but also open up new possibilities for data transformation and analysis.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="transactional-data-stores"></a>Transactional Data Stores<a class="hash-link" href="#transactional-data-stores" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sql-databases"></a>SQL Databases<a class="hash-link" href="#sql-databases" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="postgresql-db"></a>PostgreSQL DB<a class="hash-link" href="#postgresql-db" title="Direct link to heading">#</a></h5><p>Homepage:</p><p><a href="https://www.postgresql.org/" target="_blank" rel="noopener noreferrer">https://www.postgresql.org/</a></p><p>PostgreSQL vs MongoDB:</p><p><a href="https://blog.panoply.io/postgresql-vs-mongodb" target="_blank" rel="noopener noreferrer">https://blog.panoply.io/postgresql-vs-mongodb</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="database-design"></a>Database Design<a class="hash-link" href="#database-design" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sql-queries"></a>SQL Queries<a class="hash-link" href="#sql-queries" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="stored-procedures"></a>Stored Procedures<a class="hash-link" href="#stored-procedures" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="odbcjdbc-server-connections"></a>ODBC/JDBC Server Connections<a class="hash-link" href="#odbcjdbc-server-connections" title="Direct link to heading">#</a></h5><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="nosql-stores"></a>NoSQL Stores<a class="hash-link" href="#nosql-stores" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="keyvalue-stores-hbase"></a>KeyValue Stores (HBase)<a class="hash-link" href="#keyvalue-stores-hbase" title="Direct link to heading">#</a></h5><table><thead><tr><th>Podcast Episode: #056 NoSQL Key Value Stores Explained with HBase</th></tr></thead><tbody><tr><td>What is the diﬀerence between SQL and NoSQL? In this episode I show you on the example of HBase how a key/value store works.</td></tr><tr><td><a href="https://youtu.be/67hIkbpzFc8" target="_blank" rel="noopener noreferrer">Watch on YouTube</a> \ <a href="https://anchor.fm/andreaskayy/episodes/056-NoSQL-Key-Value-Stores-Explained-With-HBase-e45ifb" target="_blank" rel="noopener noreferrer">Listen on Anchor</a></td></tr></tbody></table><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="document-store-hdfs"></a>Document Store HDFS<a class="hash-link" href="#document-store-hdfs" title="Direct link to heading">#</a></h5><p>The Hadoop distributed file system, or HDFS, allows you to store files
in Hadoop. The difference between HDFS and other file systems like NTFS
or EXT is that it is a distributed one.</p><p>What does that mean exactly?</p><p>A typical file system stores your data on the actual hard drive. It is
hardware dependent.</p><p>If you have two disks then you need to format every disk with its own
file system. They are completely separate.</p><p>You then decide on which disk you physically store your data.</p><p>HDFS works different to a typical file system. HDFS is hardware
independent.</p><p>Not only does it span over many disks in a server. It also spans over
many servers.</p><p>HDFS will automatically place your files somewhere in the Hadoop server
collective.</p><p>It will not only store your file, Hadoop will also replicate it two or
three times (you can define that). Replication means replicas of the
file will be distributed to different servers.</p><p><img alt="HDFS Master and Data Nodes" src="/assets/images/HDFS-Master-DataNodes-a633d7c9c6508866a7808ce66346d582.jpg"></p><p>This gives you superior fault tolerance. If one server goes down, then
your data stays available on a different server.</p><p>Another great thing about HDFS is, that there is no limit how big the
files can be. You can have server log files that are terabytes big.</p><p>How can files get so big? HDFS allows you to append data to files.
Therefore, you can continuously dump data into a single file without
worries.</p><p>HDFS physically stores files different then a normal file system. It
splits the file into blocks.</p><p>These blocks are then distributed and replicated on the Hadoop cluster.
The splitting happens automatically.</p><p><img alt="Distribution of Blocks for a 512MB File" src="/assets/images/HDFS-Distributed-FileSystem-3ee28ba825acb0ca5e7e2e5242d29a27.jpg"></p><p>In the configuration you can define how big the blocks should be. 128
megabyte or 1 gigabyte?</p><p>No problem at all.</p><p>This mechanic of splitting a large file in blocks and distributing them
over the servers is great for processing. See the MapReduce section for
an example.</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="document-store-mongodb"></a>Document Store MongoDB<a class="hash-link" href="#document-store-mongodb" title="Direct link to heading">#</a></h5><table><thead><tr><th>Podcast Episode: #093 What is MongoDB</th></tr></thead><tbody><tr><td>What is the diﬀerence between SQL and NoSQL? In this episode I show you on the example of HBase how a key/value store works.</td></tr><tr><td><a href="https://youtu.be/U05knQN29FA" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></td></tr></tbody></table><p><strong>Links:</strong></p><p>What is MongoDB:</p><p><a href="https://www.guru99.com/what-is-mongodb.html#4" target="_blank" rel="noopener noreferrer">https://www.guru99.com/what-is-mongodb.html#4</a></p><p>Or directly from MongoDB.com:</p><p><a href="https://www.mongodb.com/what-is-mongodb" target="_blank" rel="noopener noreferrer">https://www.mongodb.com/what-is-mongodb</a></p><p>Storage in BSON files:</p><p><a href="https://en.wikipedia.org/wiki/BSON" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/BSON</a></p><p>Hello World in MongoDB:</p><p><a href="https://www.mkyong.com/mongodb/mongodb-hello-world-example" target="_blank" rel="noopener noreferrer">https://www.mkyong.com/mongodb/mongodb-hello-world-example</a></p><p>Real-Time Analytics on MongoDB Data in Power BI:</p><p><a href="https://dzone.com/articles/real-time-analytics-on-mongodb-data-in-power-bi" target="_blank" rel="noopener noreferrer">https://dzone.com/articles/real-time-analytics-on-mongodb-data-in-power-bi</a></p><p>Spark and MongoDB:</p><p><a href="https://www.mongodb.com/scale/when-to-use-apache-spark-with-mongodb" target="_blank" rel="noopener noreferrer">https://www.mongodb.com/scale/when-to-use-apache-spark-with-mongodb</a></p><p>MongoDB vs Time Series Database:</p><p><a href="https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016/" target="_blank" rel="noopener noreferrer">https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016/</a></p><p>Fun article titled why you should never use mongodb:</p><p><a href="http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/" target="_blank" rel="noopener noreferrer">http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/</a></p><p>MongoDB vs Cassandra:</p><p><a href="https://blog.panoply.io/cassandra-vs-mongodb" target="_blank" rel="noopener noreferrer">https://blog.panoply.io/cassandra-vs-mongodb</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="elasticsearch-search-engine-and-document-store"></a>Elasticsearch Search Engine and Document Store<a class="hash-link" href="#elasticsearch-search-engine-and-document-store" title="Direct link to heading">#</a></h5><p>Elasticsearch is not a DB but firstly a search engine that indexes JSON
documents.</p><p>| Podcast Episode: #095 What is Elasticsearch &amp; Why is It So Popular?
|------------------|
|Elasticsearch is a super popular tool for indexing and searching data. On this stream we check out how it works, architectures and what to use it for. There must be a reason why it is so popular.<br>
| <a href="https://youtu.be/hNb5zB4OPXM" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><p>Links:</p><p>Great example for architecture with Elasticsearch, Logstash and Kibana:\
<a href="https://www.elastic.co/pdf/architecture-best-practices.pdf" target="_blank" rel="noopener noreferrer">https://www.elastic.co/pdf/architecture-best-practices.pdf</a></p><p>Introduction to Elasticsearch in the documentation:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html</a></p><p>Working with JSON documents:\
<a href="https://www.slideshare.net/openthinklabs/03-elasticsearch-data-in-data-out" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/openthinklabs/03-elasticsearch-data-in-data-out</a></p><p>JSONs need to be flattened heres how to work with nested objects in the
JSON:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html</a></p><p>Indexing basics:\
<a href="https://www.slideshare.net/knoldus/deep-dive-into-elasticsearch" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/knoldus/deep-dive-into-elasticsearch</a></p><p>How to do searches with search API:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html</a></p><p>General recommendations when working with Elasticsearch:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/general-recommendations.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/general-recommendations.html</a></p><p>JSON document example and intro to Kibana:\
<a href="https://www.slideshare.net/objectrocket/an-intro-to-elasticsearch-and-kibana" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/objectrocket/an-intro-to-elasticsearch-and-kibana</a></p><p>How to connect Tableau to Elasticsearch:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-client-apps-tableau.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-client-apps-tableau.html</a></p><p>Benchmarks how fast Elasticsearch is:\
<a href="https://medium.appbase.io/benchmarking-elasticsearch-1-million-writes-per-sec-bf37e7ca8a4c" target="_blank" rel="noopener noreferrer">https://medium.appbase.io/benchmarking-elasticsearch-1-million-writes-per-sec-bf37e7ca8a4c</a></p><p>Elasticsearch vs MongoDB quick overview:\
<a href="https://db-engines.com/en/system/Elasticsearch%3BMongoDB" target="_blank" rel="noopener noreferrer">https://db-engines.com/en/system/Elasticsearch%3BMongoDB</a></p><p>Logstash overview (preprocesses data before insert into Elasticsearch)
<a href="https://www.elastic.co/products/logstash" target="_blank" rel="noopener noreferrer">https://www.elastic.co/products/logstash</a></p><p>X-Pack Security for Elasticsearch:\
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html" target="_blank" rel="noopener noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html</a></p><p>Google Trends Grafana vs Kibana:\
<a href="https://trends.google.com/trends/explore?geo=US&amp;q=%2Fg%2F11fy132gmf,%2Fg%2F11cknd0blr" target="_blank" rel="noopener noreferrer">https://trends.google.com/trends/explore?geo=US&amp;q=%2Fg%2F11fy132gmf,%2Fg%2F11cknd0blr</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-impala"></a>Apache Impala<a class="hash-link" href="#apache-impala" title="Direct link to heading">#</a></h5><p><a href="https://impala.apache.org/" target="_blank" rel="noopener noreferrer">Apache Impala Homepage</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="kudu"></a>Kudu<a class="hash-link" href="#kudu" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="apache-druid"></a>Apache Druid<a class="hash-link" href="#apache-druid" title="Direct link to heading">#</a></h5><p>| Podcast Episode: Druid NoSQL DB and Analytics DB Introduction
|------------------|
|In this video I explain what Druid is and how it works. We look into the architecture of a Druid cluster and check out how Clients access the data.
|<a href="https://youtu.be/EiEIeBXSWjM" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="influxdb-time-series-database"></a>InfluxDB Time Series Database<a class="hash-link" href="#influxdb-time-series-database" title="Direct link to heading">#</a></h5><p>What is time-series data?</p><p><a href="https://questdb.io/blog/what-is-time-series-data/" target="_blank" rel="noopener noreferrer">https://questdb.io/blog/what-is-time-series-data/</a></p><p>Key concepts:</p><p><a href="https://docs.influxdata.com/influxdb/v1.7/concepts/key_concepts/" target="_blank" rel="noopener noreferrer">https://docs.influxdata.com/influxdb/v1.7/concepts/key_concepts/</a></p><p>InfluxDB and Spark Streaming</p><p><a href="https://towardsdatascience.com/processing-time-series-data-in-real-time-with-influxdb-and-structured-streaming-d1864154cf8b" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/processing-time-series-data-in-real-time-with-influxdb-and-structured-streaming-d1864154cf8b</a></p><p>Building a Streaming application with spark, grafana, chronogram and
influx:</p><p><a href="https://medium.com/@xaviergeerinck/building-a-real-time-streaming-dashboard-with-spark-grafana-chronograf-and-influxdb-e262b68087de" target="_blank" rel="noopener noreferrer">https://medium.com/@xaviergeerinck/building-a-real-time-streaming-dashboard-with-spark-grafana-chronograf-and-influxdb-e262b68087de</a></p><p>Performance Dashboard Spark and InfluxDB:</p><p><a href="https://db-blog.web.cern.ch/blog/luca-canali/2019-02-performance-dashboard-apache-spark" target="_blank" rel="noopener noreferrer">https://db-blog.web.cern.ch/blog/luca-canali/2019-02-performance-dashboard-apache-spark</a></p><p>Other alternatives for time series databases are: DalmatinerDB,
QuestDB, Prometheus, Riak TS, OpenTSDB, KairosDB</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="mpp-databases-greenplum"></a>MPP Databases (Greenplum)<a class="hash-link" href="#mpp-databases-greenplum" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="azure-cosmos-db"></a>Azure Cosmos DB<a class="hash-link" href="#azure-cosmos-db" title="Direct link to heading">#</a></h5><p><a href="https://azure.microsoft.com/en-us/services/cosmos-db/" target="_blank" rel="noopener noreferrer">https://azure.microsoft.com/en-us/services/cosmos-db/</a></p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="azure-table-storage"></a>Azure Table-Storage<a class="hash-link" href="#azure-table-storage" title="Direct link to heading">#</a></h5><p><a href="https://azure.microsoft.com/en-us/services/storage/tables/" target="_blank" rel="noopener noreferrer">https://azure.microsoft.com/en-us/services/storage/tables/</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="nosql-data-warehouse"></a>NoSQL Data warehouse<a class="hash-link" href="#nosql-data-warehouse" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="hive-warehouse"></a>Hive Warehouse<a class="hash-link" href="#hive-warehouse" title="Direct link to heading">#</a></h5><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="impala"></a>Impala<a class="hash-link" href="#impala" title="Direct link to heading">#</a></h5><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="visualize"></a>Visualize<a class="hash-link" href="#visualize" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="android--ios"></a>Android &amp; IOS<a class="hash-link" href="#android--ios" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-design-apis-for-mobile-apps"></a>How to design APIs for mobile apps<a class="hash-link" href="#how-to-design-apis-for-mobile-apps" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-use-webservers-to-display-content"></a>How to use Webservers to display content<a class="hash-link" href="#how-to-use-webservers-to-display-content" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="dashboards"></a>Dashboards<a class="hash-link" href="#dashboards" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="grafana"></a>Grafana<a class="hash-link" href="#grafana" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="kibana"></a>Kibana<a class="hash-link" href="#kibana" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tomcat"></a>Tomcat<a class="hash-link" href="#tomcat" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="jetty"></a>Jetty<a class="hash-link" href="#jetty" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="nodered"></a>NodeRED<a class="hash-link" href="#nodered" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="react"></a>React<a class="hash-link" href="#react" title="Direct link to heading">#</a></h4><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="business-intelligence-tools"></a>Business Intelligence Tools<a class="hash-link" href="#business-intelligence-tools" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tableau"></a>Tableau<a class="hash-link" href="#tableau" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="powerbi"></a>PowerBI<a class="hash-link" href="#powerbi" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="quliksense"></a>Quliksense<a class="hash-link" href="#quliksense" title="Direct link to heading">#</a></h4><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="identity--device-management"></a>Identity &amp; Device Management<a class="hash-link" href="#identity--device-management" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-a-digital-twin"></a>What is a digital twin?<a class="hash-link" href="#what-is-a-digital-twin" title="Direct link to heading">#</a></h4><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="active-directory"></a>Active Directory<a class="hash-link" href="#active-directory" title="Direct link to heading">#</a></h4><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="machine-learning"></a>Machine Learning<a class="hash-link" href="#machine-learning" title="Direct link to heading">#</a></h2><p>| Podcast Episode: Machine Learning In Production
|------------------|
|Doing machine learning in production is very diﬀerent than for proof of concepts or in education. One of the hardest parts is keeping models updated.<br>
| <a href="https://anchor.fm/andreaskayy/episodes/Machine-Learning-In-Production-e11bbk" target="_blank" rel="noopener noreferrer">Listen on Anchor</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-do-machine-learning-in-production"></a>How to do Machine Learning in production<a class="hash-link" href="#how-to-do-machine-learning-in-production" title="Direct link to heading">#</a></h3><p>Machine learning in production is using stream and batch processing. In
the batch processing layer you are creating the models, because you have
all the data available for training.</p><p>In the stream in processing layer you are using the created models, you
are applying them to new data.</p><p>The idea that you need to incorporate is that it is a constant cycle.
Training, applying, re-training, pushing into production and applying.</p><p>What you don&#x27;t want to do is doing this manually. You need to figure out
a process of automatic retraining and automatic pushing to into
production of models.</p><p>In the retraining phase the system automatically evaluates the training.
If the model no longer fits it works as long as it needs to create a
good model.</p><p>After the evaluation of the model is complete and it&#x27;s good, the model
gets pushed into production. Into the stream processing.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-machine-learning-in-production-is-harder-then-you-think"></a>Why machine learning in production is harder then you think<a class="hash-link" href="#why-machine-learning-in-production-is-harder-then-you-think" title="Direct link to heading">#</a></h3><p>How to automate machine learning is something that drives me day in and
day out.</p><p>What you do in development or education is, that you create a model and
fit it to the data. Then that model is basically done forever.</p><p>Where I&#x27;m coming from, the IoT world, the problem is that machines are
very different. They behave very different and experience wear.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="models-do-not-work-forever"></a>Models Do Not Work Forever<a class="hash-link" href="#models-do-not-work-forever" title="Direct link to heading">#</a></h3><p>Machines have certain processes that decrease the actual health of the
machine. Machine wear is a huge issue. Models that that are built on top
of a good machine don&#x27;t work forever.</p><p>When the Machine wears out, the models need to be adjusted. They need to
be maintained, retrained.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="where-the-platforms-that-support-this"></a>Where The Platforms That Support This?<a class="hash-link" href="#where-the-platforms-that-support-this" title="Direct link to heading">#</a></h3><p>Automatic re-training and re-deploying is a very big issue, a very big
problem for a lot of companies. Because most existing platforms don&#x27;t
have this capability (I actually haven&#x27;t seen one until now).</p><p>Look at AWS machine learning for instance. The process is: build, train,
tune deploy. Where&#x27;s the loop of retraining?</p><p>You can create models and then use them in production. But this loop is
almost nowhere to be seen.</p><p>It is a very big issue that needs to be solved. If you want to do
machine learning in production you can start with manual interaction of
the training, but at some point you need to automate everything.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="training-parameter-management"></a>Training Parameter Management<a class="hash-link" href="#training-parameter-management" title="Direct link to heading">#</a></h3><p>To train a model you are manipulating input parameters of the models.</p><p>Take deep learning for instance.</p><p>To train you are manipulating for instance:</p><p>- How many layers do you use. - The depth of the layers, which means
how many neurons you have in a layer. - What activation function you
use, how long are you training and so on.</p><p>You also need to keep track of what data you used to train which model.</p><p>All those parameters need to be manipulated automatically, models
trained and tested.</p><p>To do all that, you basically need a database that keeps track of those
variables.</p><p>How to automate this, for me, is like the big secret. I am still working
on figuring it out.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="whats-your-solution"></a>What&#x27;s Your Solution?<a class="hash-link" href="#whats-your-solution" title="Direct link to heading">#</a></h3><p>Did you already have the problem of automatic re-training and deploying
of models as well?</p><p>Were you able to use a cloud platform like Google, AWS or Azure?</p><p>It would be really awesome if you share your experience :)</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-convince-people-machine-learning-works"></a>How to convince people machine learning works<a class="hash-link" href="#how-to-convince-people-machine-learning-works" title="Direct link to heading">#</a></h3><p>Many people still are not convinced that machine learning works
reliably. But they want analytics insight and most of the time machine
learning is the way to go.</p><p>This means, when you are working with customers you need to do a lot of
convincing. Especially if they are not into machine learning themselves.</p><p>But it&#x27;s actually quite easy.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="no-rules-no-physical-models"></a>No Rules, No Physical Models<a class="hash-link" href="#no-rules-no-physical-models" title="Direct link to heading">#</a></h3><p>Many people are still under the impression that analytics only works
when it&#x27;s based on physics. When there are strict mathematical rules to
a problem.</p><p>Especially in engineering heavy countries like Germany this is the norm:</p><p>&quot;Sere has to be a Rule for Everysing!&quot; (imagine a German accent). When
you&#x27;re engineering you are calculating stuff based on physics and not
based on data. If you are constructing an airplane wing, you better make
sure to use calculations so it doesn&#x27;t fall off.</p><p>And that&#x27;s totally fine.</p><p>Keep doing that!</p><p>Machine learning has been around for decades. It didn&#x27;t quite work as
good as people hoped. We have to admit that. But there is this
preconception that it still doesn&#x27;t work.</p><p>Which is not true: Machine learning works.</p><p>Somehow you need to convince people that it is a viable approach. That
learning from data to make predictions is working perfectly.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="you-have-the-data-use-it"></a>You Have The Data. USE IT!<a class="hash-link" href="#you-have-the-data-use-it" title="Direct link to heading">#</a></h3><p>As a data scientist you have one ace up your sleeve, it&#x27;s the obvious
one:</p><p>It&#x27;s the data and it&#x27;s statistics.</p><p>You can use that data and those statistics to counter peoples
preconceptions. It&#x27;s very powerful if someone says: &quot;This doesn&#x27;t work&quot;</p><p>You bring the data. You show the statistics and you show that it works
reliably.</p><p>A lot of discussions end there.</p><p>Data doesn&#x27;t lie. You can&#x27;t fight data. The data is always right.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-is-stronger-than-opinions"></a>Data is Stronger Than Opinions<a class="hash-link" href="#data-is-stronger-than-opinions" title="Direct link to heading">#</a></h3><p>This is also why I believe that autonomous driving will come quicker
than many of us think. Because a lot of people say, they are not safe.
That you cannot rely on those cars.</p><p>The thing is: When you have the data you can do the statistics.</p><p>You can show people that autonomous driving really works reliably. You
will see, the question of \&quot;Is this allowed or is this not allowed?\&quot;
will be gone quicker than you think.</p><p>Because government agencies can start testing the algorithms based on
predefined scenarios. They can run benchmarks and score the cars
performance.</p><p>All those opinions, if it works, or if it doesn&#x27;t work, they will be
gone.</p><p>The motor agency has the statistics. The stats show people how good cars
work.</p><p>Companies like Tesla, they have it very easy. Because the data is
already there.</p><p><strong>They just need to show us that the algorithms work. The end.</strong></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="aws-sagemaker"></a>AWS Sagemaker<a class="hash-link" href="#aws-sagemaker" title="Direct link to heading">#</a></h3><p>Train and apply models online with Sagemaker</p><p>Link to the OLX Slideshare with pros, cons and how to use Sagemaker:
<a href="https://www.slideshare.net/mobile/AlexeyGrigorev/image-models-infrastructure-at-olx" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/mobile/AlexeyGrigorev/image-models-infrastructure-at-olx</a></p></div></article><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/02-BasicSkills"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« 02-BasicSkills</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/04-HandsOnCourse"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">04-HandsOnCourse »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#contents" class="table-of-contents__link">Contents</a></li><li><a href="#data-science-platform" class="table-of-contents__link">Data Science Platform</a><ul><li><a href="#why-a-good-data-platform-is-important" class="table-of-contents__link">Why a Good Data Platform Is Important</a></li><li><a href="#big-data-vs-data-science-and-analytics" class="table-of-contents__link">Big Data vs Data Science and Analytics</a></li><li><a href="#the-4-vs-of-big-data" class="table-of-contents__link">The 4 Vs of Big Data</a></li><li><a href="#why-big-data" class="table-of-contents__link">Why Big Data?</a></li><li><a href="#planning-is-everything" class="table-of-contents__link">Planning is Everything</a></li><li><a href="#the-problem-with-etl" class="table-of-contents__link">The problem with ETL</a></li><li><a href="#scaling-up" class="table-of-contents__link">Scaling Up</a></li><li><a href="#scaling-out" class="table-of-contents__link">Scaling Out</a></li><li><a href="#please-dont-go-big-data" class="table-of-contents__link">Please don&#39;t go Big Data</a></li></ul></li><li><a href="#81-platform-and-pipeline-design-questions" class="table-of-contents__link">81 Platform and Pipeline Design Questions</a><ul><li><a href="#data-source-questions" class="table-of-contents__link">Data Source Questions</a></li><li><a href="#goals-and-destination-questions" class="table-of-contents__link">Goals and Destination Questions</a></li></ul></li><li><a href="#connect" class="table-of-contents__link">Connect</a><ul><li><a href="#rest-apis" class="table-of-contents__link">REST APIs</a></li><li><a href="#apache-nifi" class="table-of-contents__link">Apache Nifi</a></li><li><a href="#logstash" class="table-of-contents__link">Logstash</a></li><li><a href="#fluentd" class="table-of-contents__link">FluentD</a></li><li><a href="#apache-flume" class="table-of-contents__link">Apache Flume</a></li><li><a href="#sqoop" class="table-of-contents__link">Sqoop</a></li><li><a href="#azure-iothub" class="table-of-contents__link">Azure IoTHub</a></li></ul></li><li><a href="#buffer" class="table-of-contents__link">Buffer</a><ul><li><a href="#apache-kafka" class="table-of-contents__link">Apache Kafka</a></li><li><a href="#redis-pub-sub" class="table-of-contents__link">Redis Pub-Sub</a></li><li><a href="#aws-kinesis" class="table-of-contents__link">AWS Kinesis</a></li><li><a href="#google-cloud-pubsub" class="table-of-contents__link">Google Cloud PubSub</a></li></ul></li><li><a href="#processing-frameworks" class="table-of-contents__link">Processing Frameworks</a><ul><li><a href="#lambda-and-kappa-architecture" class="table-of-contents__link">Lambda and Kappa Architecture</a></li><li><a href="#batch-processing" class="table-of-contents__link">Batch Processing</a></li><li><a href="#stream-processing" class="table-of-contents__link">Stream Processing</a></li><li><a href="#should-you-do-stream-or-batch-processing" class="table-of-contents__link">Should you do stream or batch processing?</a></li><li><a href="#is-etl-still-relevant-for-analytics" class="table-of-contents__link">Is ETL still relevant for Analytics?</a></li><li><a href="#mapreduce" class="table-of-contents__link">MapReduce</a></li><li><a href="#apache-spark" class="table-of-contents__link">Apache Spark</a></li><li><a href="#samza" class="table-of-contents__link">Samza</a></li><li><a href="#aws-lambda" class="table-of-contents__link">AWS Lambda</a></li><li><a href="#apache-flink" class="table-of-contents__link">Apache Flink</a></li><li><a href="#elasticsearch" class="table-of-contents__link">Elasticsearch</a></li><li><a href="#graph-db" class="table-of-contents__link">Graph DB</a></li><li><a href="#apache-solr" class="table-of-contents__link">Apache Solr</a></li><li><a href="#apache-drill" class="table-of-contents__link">Apache Drill</a></li><li><a href="#apache-storm" class="table-of-contents__link">Apache Storm</a></li><li><a href="#streamsets" class="table-of-contents__link">StreamSets</a></li></ul></li><li><a href="#store" class="table-of-contents__link">Store</a><ul><li><a href="#analytical-data-stores" class="table-of-contents__link">Analytical Data Stores</a></li><li><a href="#transactional-data-stores" class="table-of-contents__link">Transactional Data Stores</a></li></ul></li><li><a href="#visualize" class="table-of-contents__link">Visualize</a><ul><li><a href="#android--ios" class="table-of-contents__link">Android &amp; IOS</a></li><li><a href="#how-to-design-apis-for-mobile-apps" class="table-of-contents__link">How to design APIs for mobile apps</a></li><li><a href="#how-to-use-webservers-to-display-content" class="table-of-contents__link">How to use Webservers to display content</a></li><li><a href="#dashboards" class="table-of-contents__link">Dashboards</a></li><li><a href="#business-intelligence-tools" class="table-of-contents__link">Business Intelligence Tools</a></li><li><a href="#identity--device-management" class="table-of-contents__link">Identity &amp; Device Management</a></li></ul></li><li><a href="#machine-learning" class="table-of-contents__link">Machine Learning</a><ul><li><a href="#how-to-do-machine-learning-in-production" class="table-of-contents__link">How to do Machine Learning in production</a></li><li><a href="#why-machine-learning-in-production-is-harder-then-you-think" class="table-of-contents__link">Why machine learning in production is harder then you think</a></li><li><a href="#models-do-not-work-forever" class="table-of-contents__link">Models Do Not Work Forever</a></li><li><a href="#where-the-platforms-that-support-this" class="table-of-contents__link">Where The Platforms That Support This?</a></li><li><a href="#training-parameter-management" class="table-of-contents__link">Training Parameter Management</a></li><li><a href="#whats-your-solution" class="table-of-contents__link">What&#39;s Your Solution?</a></li><li><a href="#how-to-convince-people-machine-learning-works" class="table-of-contents__link">How to convince people machine learning works</a></li><li><a href="#no-rules-no-physical-models" class="table-of-contents__link">No Rules, No Physical Models</a></li><li><a href="#you-have-the-data-use-it" class="table-of-contents__link">You Have The Data. USE IT!</a></li><li><a href="#data-is-stronger-than-opinions" class="table-of-contents__link">Data is Stronger Than Opinions</a></li><li><a href="#aws-sagemaker" class="table-of-contents__link">AWS Sagemaker</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Andreas Kretz. Built by Kristijan Bakaric with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/styles.4aacfb65.js"></script>
<script src="/assets/js/runtime~main.23366044.js"></script>
<script src="/assets/js/main.4c5ae3c4.js"></script>
<script src="/assets/js/1.4743d043.js"></script>
<script src="/assets/js/2.d8ee9dd6.js"></script>
<script src="/assets/js/18.28e25878.js"></script>
<script src="/assets/js/19.f0bdf1f6.js"></script>
<script src="/assets/js/935f2afb.cff03dc6.js"></script>
<script src="/assets/js/17896441.aacbb830.js"></script>
<script src="/assets/js/0a15610b.cd0a632d.js"></script>
</body>
</html>