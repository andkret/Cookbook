<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.fd17476c3">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="THE DATA ENGINEERING COOKBOOK Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="THE DATA ENGINEERING COOKBOOK Blog Atom Feed"><title data-react-helmet="true">04-HandsOnCourse | THE DATA ENGINEERING COOKBOOK</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="04-HandsOnCourse | THE DATA ENGINEERING COOKBOOK"><meta data-react-helmet="true" name="description" content="Data Engineering Course: Building A Data Platform"><meta data-react-helmet="true" property="og:description" content="Data Engineering Course: Building A Data Platform"><meta data-react-helmet="true" property="og:url" content="http://cookbook.learndataengineering.com/docs/04-HandsOnCourse"><link data-react-helmet="true" rel="shortcut icon" href="/images/CookbookCover.jpg"><link data-react-helmet="true" rel="alternate" href="http://cookbook.learndataengineering.com/docs/04-HandsOnCourse" hreflang="x-default"><link data-react-helmet="true" rel="canonical" href="http://cookbook.learndataengineering.com/docs/04-HandsOnCourse"><link rel="stylesheet" href="/assets/css/styles.5ba6ab3f.css">
<link rel="preload" href="/assets/js/styles.4aacfb65.js" as="script">
<link rel="preload" href="/assets/js/runtime~main.09f2b5c3.js" as="script">
<link rel="preload" href="/assets/js/main.da980c98.js" as="script">
<link rel="preload" href="/assets/js/1.4743d043.js" as="script">
<link rel="preload" href="/assets/js/2.d8ee9dd6.js" as="script">
<link rel="preload" href="/assets/js/18.28e25878.js" as="script">
<link rel="preload" href="/assets/js/19.f0bdf1f6.js" as="script">
<link rel="preload" href="/assets/js/935f2afb.cff03dc6.js" as="script">
<link rel="preload" href="/assets/js/17896441.aacbb830.js" as="script">
<link rel="preload" href="/assets/js/b42f3af8.8ca0480f.js" as="script">
</head>
<body>
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_1oUP">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a><a class="navbar__item navbar__link" href="/docs/01-Introduction">Cookbook</a></div><div class="navbar__items navbar__items--right"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Data Engineering Academy</a><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Plumbers Of Data Science</a><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/images/CookbookCover.jpg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Data Engineering Cookbook</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/01-Introduction">Cookbook</a></li><li class="menu__list-item"><a href="https://learndataengineering.com/" target="_blank" rel="noopener noreferrer" class="menu__link">Data Engineering Academy</a></li><li class="menu__list-item"><a href="https://medium.com/plumbersofdatascience" target="_blank" rel="noopener noreferrer" class="menu__link">Plumbers Of Data Science</a></li><li class="menu__list-item"><a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Data Engineering</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/01-Introduction">01-Introduction</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/02-BasicSkills">02-BasicSkills</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/03-AdvancedSkills">03-AdvancedSkills</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/04-HandsOnCourse">04-HandsOnCourse</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/05-CaseStudies">05-CaseStudies</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/06-BestPracticesCloud">06-BestPracticesCloud</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/07-DataSources">07-DataSources</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/08-InterviewQuestions">08-InterviewQuestions</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/09-BooksAndCourses">09-BooksAndCourses</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/10-Updates">10-Updates</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">04-HandsOnCourse</h1></header><div class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="data-engineering-course-building-a-data-platform"></a>Data Engineering Course: Building A Data Platform<a class="hash-link" href="#data-engineering-course-building-a-data-platform" title="Direct link to heading">#</a></h1><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="contents"></a>Contents<a class="hash-link" href="#contents" title="Direct link to heading">#</a></h2><ul><li><a href="/docs/04-HandsOnCourse#genai-retrieval-augmented-generation-with-ollama-and-elasticsearch">GenAI Retrieval Augmented Generation with Ollama and ElasticSearch</a></li><li><a href="/docs/04-HandsOnCourse#free-data-engineering-course-with-aws-tdengine-docker-and-grafana">Free Data Engineering Course with AWS, TDengine, Docker and Grafana</a></li><li><a href="/docs/04-HandsOnCourse#monitor-your-data-in-dbt-and-detect-quality-issues-with-elementary">Monitor your data in dbt &amp; detect quality issues with Elementary</a></li><li><a href="/docs/04-HandsOnCourse#solving-engineers-4-biggest-airflow-problems">Solving Engineers 4 Biggest Airflow Problems</a></li><li><a href="/docs/04-HandsOnCourse#the-best-alternative-to-airlfow?-mage.ai">The best alternative to Airlfow? Mage.ai</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="genai-retrieval-augmented-generation-with-ollama-and-elasticsearch"></a>GenAI Retrieval Augmented Generation with Ollama and ElasticSearch<a class="hash-link" href="#genai-retrieval-augmented-generation-with-ollama-and-elasticsearch" title="Direct link to heading">#</a></h2><ul><li>This how-to is based on this one from Elasticsearch: <a href="https://www.elastic.co/search-labs/blog/rag-with-llamaIndex-and-elasticsearch" target="_blank" rel="noopener noreferrer">https://www.elastic.co/search-labs/blog/rag-with-llamaIndex-and-elasticsearch</a></li><li>Instead of Elasticsearch cloud we&#x27;re going to run everything locally</li><li>The simplest way to get this done is to just clone this GitHub Repo for the code and docker setup</li><li>I&#x27;ve tried this on a M1 Mac. Changes for Windows with WSL will come later.</li><li>The biggest problems that I had were actually installing the dependencies rather than the code itself.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-ollama"></a>Install Ollama<a class="hash-link" href="#install-ollama" title="Direct link to heading">#</a></h3><ol><li>Download Ollama from here <a href="https://ollama.com/download/mac" target="_blank" rel="noopener noreferrer">https://ollama.com/download/mac</a></li><li>Unzip, drag into applications and install</li><li>do <code>ollama run mistral</code> (It&#x27;s going to download the Mistral 7b model, 4.1GB size)</li><li>Create a new folder in Documents &quot;Elasticsearch-RAG&quot;</li><li>Open that folder in VSCode</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-elasticsearch--kibana-docker"></a>Install Elasticsearch &amp; Kibana (Docker)<a class="hash-link" href="#install-elasticsearch--kibana-docker" title="Direct link to heading">#</a></h3><ol><li>Use the docker-compose file from the Log Monitoring course: <a href="https://github.com/team-data-science/GenAI-RAG/blob/main/docker-compose.yml" target="_blank" rel="noopener noreferrer">https://github.com/team-data-science/GenAI-RAG/blob/main/docker-compose.yml</a></li><li>Download Docker Desktop from here: <a href="https://www.docker.com/products/docker-desktop/" target="_blank" rel="noopener noreferrer">https://www.docker.com/products/docker-desktop/</a></li><li>Install docker desktop and sign in in the app/create a user -&gt; sends you to the browser</li></ol><p><strong>For Windows Users</strong>
Configure WSL2 to use max only 4GB of ram:</p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">wsl --shutdown</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">notepad &quot;$env:USERPROFILE/.wslconfig&quot;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>.wslconfig file:</p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">[wsl2]</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">memory=4GB   # Limits VM memory in WSL 2 up to 4GB</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p><strong>Modify the Linux kernel map count in WSL</strong>
Do this before the start because Elasticsearch requires a higher value to work
<code>sudo sysctl -w vm.max_map_count=262144</code></p><ol start="4"><li>go to the Elasticsearch-RAG folder and do <code>docker compose up</code></li><li>make sure you have Elasticsearch 8.11 or later (we use 8.16 here in this project) if you want to use your own Elasticsearch image</li><li>if you get this error on a mac then just open the console in the docker app: <em>error getting credentials - err: exec: docker-credential-desktop: executable file not found in $PATH, out:</em></li><li>Install xcode command line tools: <code>xcode-select --install</code></li><li>make sure you&#x27;re at python 3.8.1 or larger -&gt; installed 3.13.0 from <a href="https://www.python.org/downloads/" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a></li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="setup-the-virtual-python-environment"></a>Setup the virtual Python environment<a class="hash-link" href="#setup-the-virtual-python-environment" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="preparation-on-a-mac"></a>preparation on a Mac<a class="hash-link" href="#preparation-on-a-mac" title="Direct link to heading">#</a></h4><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-brew"></a>install brew<a class="hash-link" href="#install-brew" title="Direct link to heading">#</a></h5><p>which brew
/bin/bash -c &quot;$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)%22" target="_blank" rel="noopener noreferrer">https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;</a>
export PATH=&quot;/opt/homebrew/bin:$PATH&quot;
brew --version
brew install pyenv
brew install pyenv-virtualenv</p><h5><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-pyenv"></a>install pyenv<a class="hash-link" href="#install-pyenv" title="Direct link to heading">#</a></h5><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">brew install pyenv</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">brew install pyenv-virtualenv</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>Modify the path so that pyenv is in the path variable
<code>nano ~/.zshrc</code></p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">export PYENV_ROOT=&quot;$HOME/.pyenv&quot;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">eval &quot;$(pyenv init --path)&quot;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">eval &quot;$(pyenv init -)&quot;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">eval &quot;$(pyenv virtualenv-init -)&quot;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>install dependencies for building python versions
<code>brew install openssl readline sqlite3 xz zlib</code></p><p>Reload to apply changes
<code>source ~/.zshrc</code></p><p>install python</p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">pyenv install 3.11.6</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pyenv version</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>Set Python version system wide
<code>pyenv global 3.11.6</code></p><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">pyenv virtualenv &lt;python-version&gt; &lt;new-virtualenv-name&gt;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pyenv activate &lt;your-virtualenv-name&gt;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pyenv virtualenv-delete &lt;your-virtualenv-name&gt;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="windows-without-pyenv"></a>Windows without pyenv<a class="hash-link" href="#windows-without-pyenv" title="Direct link to heading">#</a></h4><p>setup virtual python environment - go to the Elasticsearch-RAG folder and do
<code>python3 -m venv .elkrag</code>
enable the environment
<code>source .elkrag/bin/activate</code></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-required-libraries-do-one-at-a-time-so-you-see-errors"></a>Install required libraries (do one at a time so you see errors):<a class="hash-link" href="#install-required-libraries-do-one-at-a-time-so-you-see-errors" title="Direct link to heading">#</a></h3><div class="mdxCodeBlock_3lFL"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">pip install llama-index (optional python3 -m pip install package name)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pip install llama-index-embeddings-ollama</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pip install llama-index-llms-ollama</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pip install llama-index-vector-stores-elasticsearch</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pip install python-dotenv</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="write-the-data-to-elasticsearch"></a>Write the data to Elasticsearch<a class="hash-link" href="#write-the-data-to-elasticsearch" title="Direct link to heading">#</a></h3><ol><li>create / copy in the index.py file</li><li>download the conversations.json file from the folder code examples/GenAI-RAG</li><li>if you get an error with the execution then check if pedantic version is &lt;2.0 <code>pip show pydantic</code> if not do this: <code>pip install &quot;pydantic&lt;2.0</code></li><li>run the program index.py: <a href="https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/index.py" target="_blank" rel="noopener noreferrer">https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/index.py</a></li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="check-the-data-in-elasticsearch"></a>Check the data in Elasticsearch<a class="hash-link" href="#check-the-data-in-elasticsearch" title="Direct link to heading">#</a></h3><ol><li>go to kibana http://localhost:5601/app/management/data/index_management/indices and see the new index called calls</li><li>go to dev tools and try out this query <code>GET calls/_search?size=1 http://localhost:5601/app/dev_tools#/console/shell</code></li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="query-data-from-elasticsearch-and-create-an-output-with-mistral"></a>Query data from elasticsearch and create an output with Mistral<a class="hash-link" href="#query-data-from-elasticsearch-and-create-an-output-with-mistral" title="Direct link to heading">#</a></h3><ol><li>if everything is good then run the query.py file <a href="https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/query.py" target="_blank" rel="noopener noreferrer">https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/query.py</a></li><li>try a few queries :)</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="install-libraries-to-extract-text-from-pdfs"></a>Install libraries to extract text from pdfs<a class="hash-link" href="#install-libraries-to-extract-text-from-pdfs" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="extract-data-from-cv-and-put-it-into-elasticsearch"></a>Extract data from CV and put it into Elasticsearch<a class="hash-link" href="#extract-data-from-cv-and-put-it-into-elasticsearch" title="Direct link to heading">#</a></h3><p>I created a CV with ChatGPT <a href="https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/Liam_McGivney_CV.pdf" target="_blank" rel="noopener noreferrer">https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/Liam_McGivney_CV.pdf</a></p><p>Install the library to extract text from the pdf
<code>pip install PyMuPDF</code>
I had to Shift+Command+p then python clear workspace cache and reload window. Then it saw it :/</p><p>The file cvpipeline.py has the python code for the indexing. It&#x27;s not working right now though!
<a href="https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/cvpipeline.py" target="_blank" rel="noopener noreferrer">https://github.com/andkret/Cookbook/blob/master/Code%20Examples/GenAI-RAG/cvpipeline.py</a></p><p>I&#x27;ll keep developing this and update it once it&#x27;s working.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="free-data-engineering-course-with-aws-tdengine-docker-and-grafana"></a>Free Data Engineering Course with AWS TDengine Docker and Grafana<a class="hash-link" href="#free-data-engineering-course-with-aws-tdengine-docker-and-grafana" title="Direct link to heading">#</a></h2><p><strong>Free hands-on course:</strong> <a href="https://youtu.be/eoj-CnrR9jA" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><p>In this detailed tutorial video, Andreas guides viewers through creating an end-to-end data pipeline using time series data. The project focuses on fetching weather data from a Weather API, processing it on AWS, storing it in TDengine (a time series database), and visualizing the data with Grafana. Here&#x27;s a concise summary of what the video covers:</p><ol><li><strong>Introduction and Setup:</strong></li></ol><ul><li>The project is introduced along with a GitHub repository containing all necessary resources and a step-by-step guide.</li><li>The pipeline architecture includes an IoT weather station, a Weather API, AWS for processing, TDengine for data storage, and Grafana for visualization.</li></ul><ol start="2"><li><strong>Project Components:</strong></li></ol><ul><li><strong>Weather API:</strong> Utilizes weatherapi.com to fetch weather data.</li><li><strong>AWS Lambda:</strong> Processes the data fetched from the Weather API.</li><li><strong>TDengine:</strong> Serves as the time series database to store processed data. It&#x27;s highlighted for its performance and simplicity, especially for time series data.</li><li><strong>Grafana:</strong> Used for creating dashboards to visualize the time series data.</li></ul><ol start="3"><li><strong>Development and Deployment:</strong></li></ol><ul><li>The local development environment setup includes Python, Docker, and VS Code.</li><li>The tutorial covers the creation of a Docker image for the project and deploying it to AWS Elastic Container Registry (ECR).</li><li>AWS Lambda is then configured to use the Docker image from ECR.</li><li>AWS EventBridge is used to schedule the Lambda function to run at specified intervals.</li></ul><ol start="4"><li><strong>Time Series Data:</strong></li></ol><ul><li>The importance of time series data and the benefits of using a time series database like TDengine over traditional relational databases are discussed.</li><li>TDengine&#x27;s features such as speed, scaling, data retention, and built-in functions for time series data are highlighted.</li></ul><ol start="5"><li><strong>Building the Pipeline:</strong></li></ol><ul><li>Detailed instructions are provided for setting up each component of the pipeline:<ul><li>Fetching weather data from the Weather API.</li><li>Processing and sending the data to TDengine using an AWS Lambda function.</li><li>Visualizing the data with Grafana.</li></ul></li><li>Each step includes code snippets and configurations needed to implement the pipeline.</li></ul><ol start="6"><li><strong>Conclusion:</strong></li></ol><ul><li>The video concludes with a demonstration of the completed pipeline, showing weather data visualizations in Grafana.</li><li>Viewers are encouraged to replicate the project using the resources provided in the GitHub repository linked in the video description.</li></ul><p>This video provides a comprehensive guide to building a data pipeline with a focus on time series data, demonstrating the integration of various technologies and platforms to achieve an end-to-end solution.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="monitor-your-data-in-dbt-and-detect-quality-issues-with-elementary"></a>Monitor your data in dbt and detect quality issues with Elementary<a class="hash-link" href="#monitor-your-data-in-dbt-and-detect-quality-issues-with-elementary" title="Direct link to heading">#</a></h2><p><strong>Free hands-on tutorial:</strong> <a href="https://youtu.be/6fnU91Q2gq0" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><p>In this comprehensive tutorial, Andreas delves into the integration of dbt (data build tool) with Elementary to enhance data monitoring and quality detection within Snowflake databases. The tutorial is structured to guide viewers through a hands-on experience, starting with an introduction to a sample project setup and the common challenges faced in monitoring dbt jobs. It then transitions into how Elementary can be utilized to address these challenges effectively.</p><p>Key learning points and tutorial structure include:</p><ol><li><strong>Introduction to the Sample Project:</strong> Andreas showcases a project setup involving Snowflake as the data warehouse, dbt for data modeling and testing, and a visualization tool for data analysis. This setup serves as the basis for the tutorial.</li><li><strong>Challenges in Monitoring dbt Jobs:</strong> Common issues in monitoring dbt jobs are discussed, highlighting the limitations of the dbt interface in providing comprehensive monitoring capabilities.</li><li><strong>Introduction to Elementary:</strong> Elementary is introduced as a dbt-native data observability tool designed to enhance the monitoring and analysis of dbt jobs. It offers both open-source and cloud versions, with the tutorial focusing on the cloud version.</li><li><strong>Setup Requirements:</strong> The tutorial covers the necessary setup on both the Snowflake and dbt sides, including schema creation, user and role configuration in Snowflake, and modifications to the dbt project for integrating with Elementary.</li><li><strong>Elementary&#x27;s User Interface and Features:</strong> A thorough walkthrough of Elementary&#x27;s interface is provided, showcasing its dashboard, test results, model runs, data catalog, and data lineage features. The tool&#x27;s ability to automatically run additional tests, like anomaly detection and schema change detection, is also highlighted.</li><li><strong>Advantages of Using Elementary:</strong> The presenter outlines several benefits of using Elementary, such as easy implementation, native test integration, clean and straightforward UI, and enhanced privacy due to data being stored within the user&#x27;s data warehouse.</li><li><strong>Potential Drawbacks:</strong> Some potential drawbacks are discussed, including the additional load on dbt job execution due to more models being run and limitations in dashboard customization.</li><li><strong>Summary and Verdict:</strong> The tutorial concludes with a summary of the key features and benefits of using Elementary with dbt, emphasizing its value in improving data quality monitoring and detection.</li></ol><p>Overall, viewers are guided through setting up and utilizing Elementary for dbt data monitoring, gaining insights into its capabilities, setup process, and the practical benefits it offers for data quality assurance.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="solving-engineers-4-biggest-airflow-problems"></a>Solving Engineers 4 Biggest Airflow Problems<a class="hash-link" href="#solving-engineers-4-biggest-airflow-problems" title="Direct link to heading">#</a></h2><p><strong>Free hands-on tutorial:</strong> <a href="https://youtu.be/b9bMNEh8bes" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><p>In this informative video, Andreas discusses the four major challenges engineers face when working with Apache Airflow and introduces Astronomer, a managed Airflow service that addresses these issues effectively. Astronomer is highlighted as a solution that simplifies Airflow deployment and management, making it easier for engineers to develop, deploy, and monitor their data pipelines. Here&#x27;s a summary of the key points discussed for each challenge and how Astronomer provides solutions:</p><ol><li>Managing Airflow Deployments:</li></ol><ul><li><strong>Challenge:</strong> Setting up and maintaining Airflow deployments is complex and time-consuming, involving configuring cloud instances, managing resources, scaling, and updating the Airflow system.</li><li><strong>Solution with Astronomer:</strong> Offers a straightforward deployment process where users can easily configure their deployments, choose cloud providers (GCP, AWS, Azure), and set up scaling with just a few clicks. Astronomer handles the complexity, making it easier to manage production and quality environments.</li></ul><ol start="2"><li>Development Environment and Deployment:</li></ol><ul><li><strong>Challenge:</strong> Local installation of Airflow is complicated due to its dependency on multiple Docker containers and the need for extensive configuration.</li><li><strong>Solution with Astronomer:</strong> Provides a CLI tool for setting up a local development environment with a single command, simplifying the process of developing, testing, and deploying pipelines. The Astronomer CLI also helps in initializing project templates and deploying Dags to the cloud effortlessly.</li></ul><ol start="3"><li>Source Code Management and CI/CD Pipelines:</li></ol><ul><li><strong>Challenge:</strong> Collaborative development and continuous integration/deployment (CI/CD) are essential but challenging to implement effectively with Airflow alone.</li><li><strong>Solution with Astronomer:</strong> Facilitates easy integration with GitHub for source code management and GitHub Actions for CI/CD. This allows automatic testing and deployment of pipeline code, ensuring a smooth workflow for teams working on pipeline development.</li></ul><ol start="4"><li>Observing Pipelines and Alarms:</li></ol><ul><li><strong>Challenge:</strong> Monitoring data pipelines and getting timely alerts when issues occur is crucial but often difficult to achieve.</li><li><strong>Solution with Astronomer:</strong> The Astronomer platform provides a user-friendly interface for monitoring pipeline status and performance. It also offers customizable alerts for failures or prolonged task durations, with notifications via email, PagerDuty, or Slack, ensuring immediate awareness and response to issues.</li></ul><p>Overall, the video shows Astronomer as a powerful and user-friendly platform that addresses the common challenges of using Airflow, from deployment and development to collaboration, CI/CD, and monitoring. It suggests that Astronomer can significantly improve the experience of engineers working with Airflow, making it easier to manage, develop, and monitor data pipelines.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="the-best-alternative-to-airlfow-mageai"></a>The best alternative to Airlfow? Mage.ai<a class="hash-link" href="#the-best-alternative-to-airlfow-mageai" title="Direct link to heading">#</a></h2><p><strong>Free hands-on tutorial:</strong> <a href="https://youtu.be/3gXsFEC3aYA" target="_blank" rel="noopener noreferrer">Watch on YouTube</a></p><p>In this insightful video, Andreas introduces Mage, a promising alternative to Apache Airflow, focusing on its simplicity, user-friendliness, and scalability. The video provides a comprehensive walkthrough of Mage, highlighting its key features and advantages over Airflow. Here&#x27;s a breakdown of what viewers can learn and expect from the video:</p><ol><li><strong>Deployment Ease:</strong> Mage offers a stark contrast to Airflow&#x27;s complex setup process. It simplifies deployment to a single Docker image, making it straightforward to install and start on any machine, whether it&#x27;s local or cloud-based on AWS, GCP, or Azure. This simplicity extends to scaling, which Mage handles horizontally, particularly beneficial in Kubernetes environments where performance scales with the number of pipelines.</li><li><strong>User Interface (UI):</strong> Mage shines with its UI, presenting a dark mode interface that&#x27;s not only visually appealing but also simplifies navigation and pipeline management. The UI facilitates easy access to pipelines, scheduling, and monitoring of pipeline runs, offering a more intuitive experience compared to Airflow.</li><li><strong>Pipeline Creation and Modification:</strong> Mage streamlines the creation of ETL pipelines, allowing users to easily add data loaders, transformers, and exporters through its UI. It supports direct interaction with APIs for data loading and provides a visual representation of the data flow, enhancing the overall pipeline design experience.</li><li><strong>Data Visualization and Exploration:</strong> Beyond simple pipeline creation, Mage enables in-depth data exploration within the UI. Users can generate various charts, such as histograms and bar charts, to analyze the data directly, a feature that greatly enhances the tool&#x27;s utility.</li><li><strong>Testing and Scheduling:</strong> Testing pipelines in Mage is straightforward, allowing for quick integration of tests to ensure data quality and pipeline reliability. Scheduling is also versatile, supporting standard time-based triggers, event-based triggers for real-time data ingestion, and API calls for on-demand pipeline execution.</li><li><strong>Support for Streaming and ELT Processes:</strong> Mage is not limited to ETL workflows but also supports streaming and ELT processes. It integrates seamlessly with DBT models for in-warehouse transformations and Spark for big data processing, showcasing its versatility and scalability.</li><li><strong>Conclusion and Call to Action:</strong> Andreas concludes by praising the direction in which the industry is moving, with tools like Mage simplifying data engineering processes. He encourages viewers to try Mage and engage with the content by liking, subscribing, and commenting on their current tools and the potential impact of Mage.</li></ol><p>Overall, the video shows Mage as a highly user-friendly, scalable, and versatile tool for data pipeline creation and management, offering a compelling alternative to traditional tools like Airflow.</p></div></article><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/03-AdvancedSkills"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« 03-AdvancedSkills</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/05-CaseStudies"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">05-CaseStudies »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#contents" class="table-of-contents__link">Contents</a></li><li><a href="#genai-retrieval-augmented-generation-with-ollama-and-elasticsearch" class="table-of-contents__link">GenAI Retrieval Augmented Generation with Ollama and ElasticSearch</a><ul><li><a href="#install-ollama" class="table-of-contents__link">Install Ollama</a></li><li><a href="#install-elasticsearch--kibana-docker" class="table-of-contents__link">Install Elasticsearch &amp; Kibana (Docker)</a></li><li><a href="#setup-the-virtual-python-environment" class="table-of-contents__link">Setup the virtual Python environment</a></li><li><a href="#install-required-libraries-do-one-at-a-time-so-you-see-errors" class="table-of-contents__link">Install required libraries (do one at a time so you see errors):</a></li><li><a href="#write-the-data-to-elasticsearch" class="table-of-contents__link">Write the data to Elasticsearch</a></li><li><a href="#check-the-data-in-elasticsearch" class="table-of-contents__link">Check the data in Elasticsearch</a></li><li><a href="#query-data-from-elasticsearch-and-create-an-output-with-mistral" class="table-of-contents__link">Query data from elasticsearch and create an output with Mistral</a></li><li><a href="#install-libraries-to-extract-text-from-pdfs" class="table-of-contents__link">Install libraries to extract text from pdfs</a></li><li><a href="#extract-data-from-cv-and-put-it-into-elasticsearch" class="table-of-contents__link">Extract data from CV and put it into Elasticsearch</a></li></ul></li><li><a href="#free-data-engineering-course-with-aws-tdengine-docker-and-grafana" class="table-of-contents__link">Free Data Engineering Course with AWS TDengine Docker and Grafana</a></li><li><a href="#monitor-your-data-in-dbt-and-detect-quality-issues-with-elementary" class="table-of-contents__link">Monitor your data in dbt and detect quality issues with Elementary</a></li><li><a href="#solving-engineers-4-biggest-airflow-problems" class="table-of-contents__link">Solving Engineers 4 Biggest Airflow Problems</a></li><li><a href="#the-best-alternative-to-airlfow-mageai" class="table-of-contents__link">The best alternative to Airlfow? Mage.ai</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Andreas Kretz. Built by Kristijan Bakaric with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/styles.4aacfb65.js"></script>
<script src="/assets/js/runtime~main.09f2b5c3.js"></script>
<script src="/assets/js/main.da980c98.js"></script>
<script src="/assets/js/1.4743d043.js"></script>
<script src="/assets/js/2.d8ee9dd6.js"></script>
<script src="/assets/js/18.28e25878.js"></script>
<script src="/assets/js/19.f0bdf1f6.js"></script>
<script src="/assets/js/935f2afb.cff03dc6.js"></script>
<script src="/assets/js/17896441.aacbb830.js"></script>
<script src="/assets/js/b42f3af8.8ca0480f.js"></script>
</body>
</html>